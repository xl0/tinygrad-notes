{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - The `.arange()` insanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CPU\"] = \"1\"\n",
    "os.environ[\"DEBUG\"]=\"4\"\n",
    "os.environ[\"NOOPT\"]=\"1\"\n",
    "\n",
    "from tinygrad import Tensor, dtypes\n",
    "from tinygrad.ops import UOp, Ops, PatternMatcher, UPat, graph_rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something light and fun - a simple arange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UOp(Ops.ADD, dtypes.float, arg=None, src=(\n",
       "  UOp(Ops.RESHAPE, dtypes.float, arg=(8,), src=(\n",
       "    UOp(Ops.REDUCE_AXIS, dtypes.float, arg=(Ops.ADD, (1,)), src=(\n",
       "      UOp(Ops.PERMUTE, dtypes.float, arg=(1, 0), src=(\n",
       "        UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8), src=(\n",
       "          UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8, 1), src=(\n",
       "            UOp(Ops.SHRINK, dtypes.float, arg=((0, 8), (0, 8)), src=(\n",
       "              UOp(Ops.RESHAPE, dtypes.float, arg=(8, 16), src=(\n",
       "                UOp(Ops.SHRINK, dtypes.float, arg=((0, 128),), src=(\n",
       "                  UOp(Ops.RESHAPE, dtypes.float, arg=(135,), src=(\n",
       "                    UOp(Ops.EXPAND, dtypes.float, arg=(9, 15), src=(\n",
       "                      UOp(Ops.RESHAPE, dtypes.float, arg=(1, 15), src=(\n",
       "                        UOp(Ops.PAD, dtypes.float, arg=((7, 0),), src=(\n",
       "                          UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n",
       "                            UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n",
       "                              UOp(Ops.CONST, dtypes.float, arg=0.2, src=(\n",
       "                                x15:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n",
       "                                  UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),)),)),)),)),)),)),)),)),)),)),)),)),)),)),\n",
       "  UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n",
       "    UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n",
       "      UOp(Ops.CONST, dtypes.float, arg=0.3, src=(\n",
       "         x15,)),)),)),))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor.arange(0.5, 2, 0.2) # Start, stop, step => [0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9]\n",
    "a.lazydata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ohh, what the hell? I just want 8 numbers, what's this insanity?\n",
    "\n",
    "When something is not obvious, you can usually just go step by step.\n",
    "\n",
    "```py\n",
    "                          UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n",
    "                            UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n",
    "                              UOp(Ops.CONST, dtypes.float, arg=0.2, src=(\n",
    "```\n",
    "\n",
    "0.2 is the start element (0.5) minus the step (0.3)\n",
    "\n",
    "![](arange_1.png){width=30% align=\"left\"}\n",
    "\n",
    "Next we pad it with 7 (n-1) 0 elements on the left and expand vertically to 9 (n+1):\n",
    "```py\n",
    "                    UOp(Ops.EXPAND, dtypes.float, arg=(9, 15), src=(\n",
    "                      UOp(Ops.RESHAPE, dtypes.float, arg=(1, 15), src=(\n",
    "                        UOp(Ops.PAD, dtypes.float, arg=((7, 0),), src=(\n",
    "```\n",
    "\n",
    "![](arange_2.png){width=50% align=\"left\"}\n",
    "\n",
    "Next, we string it into a 135-element array, cut off the last 7 elements, making it end in `... 0, 0, 0, 0.2 ]`:\n",
    "\n",
    "Then we reshape it back to a rectangle, now (8, 16). The transformation created an interesting pattern.\n",
    "```py\n",
    "              UOp(Ops.RESHAPE, dtypes.float, arg=(8, 16), src=(\n",
    "                UOp(Ops.SHRINK, dtypes.float, arg=((0, 128),), src=(\n",
    "                  UOp(Ops.RESHAPE, dtypes.float, arg=(135,), src=(\n",
    "```\n",
    "\n",
    "![](arange_3.png){width=80% align=\"left\"} \n",
    "\n",
    "Next we take just the left half othe the pattern, and apply 3 transformations that don't actually do anything.\n",
    "\n",
    "We wil explore the source of those transforms later.\n",
    "\n",
    "```py\n",
    "      UOp(Ops.PERMUTE, dtypes.float, arg=(1, 0), src=(\n",
    "        UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8), src=(\n",
    "          UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8, 1), src=(\n",
    "            UOp(Ops.SHRINK, dtypes.float, arg=((0, 8), (0, 8)), src=(\n",
    "```\n",
    "\n",
    "![](arange_4.png){width=50% align=\"left\"} \n",
    "\n",
    "This was a long wat to get a triangular matrix (sad Linear Algebra noises).\n",
    "\n",
    "But now that we have it, let's sum over the elements in one axis (axis 1 in this case):\n",
    "```py\n",
    "  UOp(Ops.RESHAPE, dtypes.float, arg=(8,), src=(\n",
    "    UOp(Ops.REDUCE_AXIS, dtypes.float, arg=(Ops.ADD, (1,)), src=(\n",
    "```\n",
    "\n",
    "![](arange_5.png){width=50% align=\"left\"}\n",
    "\n",
    "\n",
    "I thin you see where this is going. Let's conver the second branch of the top `ADD`:\n",
    "```py\n",
    "  UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n",
    "    UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n",
    "      UOp(Ops.CONST, dtypes.float, arg=0.3, src=(\n",
    "```\n",
    "\n",
    "![](arange_6.png){width=40% align=\"left\"}\n",
    "\n",
    "And the final step - we add the 2 to get to our numbers!\n",
    "```py\n",
    "UOp(Ops.ADD, dtypes.float, arg=None, src=(\n",
    "```\n",
    "\n",
    "![](arange_7.png){width=40% align=\"left\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the code I guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened device CPU from pid:136754\n",
      "r_\u001b[34m8\u001b[0m\u001b[90m_\u001b[0m\u001b[31m8\u001b[0m\u001b[90m\u001b[0m\n",
      " 0: (8, 1)                    float.ptr(8)         (1, 0)                         ShapeTracker(views=(View(shape=(8, 1), strides=(1, 0), offset=0, mask=None, contiguous=True),))\n",
      "[]\n",
      "\n",
      "void r_8_8(float* restrict data0) {\n",
      "  for (int ridx0 = 0; ridx0 < 8; ridx0++) {\n",
      "    *(data0+ridx0) = ((((float)((ridx0+1)))*0.2f)+0.3f);\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[32m*** CPU        1\u001b[0m r_\u001b[34m8\u001b[0m\u001b[90m_\u001b[0m\u001b[31m8\u001b[0m\u001b[90m\u001b[0m                                     arg  1 mem  0.00 GB tm      2.44us/     0.00ms (     0.01 GFLOPS    0.0|0.0     GB/s) ['numpy', 'arange']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.70000005, 0.90000004, 1.1       , 1.3000001 ,\n",
       "       1.5       , 1.7       , 1.9000001 ], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not gonna lie, this is an ugly way to do it. I suspect this could be improved.\n",
    "```c\n",
    "void r_8_8(float* restrict data0) {\n",
    "  for (int ridx0 = 0; ridx0 < 8; ridx0++) {\n",
    "    *(data0 + ridx0) = ((((ridx0 < 7) != 1) ? 0.2f : 0.0f) +\n",
    "                        (((ridx0 < 6) != 1) ? 0.2f : 0.0f) +\n",
    "                        (((ridx0 < 5) != 1) ? 0.2f : 0.0f) +\n",
    "                        (((ridx0 < 4) != 1) ? 0.2f : 0.0f) +\n",
    "                        (((ridx0 < 3) != 1) ? 0.2f : 0.0f) +\n",
    "                        (((ridx0 < 2) != 1) ? 0.2f : 0.0f) +\n",
    "                        (((ridx0 < 1) != 1) ? 0.2f : 0.0f) + 0.5f);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This must be some optimization mis-behaving. With `NOOPT=1` we get good code:\n",
    "```c\n",
    "void r_8_8(float* restrict data0) {\n",
    "  for (int ridx0 = 0; ridx0 < 8; ridx0++) {\n",
    "    *(data0+ridx0) = ((((float)((ridx0+1)))*0.2f)+0.3f);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "At least TinyGrad was able to optimize out all the instane transformations!\n",
    "\n",
    "Let's take a look at the code:\n",
    "\n",
    "`tensor.py` (simplified)\n",
    "```py\n",
    "  def arange(start, stop=None, step=1, **kwargs) -> Tensor:\n",
    "    dtype = ...              # Figure out the output dtype\n",
    "    output_len=ceildiv(stop-start, step)\n",
    "    return (Tensor.full((output_len,), step, dtype=dtype, **kwargs)._cumalu(0, Ops.ADD) + (start - step)).cast(dtype)\n",
    "```\n",
    "\n",
    "- `(Tensor.full((output_len,), step, dtype=dtype, **kwargs)` This creates the (8,) vector filled with `0.2`\n",
    "- `(start - step)` This is the second branch of the top ADD.\n",
    "- `_cumalu(0, Ops.ADD)` And all the magic happens here.\n",
    "\n",
    "```py\n",
    "  def _cumalu(self, axis:int, op:Ops, _include_initial=False) -> Tensor:\n",
    "    pl_sz = self.shape[axis] - int(not _include_initial)\n",
    "    pooled = self.transpose(axis,-1).pad((pl_sz, -int(_include_initial)), value=identity_element(op, self.dtype))._pool((self.shape[axis],))\n",
    "\n",
    "    # For ADD:\n",
    "    return pooled.sum(-1).transpose(axis, -1)\n",
    "```\n",
    "> Note: I think the last line should be `return {Ops.ADD: pooled.sum, Ops.MAX: pooled.max, Ops.MUL: pooled.prod}[op](-1).transpose(axis, -1)`\n",
    "\n",
    "- `pl_sz` will be 7\n",
    "- `self.transpose(axis,-1)` makes the axis we are interested in the last. In our case we only have 1 axis, so this does nothing, and gets optimized out immediately.\n",
    "- `pad((pl_sz, -int(_include_initial))` - Here is the pad that adds the 7 elements to the left filled with zeros (identity element for ADD).\n",
    "- `_pool((self.shape[axis],))`: Here be dragons\n",
    "```py\n",
    "  def _pool(self, k_:tuple[sint, ...], stride:int|tuple[int, ...]=1, dilation:int|tuple[int, ...]=1) -> Tensor:\n",
    "    assert len(self.shape) >= len(k_), f\"can't pool {self.shape} with {k_}\"\n",
    "    s_, d_ = make_tuple(stride, len(k_)), make_tuple(dilation, len(k_))\n",
    "    assert len(k_) == len(s_) == len(d_), f\"stride/dilation mismatch kernel:{k_} stride:{s_} dilation:{d_}\"\n",
    "    noop, i_ = [None] * (self.ndim-len(k_)), self.shape[-len(k_):]\n",
    "    assert all(resolve(d*(k-1)+1 <= i) for k,d,i in zip(k_,d_,i_)), \"kernel size cannot be greater than actual input size\"\n",
    "    o_ = [ceildiv(i-d*(k-1), s) for i,d,k,s in zip(i_,d_,k_,s_)]\n",
    "    if any(resolve(k > s) for k,s in zip(k_,s_)) or any(d != 1 for d in d_):\n",
    "      # input size scaling factor to make sure shrink for stride is possible\n",
    "      f_ = [1 + int(resolve(o*s > (i - d*(k-1)))) for o,s,i,d,k in zip(o_,s_,i_,d_,k_)]\n",
    "      # # repeats such that we don't need padding\n",
    "      x = self.repeat([1]*len(noop) + [ceildiv(k*(i*f+d),i) for k,i,d,f in zip(k_,i_,d_,f_)])\n",
    "      # handle dilation\n",
    "      x = x.shrink(tuple(noop + [(0,k*(i*f+d)) for k,i,d,f in zip(k_,i_,d_,f_)])).reshape(noop + flatten((k,(i*f+d)) for k,i,d,f in zip(k_,i_,d_,f_)))\n",
    "      # handle stride\n",
    "      x = x.shrink(tuple(noop + flatten(((0,k), (0,o*s)) for k,o,s in zip(k_,o_,s_)))).reshape(noop + flatten((k,o,s) for k,o,s in zip(k_,o_,s_)))\n",
    "      x = x.shrink(tuple(noop + flatten(((0,k), (0,o), (0,1)) for k,o in zip(k_,o_)))).reshape(noop + flatten((k,o) for k,o in zip(k_,o_)))\n",
    "      # permute to move reduce to the end\n",
    "      return x.permute(*range(len(noop)), *[len(noop)+i*2+1 for i in range(len(i_))], *[len(noop)+i*2 for i in range(len(i_))])\n",
    "    # TODO: once the shapetracker can optimize well, remove this alternative implementation\n",
    "    x = self.pad(tuple(noop + [(0, max(0,o*s-i)) for i,o,s in zip(i_,o_,s_)])).shrink(tuple(noop + [(0,o*s) for o,s in zip(o_,s_)]))\n",
    "    x = x.reshape(noop + flatten(((o,s) for o,s in zip(o_,s_))))\n",
    "    x = x.shrink(tuple(noop + flatten(((0,o), (0,k)) for o,k in zip(o_,k_))))\n",
    "    return x.permute(*range(len(noop)), *[len(noop)+i*2 for i in range(len(i_))], *[len(noop)+i*2+1 for i in range(len(i_))])\n",
    "```\n",
    "\n",
    "Lol I'm not going to pretend I can follow what's going on here. This obviously does the crazy triangle.\n",
    "\n",
    "- `pooled.sum(-1).transpose(axis, -1)` This must be that REDUCE_AXIS operation, and another transpose that does nothing in this case.\n",
    "\n",
    "Sometimes I wish TinyGrad was easier to understand. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
