{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc 2 - CUDA Runtime library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyGrad has 2 backends for Nvidia GPUS - `CUDA` and `NV`.\n",
    "\n",
    "- `CUDA` just performs calls into the CUDA Runtime library, not much different from host code in a .cu file.\n",
    "- `NV` skips the library, and talks to the driver directly.\n",
    "\n",
    "We will play with the CUDA library here, and do things NV-style in the next chapter.\n",
    "\n",
    "See [CUDA Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html).\n",
    "\n",
    "\n",
    "### `saxpy.cu`\n",
    "Let's look at a free-standing CUDA kernel. This does not include any host code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "`saxpy.cu`\n",
      "\n",
      "::: {.sourceCode}\n",
      "```cpp\n",
      "#include <stdint.h>\n",
      "\n",
      "// SAXPY kernel: y = alpha*x + y\n",
      "__global__ void saxpy(float alpha, float *x, float *y, int32_t n)\n",
      "{\n",
      "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    if (i < n) {\n",
      "        y[i] = alpha * x[i] + y[i];\n",
      "    }\n",
      "}\n",
      "\n",
      "```\n",
      ":::\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|output: asis\n",
    "#|echo: false\n",
    "\n",
    "filename=\"saxpy.cu\"\n",
    "from pathlib import Path\n",
    "c_code = Path(filename).read_text()\n",
    "print(f'''\n",
    "`{filename}`\n",
    "\n",
    "::: {{.sourceCode}}\n",
    "```cpp\n",
    "{c_code}\n",
    "```\n",
    ":::\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -cubin saxpy.cu -o saxpy.cubin -arch=sm_86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda compilation.\n",
    "\n",
    ".cu files often contain a mixture of host and device code. Cuda will split and group them, and will compile them separately, with different compilers for host and device. In out example, we don't have any host code - we will perform all calls to CUDA Runtime Library from python using `ctypes`.\n",
    "\n",
    "NVCC compile the device part of .cu in 2 stages:\n",
    "\n",
    "<a href=\"https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#virtual-architectures\"><img src=\"cuda_compilaiton.png\" style=\"background-color: white; width: 60%\"></a>\n",
    "\n",
    "Source: [NVCC Documentation](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#virtual-architectures)\n",
    "\n",
    "- PTX is the higher-level and mode device-independent aseembly-like code.\n",
    "\n",
    "- SASS is the final assembly code that maps 1:1 to machine code. It is probably backward-compatible, but if you compile old PTX for a new target device SASS, you might get more optimal code that won't run on the old devices.\n",
    "\n",
    "Here we combined the 2 steps into one, and generated a .cubin. It's actually just an ELF file that contains metadata and the machine code for the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saxpy.cubin:     file format elf64-little\n",
      "\n",
      "Sections:\n",
      "Idx Name          Size      VMA               LMA               File off  Algn\n",
      "  0 .debug_frame  00000070  0000000000000000  0000000000000000  00000320  2**0\n",
      "                  CONTENTS, RELOC, READONLY, DEBUGGING, OCTETS\n",
      "  1 .nv.info      00000024  0000000000000000  0000000000000000  00000390  2**2\n",
      "                  CONTENTS, READONLY\n",
      "  2 .nv.info._Z5saxpyfPfS_i 0000006c  0000000000000000  0000000000000000  000003b4  2**2\n",
      "                  CONTENTS, READONLY\n",
      "  3 .nv.callgraph 00000020  0000000000000000  0000000000000000  00000420  2**2\n",
      "                  CONTENTS, READONLY\n",
      "  4 .nv.rel.action 00000010  0000000000000000  0000000000000000  00000440  2**3\n",
      "                  CONTENTS, READONLY\n",
      "  5 .nv.constant0._Z5saxpyfPfS_i 0000017c  0000000000000000  0000000000000000  00000460  2**2\n",
      "                  CONTENTS, ALLOC, LOAD, READONLY, DATA\n",
      "  6 .text._Z5saxpyfPfS_i 00000180  0000000000000000  0000000000000000  00000600  2**7\n",
      "                  CONTENTS, ALLOC, LOAD, READONLY, CODE\n"
     ]
    }
   ],
   "source": [
    "!objdump --headers saxpy.cubin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.text._Z5saxpyfPfS_i` must have the code for the kernel.\n",
    "\n",
    "I don't know why .nv.constant0._Z5saxpyfPfS_i is so large (300 bytes), but it's all zeros.\n",
    "\n",
    "There is also `cuobjdump` that can parse the CUDA-specific sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resource usage:\n",
      " Common:\n",
      "  GLOBAL:0\n",
      " Function _Z5saxpyfPfS_i:\n",
      "  REG:10 STACK:0 SHARED:0 LOCAL:0 CONSTANT[0]:380 TEXTURE:0 SURFACE:0 SAMPLER:0\n"
     ]
    }
   ],
   "source": [
    "!cuobjdump saxpy.cubin --dump-resource-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or disassemble the code, although it's not very pretty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tcode for sm_86\n",
      "\t\tFunction : _Z5saxpyfPfS_i\n",
      "\t.headerflags\t@\"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM86 EF_CUDA_VIRTUAL_SM(EF_CUDA_SM86)\"\n",
      "        /*0000*/                   MOV R1, c[0x0][0x28] ;                        /* 0x00000a0000017a02 */\n",
      "                                                                                 /* 0x000fe40000000f00 */\n",
      "        /*0010*/                   S2R R4, SR_CTAID.X ;                          /* 0x0000000000047919 */\n",
      "                                                                                 /* 0x000e280000002500 */\n",
      "        /*0020*/                   S2R R3, SR_TID.X ;                            /* 0x0000000000037919 */\n",
      "                                                                                 /* 0x000e240000002100 */\n",
      "        /*0030*/                   IMAD R4, R4, c[0x0][0x0], R3 ;                /* 0x0000000004047a24 */\n",
      "                                                                                 /* 0x001fca00078e0203 */\n",
      "        /*0040*/                   ISETP.GE.AND P0, PT, R4, c[0x0][0x178], PT ;  /* 0x00005e0004007a0c */\n",
      "                                                                                 /* 0x000fda0003f06270 */\n",
      "        /*0050*/               @P0 EXIT ;                                        /* 0x000000000000094d */\n",
      "                                                                                 /* 0x000fea0003800000 */\n",
      "        /*0060*/                   MOV R5, 0x4 ;                                 /* 0x0000000400057802 */\n",
      "                                                                                 /* 0x000fe20000000f00 */\n",
      "        /*0070*/                   ULDC.64 UR4, c[0x0][0x118] ;                  /* 0x0000460000047ab9 */\n",
      "                                                                                 /* 0x000fc80000000a00 */\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "!cuobjdump saxpy.cubin --dump-sass | head -n 20 && echo \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device init\n",
    "\n",
    "TinyGrad autogenerates `ctypes` bindings for the CUDA library, so let's just use them.\n",
    "\n",
    "This works pretty much the same way as calling those functions from C.\n",
    "\n",
    "I will use the lower-level [CUDA Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html) functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "from tinygrad.runtime.autogen import cuda  #\n",
    "from tinygrad.helpers import init_c_var\n",
    "\n",
    "# import os\n",
    "# os.environ[\"IOCTL\"] = \"1\"\n",
    "# import nv_ioctl\n",
    "\n",
    "def check(status):\n",
    "    if status != 0:\n",
    "        error_str = ctypes.string_at(init_c_var(ctypes.POINTER(ctypes.c_char)(),\n",
    "                                               lambda x: cuda.cuGetErrorString(status, ctypes.byref(x)))).decode()\n",
    "        raise RuntimeError(f\"CUDA Error {status}, {error_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html\n",
    "check(cuda.cuInit(0)) # This 0 is always 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_device = ctypes.c_int() # It's actually just an int with the value of the device ID\n",
    "# It fails if you try to get a device that does not exist, but oherwise if just returns the device ID you gave it.\n",
    "check(cuda.cuDeviceGet(cu_device, 0)) # cu_device is passed by pointer, it's converted automatically based on the cuDeviceGet function signature\n",
    "cu_device.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 1\n"
     ]
    }
   ],
   "source": [
    "device_count = ctypes.c_int()\n",
    "check(cuda.cuDeviceGetCount(device_count))\n",
    "print(f\"Number of CUDA devices available: {device_count.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device_name = ctypes.create_string_buffer(100)\n",
    "check(cuda.cuDeviceGetName(device_name, len(device_name), cu_device))\n",
    "print(device_name.value.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor = ctypes.c_int()\n",
    "major = ctypes.c_int()\n",
    "check(cuda.cuDeviceComputeCapability(major, minor, 0))\n",
    "major.value, minor.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory on device: 15.59 GB\n"
     ]
    }
   ],
   "source": [
    "# Get total memory on the device\n",
    "total_memory = ctypes.c_size_t()\n",
    "check(cuda.cuDeviceTotalMem_v2(ctypes.byref(total_memory), cu_device))\n",
    "print(f\"Total memory on device: {total_memory.value / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "In CUDA, a context represents the primary object for managing resources and execution on a GPU device.\n",
    "\n",
    "Each thread can have one active context at a time, and contexts can be pushed/popped from a stack.\n",
    "The context must be created before any CUDA operations can be performed on a device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tinygrad.runtime.autogen.cuda.struct_CUctx_st>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_context = cuda.CUcontext()\n",
    "\n",
    "# cuCtxCreate_v2 is actually identical to cuCtxCreate:\n",
    "# include/cuda.h:\n",
    "# ...\n",
    "# #define cuCtxCreate                         cuCtxCreate_v2\n",
    "# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html\n",
    "check(cuda.cuCtxCreate_v2(cu_context, 0, cu_device))\n",
    "cu_context.contents # This is a pointer to the context object. It is opaque, no idea what is the size or composition of the context object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory management\n",
    "\n",
    "The device is ready. Let's allocate the memory for the input and output arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 128\n",
    "\n",
    "# saxpy performs the operation y = a*x + y\n",
    "dev_x = cuda.CUdeviceptr() # 64 bit pointer to device memory\n",
    "dev_y = cuda.CUdeviceptr()\n",
    "\n",
    "\n",
    "# Allocate the buffers on the device\n",
    "# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html\n",
    "# include/cuda.h:\n",
    "# #define cuMemAlloc                          cuMemAlloc_v2\n",
    "check(cuda.cuMemAlloc_v2(dev_x, N*4))\n",
    "check(cuda.cuMemAlloc_v2(dev_y, N*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_x = np.linspace(0, 100, N).astype(np.float32)\n",
    "host_y = np.zeros(N).astype(np.float32)\n",
    "\n",
    "# Copy data to device. IDK why they are all called _v2\n",
    "check(cuda.cuMemcpyHtoD_v2(dev_x, host_x.ctypes.data_as(ctypes.c_void_p), N*4))\n",
    "check(cuda.cuMemcpyHtoD_v2(dev_y, host_y.ctypes.data_as(ctypes.c_void_p), N*4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and run the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tinygrad.runtime.autogen.cuda.LP_struct_CUmod_st>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = open(\"saxpy.cubin\", \"rb\").read()\n",
    "module = cuda.CUmodule() # Another opaque object.\n",
    "\n",
    "# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MODULE.html\n",
    "check(cuda.cuModuleLoadData(module, image))\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tinygrad.runtime.autogen.cuda.LP_struct_CUfunc_st>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx = cuda.CUfunction() # You guessed it, anoher opaque object\n",
    "check(cuda.cuModuleGetFunction(fx, module, \"_Z5saxpyfPfS_i\".encode(\"utf-8\")))\n",
    "fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the parameter array for the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size =  (1,1,1)\n",
    "block_size = (N,1,1)\n",
    "\n",
    "shared_mem_size = 0 # We don't need shared memory for this kernel\n",
    "\n",
    "# Args\n",
    "# 0 - alpha (float)\n",
    "# 1 - input (pointer to float)\n",
    "# 2 - output (pointer to float)\n",
    "# 3 - N (int32)\n",
    "\n",
    "# The params argument to cuLaunchKernel is a pointer to an array of pointers to the parameters.\n",
    "# The CUDA library knows the size of each parameter from the metadata, so it can figure out how to pass them to the kernel.\n",
    "# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC.html\n",
    "\n",
    "alpha_val = ctypes.c_float(2.0)\n",
    "n = ctypes.c_int32(N)\n",
    "\n",
    "alpha_ptr = ctypes.cast(ctypes.addressof(alpha_val), ctypes.c_void_p) # Pointer to alpha value\n",
    "dev_x_ptr = ctypes.cast(ctypes.addressof(dev_x), ctypes.c_void_p) # Pointer to the x array\n",
    "dev_y_ptr = ctypes.cast(ctypes.addressof(dev_y), ctypes.c_void_p) # Pointer to the y array\n",
    "n_ptr = ctypes.cast(ctypes.addressof(n), ctypes.c_void_p) # Pointer to the N value\n",
    "\n",
    "VoidPtrArrayType = ctypes.c_void_p * 4\n",
    "params = VoidPtrArrayType() # Create the array to hold pointers to args\n",
    "\n",
    "# Populate the array with pointers to the actual kernel arguments\n",
    "params[0] = alpha_ptr\n",
    "params[1] = dev_x_ptr\n",
    "params[2] = dev_y_ptr\n",
    "params[3] = n_ptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC.html\n",
    "check(cuda.cuLaunchKernel(fx, *grid_size, *block_size, shared_mem_size, None, params, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the result back to the host\n",
    "check(cuda.cuMemcpyDtoH_v2(host_y.ctypes.data_as(ctypes.c_void_p), dev_y, N*4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.78740156,  1.5748031 ,  2.3622048 ,  3.1496062 ,\n",
       "        3.937008  ,  4.7244096 ,  5.5118113 ,  6.2992125 ,  7.086614  ,\n",
       "        7.874016  ,  8.661417  ,  9.448819  , 10.23622   , 11.0236225 ,\n",
       "       11.811024  ], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_x[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  1.5748031,  3.1496062,  4.7244096,  6.2992125,\n",
       "        7.874016 ,  9.448819 , 11.0236225, 12.598425 , 14.173228 ,\n",
       "       15.748032 , 17.322834 , 18.897638 , 20.47244  , 22.047245 ,\n",
       "       23.622047 ], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_y[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(host_y == host_x * alpha_val.value).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
