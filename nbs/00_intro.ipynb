{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp uops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CPU\"] = \"1\"\n",
    "os.environ[\"TRACEMETA\"] = \"0\"\n",
    "os.environ[\"DEBUG\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygrad as tg\n",
    "from tinygrad import Tensor, dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy tensors and UOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tinygrad API is quite similar to PyTorch, with some quirks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <UOp CPU (10, 10) float (<Ops.ADD: 48>, None)> on CPU with grad None>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor.ones(10, 10)\n",
    "b = a + 2\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyGrad is lazy - it does not perform any computation until explicitly asked to.\n",
    "\n",
    "Instead, it saves the operations required to get the result as a tree of `UOps`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UOp(Ops.ADD, dtypes.float, arg=None, src=(\n",
       "  UOp(Ops.EXPAND, dtypes.float, arg=(10, 10), src=(\n",
       "    UOp(Ops.RESHAPE, dtypes.float, arg=(1, 1), src=(\n",
       "      UOp(Ops.CONST, dtypes.float, arg=1.0, src=(\n",
       "        x3:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n",
       "          UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),)),)),\n",
       "  UOp(Ops.EXPAND, dtypes.float, arg=(10, 10), src=(\n",
       "    UOp(Ops.RESHAPE, dtypes.float, arg=(1, 1), src=(\n",
       "      UOp(Ops.CONST, dtypes.float, arg=2.0, src=(\n",
       "         x3,)),)),)),))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.lazydata # It's called `lazydaya` for historic reasons. Rename to `Tensor.uops`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ADD` UOp has 2 `sources`, both being constants (`1` and `2`) that are both reshaped to (1, 1) and expanded to shape (10, 10).\n",
    "\n",
    "The `CONST` UOp takes the value as `argument`, and has a `VIEW` UOp as it's source, which in turn sources from a `DEVICE` Uop.\n",
    "\n",
    "Note that `x3:=` walrus assignment, and `x3` being reused for the second `CONST` UOp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.ops import UOp, Ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a detailed look at UOps in the next chapter, but for now, let's see how to actually compute the value of `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened device CPU from pid:958683\n",
      "E_\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m\n",
      " 0: (25, 4)                   float.ptr(100)       (4, 1)\n",
      "[Opt(op=OptOps.UPCAST, axis=0, arg=4)]\n",
      "typedef float float4 __attribute__((aligned(16),vector_size(16)));\n",
      "void E_25_4(float* restrict data0) {\n",
      "  for (int ridx0 = 0; ridx0 < 25; ridx0++) {\n",
      "    *((float4*)((data0+(ridx0<<2)))) = (float4){3.0f,3.0f,3.0f,3.0f};\n",
      "  }\n",
      "}\n",
      "\u001b[32m*** CPU        1\u001b[0m E_\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                    arg  1 mem  0.00 GB tm      3.28us/     0.00ms (     0.00 GFLOPS    0.1|0.1     GB/s) \n"
     ]
    }
   ],
   "source": [
    "# This runs the computations needed to get the value of the tensor\n",
    "# It does not get realized without the .contiguous() though (TODO: Explain why)\n",
    "# Also, should .contiguous() just always be part of .realize()?\n",
    "b_realized = b.contiguous().realize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The debug output gives us a glimpse into how tinygrad performs the computations. It will take the UOp tree, perform a number of transformations on it, and creates one or more `kernels` - functions that run on the device (potentially many instances in parallel) and do the actual computation.\n",
    "\n",
    "In this case, the device is `CPU`, which means the kernel will be just plain sequential C code, which will be compiled with `clang` into a small piece of position-independent binary, then loaded and executed using `ctypes`.\n",
    "\n",
    "The `float4` is a common optimization that you see on both CPU and GPU - it's more optimal to access memory in 128-byte chunks (4 32-bit floats) at a time, so TinyGrad is being smart here. The optimal number might be device-specific, but 128 bytes is common.\n",
    "\n",
    "And of course, since we used constants in our computation, there was no need to add `1+2` - TinyGrad was able to just fill the output with the correct value.\n",
    "\n",
    "If we ran it on an NVida GPU, it would instead generate and run CUDA code, same for other devices.\n",
    "\n",
    "We will cover the details of transformations done on the UOps tree at a later time, but for now, let's look at the result.\n",
    "\n",
    "Here is the buffer that contains the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tinygrad.device.Buffer'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<buf real:True device:CPU size:100 dtype:dtypes.float offset:0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(b_realized.lazydata.base.realized))\n",
    "\n",
    "b_realized.lazydata.base.realized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used the CPU device, it's in CPU memory, and we can peek into it directly using `memoryview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00004040'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = memoryview(b_realized.lazydata.base.realized._buf)\n",
    "view[:4].hex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0x00004040` is the hex for float32 '3.0'. Let's use numpy to get a better view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Note: The buffer is shapeless, so we use `.reshape()` to bring it back to the correct shape\n",
    "np.frombuffer(view, dtype=np.float32).reshape(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.numpy()` and `.tolist()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there is a more convenient way to access the result from Python - use `.numpy()` on the tensor.\n",
    "\n",
    "This will make sure the tensor ends up on CPU, realize it, and will give the result the correct shape and dtype.\n",
    "\n",
    "`.numpy()` will allso create a copy of the data, so the memory buffer does not just disappear from under our feer when the tensor gets garbage collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** CPU        2 E_\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                    arg  1 mem  0.00 GB tm      8.03us/     0.01ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use `.tolist()` to convert it to a python list (or a list of liss of lists ... for the correct number of dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** CPU        3 E_\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90m\u001b[0m                                    arg  1 mem  0.00 GB tm      8.17us/     0.02ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n",
       " [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyGrad has a concept of \"default device\", which we set using the \"CPU\" env variable in the beginning of the notebook.\n",
    "\n",
    "The device and dtype can be set when creating the tensor, and you can also use `.to()` to copy the tensor to a different device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <UOp CUDA () half (<Ops.CONST: 74>, None)> on CUDA with grad None>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor(1, device=\"CUDA\", dtype=tg.dtypes.float16)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <UOp CPU () half (<Ops.COPY: 9>, None)> on CPU with grad None>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.to(device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a CUDA kernel for the same computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened device CUDA from pid:958683\n",
      "E_\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90mn1\u001b[0m\n",
      " 0: (25, 4)                   float.ptr(100)       (4, 1)\n",
      "[Opt(op=OptOps.UPCAST, axis=0, arg=4)]\n",
      "#define INFINITY (__int_as_float(0x7f800000))\n",
      "#define NAN (__int_as_float(0x7fffffff))\n",
      "extern \"C\" __global__ void __launch_bounds__(1) E_25_4n1(float* data0) {\n",
      "  int gidx0 = blockIdx.x; /* 25 */\n",
      "  *((float4*)((data0+(gidx0<<2)))) = make_float4(3.0f,3.0f,3.0f,3.0f);\n",
      "}\n",
      "\u001b[32m*** CUDA       4\u001b[0m E_\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90mn1\u001b[0m                                  arg  1 mem  0.00 GB tm     25.60us/     0.05ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n",
      "\u001b[32m*** CPU        5\u001b[0m \u001b[33mcopy      400,     CPU <- CUDA   \u001b[0m         arg  2 mem  0.00 GB tm     55.73us/     0.10ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor.ones((10, 10), device=\"CUDA\")\n",
    "b = a + 2\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar pattern, but since the GPU is highly parallel, TinyGrad decided to create 25 threads, each setting its own 4-float chunk.\n",
    "\n",
    "> If you are familiar with CUDA, you might notice that we created 25 separate thread blocks with 1 thread each instead of 1 block with 25 threads, which is definitely suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** CUDA       6 E_\u001b[34m25\u001b[0m\u001b[90m_\u001b[0m\u001b[33m4\u001b[0m\u001b[90mn1\u001b[0m                                  arg  1 mem  0.00 GB tm    574.21us/     0.68ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n"
     ]
    }
   ],
   "source": [
    "b_realized = b.contiguous().realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<buf real:True device:CUDA size:100 dtype:dtypes.float offset:0>\n",
      "c_ulong(140551080902656)\n"
     ]
    }
   ],
   "source": [
    "print(b_realized.lazydata.base.realized)\n",
    "print(b_realized.lazydata.base.realized._buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the output buffer is on the GPU this time, so we can't access it from the CPU directly.\n",
    "\n",
    "But trust me, the data is definitely there. Let's use PyCuda to peek into the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycuda\n",
    "import pycuda.driver as cuda\n",
    "import numpy as np\n",
    "\n",
    "# Create a numpy array to hold the data (100 32-bit floats)\n",
    "cpu_array = np.empty((10, 10), dtype=np.float32)\n",
    "\n",
    "# Copy data from GPU to CPU\n",
    "cuda.memcpy_dtoh(cpu_array, b_realized.lazydata.base.realized._buf.value)\n",
    "\n",
    "cpu_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyGrad is a complex beast, so it's normal if this intro left you with more questions than answers. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
