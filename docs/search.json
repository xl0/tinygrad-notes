[
  {
    "objectID": "appendix_b.html",
    "href": "appendix_b.html",
    "title": "Appendix B - UOp Summary",
    "section": "",
    "text": "UOps form the intermediate representation (IR) in tinygrad’s computation graph after the initial Tensor operations are defined and before final code generation. They represent a lower-level, more explicit graph that undergoes several optimization and transformation passes.\n\n\nMeta / Framework Ops\nThese UOps are primarily used by the tinygrad framework itself for graph structure, scheduling, device management, and metadata, rather than direct computation on tensor data.\n\nSINK\n\nDescription: Represents the final output(s) of a computation graph or kernel. It acts as a root node for graph traversal and scheduling.\nPurpose: Marks the end of a computation path that needs to be realized. Used to define the boundaries of kernels during scheduling.\nArgs: Optional KernelInfo containing metadata about the kernel (name, local dims, etc.).\nSources: One or more UOps that produce the final results (often STORE or ASSIGN ops).\nStage: Graph Construction, Scheduling (marks kernel boundaries).\nNotes: Essential for defining what needs to be computed. Multiple UOps can feed into a single SINK. Simplified away before final rendering.\n\n\n\nKERNEL\n\nDescription: An internal UOp used during scheduling to encapsulate the operations belonging to a single kernel launch.\nPurpose: Groups UOps together that will be executed as one unit on the target device. Helps manage kernel boundaries and dependencies.\nArgs: Kernel object containing the kernel’s AST and metadata.\nSources: UOps representing the inputs required by the kernel (often BUFFER or other KERNEL outputs via ASSIGN).\nStage: Scheduling.\nNotes: This is a temporary node used by the scheduler and is expanded/replaced before final code generation.\n\n\n\nNAME\n\nDescription: Assigns a name (string) to a UOp, typically used for naming kernels or variables in the generated code.\nPurpose: Code readability and debugging. Allows generated kernels/variables to have meaningful names based on the high-level operations.\nArgs: str - the desired name.\nSources: Usually none, but can wrap other ops during specific transformations.\nStage: Scheduling, Code Generation.\nNotes: Often associated with SINK or DEFINE_VAR.\n\n\n\nDEVICE\n\nDescription: Represents a target device (e.g., “CPU”, “CUDA:0”).\nPurpose: Specifies the device for memory allocation or computation. Used as a source for BUFFER, COPY, CONST.\nArgs: str - the device name.\nSources: None.\nStage: Graph Construction, Scheduling.\n\n\n\nMULTI\n\nDescription: Represents a tensor sharded across multiple devices.\nPurpose: Manages data parallelism by holding references to UOps representing shards on different devices.\nArgs: (Optional[int], tuple[bool, ...]) - The sharding axis (or None) and a tuple indicating which shards hold real data (vs padding/zeros).\nSources: Multiple UOps, each representing a shard on a specific device.\nStage: Graph Construction (created by Tensor.shard).\nNotes: Handled by a specific rewrite pass (get_multi_map) to distribute operations across shards.\n\n\n\nCOPY\n\nDescription: Copies data from one buffer/device to another.\nPurpose: Explicit data movement between devices or cloning a buffer.\nArgs: bool - clone=True forces a new allocation even on the same device.\nSources: (DEVICE, UOp) - Target device and the UOp to copy from.\nStage: Graph Construction, Scheduling.\nNotes: Simplified away if source and destination devices are the same and clone=False. Can become BufferXfer for optimized transfers.\n\n\n\nASSIGN\n\nDescription: Represents an assignment operation. At the Tensor level, it signifies tensor.assign(other_tensor). At the UOp level after lowering, it often represents updating an accumulator (DEFINE_ACC).\nPurpose: In-place modification of data (conceptually) or accumulator updates.\nArgs: None.\nSources: (target, value) - The UOp being assigned to (often BUFFER or DEFINE_ACC) and the new value UOp.\nStage: Graph Construction, Lowering (Accumulator updates).\nNotes: High-level ASSIGN is often lowered to STORE or kernel operations. Low-level ASSIGN is crucial for reductions.\n\n\n\nBIND\n\nDescription: Binds a symbolic Variable (DEFINE_VAR) to a concrete integer value (CONST).\nPurpose: Resolves symbolic dimensions or parameters at runtime or during JIT compilation.\nArgs: None.\nSources: (DEFINE_VAR, CONST) - The variable and the constant value it’s bound to.\nStage: Graph Construction (Symbolic Shapes), JIT.\nNotes: Removed during scheduling/lowering by substituting the variable with its bound constant value.\n\n\n\nDEFINE_VAR\n\nDescription: Defines a symbolic variable, typically representing a dimension size.\nPurpose: Allows for operations on tensors with shapes that are not known at compile time (symbolic shapes).\nArgs: (name: str, min_val: int, max_val: int) - Name and range of the variable.\nSources: None.\nStage: Graph Construction (Symbolic Shapes).\nNotes: Usually bound using BIND before execution.\n\n\n\nUNIQUE\n\nDescription: Represents a unique identifier, typically used for buffer allocation.\nPurpose: Ensures that different BUFFER UOps representing distinct allocations get unique identities, even if they have the same size/dtype/device.\nArgs: int - A unique integer.\nSources: None.\nStage: Graph Construction (Buffer creation).\n\n\n\nEMPTY\n\nDescription: Represents an empty tensor (placeholder).\nPurpose: Used internally, often as a starting point for tensor creation before data is specified (e.g., Tensor.empty).\nArgs: None.\nSources: None.\nStage: Graph Construction.\nNotes: Usually replaced or filled quickly.\n\n\n\nNOOP\n\nDescription: A no-operation instruction.\nPurpose: Used as a placeholder or identity operation during graph transformations, often inserted and then removed by subsequent passes. Can sometimes force materialization in specific backends (e.g., before BITCAST).\nArgs: None.\nSources: (UOp,) - The UOp to pass through.\nStage: Intermediate Transformations.\n\n\n\nCUSTOM / CUSTOMI\n\nDescription: Represents a custom operation defined by a string, potentially with specific backend implementations. CUSTOMI implies inline code.\nPurpose: Allows extending tinygrad with operations not covered by standard Ops, often for backend-specific intrinsics or complex fused operations.\nArgs: str - A format string for the custom code.\nSources: Variable number of UOps, used to fill the format string.\nStage: Lowering, Code Generation.\n\n\n\n\n\nConstants\n\nCONST\n\nDescription: Represents a scalar constant value.\nPurpose: Embeds constant values directly into the computation graph.\nArgs: ConstType (int, float, bool) - The constant value.\nSources: Usually none, but can have a VIEW(DEVICE) source to indicate device placement.\nStage: All stages.\nNotes: Tensor.full creates CONST ops expanded to the correct shape.\n\n\n\nVCONST\n\nDescription: Represents a vector constant value.\nPurpose: Similar to CONST but for vector types.\nArgs: tuple[ConstType, ...] - The tuple of constant values.\nSources: Usually none.\nStage: All stages.\nNotes: Often lowered to VECTORIZE(CONST, CONST, ...) during rendering.\n\n\n\n\n\nMovement Ops\nThese ops change the logical view (shape, strides, offset, mask) of data without necessarily moving or copying it in memory immediately. They primarily manipulate the ShapeTracker associated with a buffer.\n\nRESHAPE\n\nDescription: Changes the shape of the tensor while preserving the total number of elements.\nPurpose: Modifies the logical dimensions.\nArgs: tuple[sint, ...] - The new shape.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops.\n\n\n\nPERMUTE\n\nDescription: Reorders the dimensions of the tensor.\nPurpose: Changes the logical order of axes.\nArgs: tuple[int, ...] - The permutation order.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops.\n\n\n\nEXPAND\n\nDescription: Expands dimensions of size 1 to a larger size.\nPurpose: Broadcasting.\nArgs: tuple[sint, ...] - The target shape with expanded dimensions.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops. Stride becomes 0 for expanded dimensions.\n\n\n\nPAD\n\nDescription: Adds padding to the tensor along specified dimensions.\nPurpose: Increases the size of dimensions, typically for convolutions or alignment.\nArgs: tuple[tuple[sint, sint], ...] - Padding amounts (before, after) for each dimension.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops. Adjusts offset and mask.\n\n\n\nSHRINK\n\nDescription: Shrinks the tensor along specified dimensions by selecting a sub-region.\nPurpose: Cropping or selecting parts of a tensor.\nArgs: tuple[tuple[sint, sint], ...] - Start and end indices (exclusive) for shrinking each dimension.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops. Adjusts offset and mask.\n\n\n\nFLIP\n\nDescription: Reverses the order of elements along specified dimensions.\nPurpose: Data augmentation or specific algorithms requiring reversed views.\nArgs: tuple[bool, ...] - A boolean tuple indicating which axes to flip.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops. Modifies strides and offset.\n\n\n\n\n\nLowering / Indexing Ops\nThese UOps appear during the lowering process, translating logical views and operations into memory accesses and validity checks.\n\nVIEW\n\nDescription: Represents a logical view (ShapeTracker) applied to a base buffer UOp. This is the primary way movement ops are represented after initial lowering.\nPurpose: Encapsulates shape, stride, offset, and mask information without creating new data. Connects logical tensor operations to underlying buffer representations.\nArgs: ShapeTracker - The view information.\nSources: (UOp,) - The base UOp (often BUFFER, CONST, or another VIEW). Can also have a DEVICE source for CONST.\nStage: Lowering, Scheduling.\nNotes: Multiple VIEW ops are merged into one. CONTIGUOUS ops often trigger realization before a VIEW. VIEW ops are pushed towards memory operations (LOAD/STORE) or constants during simplification.\n\n\n\nINDEX\n\nDescription: Calculates a memory address/index based on a buffer and logical indices, potentially applying a validity mask.\nPurpose: Translates multi-dimensional logical indexing into a linear memory index for LOAD and STORE. Encapsulates the ShapeTracker.to_indexed_uops logic.\nArgs: None.\nSources: (buffer_uop, logical_indices_uop, Optional[valid_uop]) - The buffer (e.g., DEFINE_GLOBAL), the calculated index expression, and an optional validity mask (dtypes.bool).\nStage: Lowering (post rewrite_shapetracker_with_index), Codegen.\nNotes: This is part of the “new style” load/store introduced to simplify rendering.\n\n\n\nVALID\n\nDescription: Represents the validity mask derived from a ShapeTracker’s mask attribute.\nPurpose: Computes whether a given logical index corresponds to a valid element within the original (unpadded, unshrunk) data. Used for masking operations, especially loads/stores near boundaries.\nArgs: None.\nSources: (VIEW,) - The VIEW UOp containing the ShapeTracker with mask information.\nStage: Lowering.\nNotes: Often simplified or combined with index calculations. Becomes the valid part of INDEX or the gate in LOAD/STORE.\n\n\n\nGEP (Get Element Pointer)\n\nDescription: Extracts specific elements from a vector UOp.\nPurpose: Accessing individual lanes of a vectorized operation or constant.\nArgs: tuple[int, ...] - The indices of the elements to extract.\nSources: (UOp,) - The vector UOp.\nStage: Codegen, Final Rendering.\nNotes: The inverse of VECTORIZE. Allows scalar operations on elements previously combined into a vector.\n\n\n\n\n\nMemory Ops\nThese UOps deal directly with memory allocation, definition, and access.\n\nBUFFER\n\nDescription: Represents a raw memory buffer allocated on a specific device. This is the “base” UOp for most tensor data after initial allocation.\nPurpose: Holds the reference to the actual allocated memory used by tensors.\nArgs: int - Size of the buffer in elements.\nSources: (DEVICE, UNIQUE) - The device and a unique identifier.\nStage: Graph Construction, Scheduling.\nNotes: BUFFER UOps map to Buffer objects which manage the actual memory. BUFFER itself doesn’t have a ShapeTracker; VIEW ops are applied on top.\n\n\n\nBUFFER_VIEW\n\nDescription: Represents a view of an existing BUFFER UOp, potentially with an offset. Introduced for DISK buffers.\nPurpose: Allows accessing parts of a larger buffer (e.g., a file on disk) without loading the entire thing.\nArgs: (size: int, offset: int) - Size in elements and offset in elements from the base buffer.\nSources: (BUFFER,) - The base buffer.\nStage: Scheduling (Specific backends like DISK).\n\n\n\nDEFINE_GLOBAL\n\nDescription: Defines a global buffer in the kernel arguments.\nPurpose: Declares input/output buffers passed into the kernel.\nArgs: int (optional, buffer index) or None.\nSources: None.\nStage: Final Lowering (inside linearize_uop), Codegen.\nNotes: Has a PtrDType. The renderer uses this to generate kernel signatures.\n\n\n\nDEFINE_LOCAL\n\nDescription: Defines a buffer in local (shared) memory.\nPurpose: Allocation of shared memory for intermediate results accessible by threads within a workgroup.\nArgs: str - Name for the local buffer.\nSources: None.\nStage: Lowering, Codegen.\nNotes: Has a PtrDType with local=True. Requires synchronization (BARRIER).\n\n\n\nDEFINE_ACC\n\nDescription: Defines an accumulator register or variable, typically initialized to an identity element and used within reduction loops.\nPurpose: Holds the intermediate state during reduction operations.\nArgs: (int,) - An accumulator index/identifier.\nSources: (initial_value, *reduce_ranges) - The identity element (CONST) and the RANGE UOps defining the reduction loops.\nStage: Lowering (created from REDUCE_AXIS).\nNotes: Lowered REDUCE_AXIS becomes DEFINE_ACC -&gt; ALU -&gt; ASSIGN(acc).\n\n\n\nLOAD\n\nDescription: Loads data from memory (global or local).\nPurpose: Reading data from a buffer into registers/variables for computation.\nArgs: Optional load configuration (e.g., cache hints, not currently used extensively).\nSources:\n\nOld style (pre-linearize): (BUFFER, VIEW, Optional[STORE]) - Buffer, ShapeTracker view, optional dependency.\nNew style (post-linearize): (INDEX, Optional[alt_value], Optional[gate], Optional[BARRIER]) - Indexed address, value if gate is false, gate condition, barrier dependency.\n\nStage: Lowering, Codegen.\nNotes: Validity checks (from ShapeTracker masks) are incorporated into the INDEX or gate.\n\n\n\nSTORE\n\nDescription: Stores data into memory (global or local).\nPurpose: Writing computation results back to a buffer.\nArgs: None.\nSources:\n\nOld style (pre-linearize): (BUFFER, VIEW, value) - Buffer, ShapeTracker view, value to store.\nNew style (post-linearize): (INDEX, value, Optional[gate]) - Indexed address, value to store, optional gate condition.\n\nStage: Lowering, Codegen.\nNotes: Often the final operation(s) feeding into a SINK.\n\n\n\n\n\nCore Compute / ALU Ops\nThese perform basic element-wise arithmetic, logical, comparison, and transcendental operations. They generally expect sources to have the same shape and dtype (except for comparisons and specific cases like WHERE).\n\nUnary (EXP2, LOG2, SIN, SQRT, RECIP, NEG)\n\nDescription: Apply standard unary mathematical functions.\nPurpose: Element-wise computation.\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: All stages.\nNotes: NEG is often represented as x * -1. Transcendental ops (EXP2, LOG2, SIN) might be rewritten to use approximations or backend-specific implementations.\n\n\n\nBinary (ADD, MUL, IDIV, MAX, MOD, CMPLT, CMPNE, XOR, SHL, SHR, OR, AND, SUB, FDIV, POW)\n\nDescription: Apply standard binary mathematical or logical functions.\nPurpose: Element-wise computation between two operands.\nArgs: None.\nSources: (UOp, UOp) - The two input UOps.\nStage: All stages.\nNotes:\n\nCommutative ops (ADD, MUL, MAX, CMPNE, XOR, AND, OR) might have sources swapped during optimization.\nIDIV is integer division (truncates towards zero). FDIV (internal) represents float division (x / y), often lowered to x * RECIP(y).\nCMPLT/CMPNE output bool dtype.\nSUB is often represented as x + (-y).\n\n\n\n\nTernary (WHERE, MULACC)\n\nDescription: Apply standard ternary functions.\nPurpose: Element-wise computation involving three operands.\nArgs: None.\nSources:\n\nWHERE: (condition, true_value, false_value) - Condition must be bool.\nMULACC: (a, b, c) - Computes a * b + c.\n\nStage: All stages.\nNotes: MULACC (Multiply-Accumulate) can often map efficiently to hardware FMA (Fused Multiply-Add) instructions.\n\n\n\nCAST\n\nDescription: Changes the data type of the elements.\nPurpose: Type conversion (e.g., float to int, int to float, float16 to float32).\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: All stages.\nNotes: Behavior depends on the source and destination types (truncation, rounding, etc.).\n\n\n\nBITCAST\n\nDescription: Reinterprets the bits of the elements as a different data type of the same size.\nPurpose: Low-level manipulation, often used for specific algorithms or interacting with hardware types (e.g., float &lt;-&gt; int).\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: All stages.\nNotes: Does not change the underlying bit pattern, only the type interpretation. Requires source and destination dtypes to have the same itemsize.\n\n\n\n\n\nReduce Ops\n\nREDUCE_AXIS\n\nDescription: Performs a reduction operation (like sum, max) along specified axes.\nPurpose: Aggregates data across dimensions.\nArgs: (Ops, tuple[int, ...]) - The reduction operation (e.g., Ops.ADD, Ops.MAX) and the axes to reduce.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into a combination of DEFINE_ACC, RANGE loops, ALU ops, and ASSIGN. Can be split or grouped during optimization.\n\n\n\nWMMA (Warp Matrix Multiply Accumulate)\n\nDescription: Represents a hardware-accelerated matrix multiplication operation performed cooperatively by a group of threads (warp/wavefront).\nPurpose: Leverages specialized hardware units (like Tensor Cores on NVIDIA GPUs, Matrix Cores on AMD GPUs, AMX on Apple Silicon) for high-performance matrix multiplication.\nArgs: (name, dims, dtype_in, dtype_out, device, threads, upcast_axes, reduce_axes) - Detailed configuration for the WMMA operation.\nSources: (A, B, C) - Input matrices A, B, and accumulator C.\nStage: Codegen (inserted by optimization passes like apply_tensor_cores).\nNotes: Highly backend-specific. Requires specific data layouts and operand types.\n\n\n\n\n\nControl Flow Ops\n\nRANGE\n\nDescription: Represents a loop range, typically used for iterating over tensor dimensions.\nPurpose: Defines the iteration space for loops in the generated code.\nArgs: int - An identifier for the loop variable (axis index).\nSources: (start, end) - UOps defining the start (inclusive) and end (exclusive) of the loop.\nStage: Lowering (created by get_index), Codegen.\nNotes: Rendered as for loops in C-style backends. Used as sources for DEFINE_ACC.\n\n\n\nIF\n\nDescription: Represents a conditional block.\nPurpose: Conditional execution in the generated code.\nArgs: None.\nSources: (condition, Optional[BARRIER]) - The boolean condition UOp and an optional barrier dependency.\nStage: Lowering, Codegen.\nNotes: Requires a corresponding ENDIF. Code between IF and ENDIF is executed only if the condition is true. Used for gating LOAD/STORE.\n\n\n\nENDRANGE / ENDIF\n\nDescription: Marks the end of a RANGE or IF block, respectively.\nPurpose: Defines the scope of loops and conditionals.\nArgs: None.\nSources: (RANGE,) or (IF,) - The corresponding start block UOp.\nStage: Lowering, Codegen.\nNotes: Rendered as closing braces } in C-style backends.\n\n\n\nBARRIER\n\nDescription: Represents a synchronization point, typically for local (shared) memory.\nPurpose: Ensures that all threads in a workgroup reach this point before any thread proceeds, making writes to shared memory visible to other threads.\nArgs: None.\nSources: Usually (STORE,) operations to local memory that need to complete before subsequent reads. Can also be a source for IF.\nStage: Lowering, Codegen.\nNotes: Essential for correctness when using shared memory for reductions or caching.\n\n\n\n\n\nVectorization / Structure Ops\n\nVECTORIZE\n\nDescription: Combines multiple scalar UOps into a single vector UOp.\nPurpose: Explicitly represents vector operations for backends that support them. Also used internally as a structural node (e.g., for VCONST).\nArgs: None.\nSources: (scalar_uop_1, scalar_uop_2, ...) - The scalar UOps to be combined.\nStage: Codegen, Final Rendering.\nNotes: The inverse of GEP. Renderers translate this into vector types and operations if supported.\n\n\n\nUNROLL\n\nDescription: Represents loop unrolling during code generation. Structurally similar to VECTORIZE but used for axes that are fully unrolled rather than vectorized.\nPurpose: Optimization to reduce loop overhead by duplicating the loop body. Also used structurally in Tensor Core lowering.\nArgs: tuple[tuple[int, int], ...] - The axes being unrolled and their sizes ((axis, size), ...).\nSources: (UOp,) - The UOp whose unrolled axes are represented.\nStage: Codegen (inserted by Kernel.apply_opt), Expansion.\nNotes: The expander pass (do_expand) processes UNROLL ops, effectively performing the unrolling by duplicating and adjusting source UOps.\n\n\n\nCONTRACT\n\nDescription: Represents the contraction (summation) part of a vectorized reduction or Tensor Core operation. Inverse of UNROLL for specific axes.\nPurpose: Used structurally during the expansion/contraction passes related to vectorization and Tensor Cores.\nArgs: tuple[tuple[int, int], ...] - The axes being contracted and their sizes ((axis, size), ...).\nSources: (UOp,) - The UOp (often an UNROLL) being contracted.\nStage: Expansion.\n\n\n\nCAT\n\nDescription: Concatenates multiple vector UOps. (Internal use).\nPurpose: Used during expansion/vectorization passes to combine vectors.\nArgs: None.\nSources: Multiple vector UOps.\nStage: Expansion.\nNotes: Lowered to VECTORIZE with GEP sources before rendering.\n\n\n\n\n\nInternal / Removed Early Ops\nThese ops exist briefly during graph construction or early simplification but are typically removed before significant lowering or scheduling.\n\nCONTIGUOUS / CONTIGUOUS_BACKWARD\n\nDescription: Marks a requirement for the data to be in contiguous memory layout. CONTIGUOUS_BACKWARD affects the backward pass only.\nPurpose: Triggers realization or specific memory layouts. Often used before operations that require contiguous inputs (like some COPY operations or external calls).\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: Graph Construction, Early Simplification.\nNotes: Usually removed by simplification rules (sym) if the input is already known to be contiguous or if the operation can be achieved via a VIEW.\n\n\n\nDETACH\n\nDescription: Removes the UOp from the computation graph for gradient calculation purposes.\nPurpose: Implements Tensor.detach().\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: Graph Construction, Early Simplification.\nNotes: Removed by simplification rules (sym).\n\n\n\nBLOCK / BLOCKSTART / BLOCKFORK / BLOCKEND\n\nDescription: Internal ops used by linearize_uop to group UOps into basic blocks based on control flow (RANGE, IF).\nPurpose: Facilitates structuring the UOp list into code blocks for rendering.\nArgs: BasicBlock or int.\nSources: Variable, depending on the block structure.\nStage: Codegen (linearize_uop).\nNotes: These are temporary structural nodes used only within the linearization process and do not appear in the final UOp list passed to the renderer.\n\n\nThis summary covers the primary UOps and their roles. The exact behavior and interactions can be complex, as they are subject to numerous rewrite rules during the compilation process.",
    "crumbs": [
      "Appendix B - UOp Summary"
    ]
  },
  {
    "objectID": "misc_1.html",
    "href": "misc_1.html",
    "title": "Misc 1 - elf.py and the ELF format",
    "section": "",
    "text": "import os\n# os.environ[\"DEBUG\"] = \"7\"\nos.environ[\"CACHELEVEL\"] =\"0\"\nos.environ[\"NOOPT\"] = \"1\"\n\nELF is the standard format for executables and libraries, as well as intermediate object files (.o) on Linux.\nTinyGrad uses it for 2 somewhat distinct roles:\n\nWhen it generates CPU code (clang or LLVM IR backends), it then needs to load the generated binary code into memory to run it.\n\n\nWe could have built a shared library (.so), but then I think we’d have to save it to use dlopen(), and it would not be portable anyway. So instead of this, TinyGrad only does the compilation step (generates a .o) and implements a minimal ELF loader.\n\n\n@uuuuvn also pointed out that doing a proper linking step is slow, and there is a bug in the OSX linker that can’t output to stdout.\n\n\nWhen it generates cuda code, it also comes out as ELF. The sections and relocation rules are different from those found on Linux, but the format is the same.\n\n\nThe CUDA Runtime library would be able to load this object file, and that’s what we do for the CUDA backend, but the NV backend does not rely on the CUDA Runtime libraries, so we parse the ELF manually.\n\nThe loader and relocation code for host (x86_64 and ARM 64) platforms is implemented in tinygrad/runtime/support/elf.py\nSince we only deal with self-contained object files (they don’t access data/function from other files), the task is less danting than you might thnk.\nLet’s first look at the ELF format, at least the bits relevant here:\n\nELF format\n\nELF Header (Elf64_Ehdr)\n\nLocated at the very beginning of the file.\nContains essential metadata:\n\ne_ident: Magic number (\\x7fELF) and info like 64-bit (ELFCLASS64), endianness (ELFDATA2LSB), OS ABI.\ne_type: File type (e.g., ET_REL for relocatable/object file, ET_EXEC for executable, ET_DYN for shared object). elf.py expects ET_REL.\ne_machine: Target architecture (e.g., EM_X86_64, EM_AARCH64).\ne_shoff: Offset to the Section Header Table.\ne_shnum: Number of entries in the Section Header Table.\ne_shstrndx: Index of the section header table entry that contains section names.\n\n\n\n\nSection Header Table:\n\nAn array of Elf64_Shdr structures.\nEach entry describes a section in the file.\nKey Elf64_Shdr fields used by elf.py:\n\nsh_name: Offset into the Section Name String Table (.shstrtab) for this section’s name.\nsh_type: Type of section (e.g., SHT_PROGBITS for code/data, SHT_SYMTAB for symbols, SHT_RELA/SHT_REL for relocations, SHT_STRTAB for strings, SHT_NOBITS for .bss).\nsh_flags: Attributes like SHF_ALLOC (load into memory), SHF_WRITE, SHF_EXECINSTR.\nsh_addr: The intended virtual memory address during execution. For .o files (type ET_REL), this is often 0 for most sections, as the final address isn’t known yet. elf.py updates this for sections it places.\nsh_offset: Offset of the section’s content within the ELF file itself.\nsh_size: Size of the section’s content in the file (or memory size for SHT_NOBITS).\nsh_link, sh_info: Used by specific section types (e.g., .symtab uses sh_link to point to its string table).\nsh_addralign: Required alignment constraint for the section’s start address.\nsh_entsize: Size of each entry if the section holds a table (like symbol table or relocation table).\n\n\n\n\nSections:\n\nContiguous chunks of bytes (or just metadata for SHT_NOBITS) described by the Section Header Table.\nCommon sections relevant here:\n\n.text: Executable code (SHT_PROGBITS).\n.data: Initialized global/static variables (SHT_PROGBITS).\n.rodata: Read-only data (constants, strings) (SHT_PROGBITS).\n.bss: Uninitialized global/static variables (SHT_NOBITS). Occupies no space in the file but needs space allocated in memory. (elf.py doesn’t explicitly handle .bss size allocation based on sh_size, it only lays out SHT_PROGBITS).\n.symtab: Symbol Table (SHT_SYMTAB). Lists defined and referenced symbols (functions, variables).\n.strtab: String Table (SHT_STRTAB). Stores names for symbols.\n.shstrtab: Section Header String Table (SHT_STRTAB). Stores names for sections.\n.rela.text, .rel.text, etc.: Relocation sections (SHT_RELA, SHT_REL). Contain instructions for patching code/data.\n\n\n\n\n\nRelocations\nEven for a self-contained file, the compiler generates an object file with symbols (constants, global vars, functions) that don’t have their final location assigned. This code might:\n\nAccess a function defined elsewhere in the same file (e.g., a helper function in another part of .text).\nReference constants or static data (e.g., accessing a string literal in .rodata or a static array in .data).\n\n\nWhen generating CPU code, PC-relative addressing are used, so the code does not care where it will be placed in memory for execution. Still, the code will refernce stuff in .rodata, so it needs to know the address.\n\nTinyGrad (at least for now) does not generate code with subroutines or global variables, which makes out task easier.\nRelocations are metadata entries that tell our loader how to patch the machine code once we’ve determined where each section will be placed in memory. For self-contained files, we only need to resolve references within the same object file, not external dependencies.\nThere are 2 types of relocation entries, REL and RELA:\n\nFor rel, the instruction will contain the address of relative to Program Counter (PC), so we will read this address, add the offset, and patch the instruction.\nFor rela, the instruction contains zeros (or any value really) for the address, and the rela entry contains the address. We add offset to it, and rewrite the value from the instruction.\n\n\nRelocation Sections:\n\nContain arrays of relocation entries.\nThe section name indicates which other section the relocations apply to (e.g., .rela.text contains patches for the .text section). elf.py uses sh.name[5:] or sh.name[4:] to find the target section name.\nThere are two main types:\n\nSHT_RELA: Entries contain an explicit addend. (Elf64_Rela)\nSHT_REL: Entries have an implicit addend (usually the value already present at the location being patched). (Elf64_Rel)\n\n\n\n\nRelocation Entries (Elf64_Rela / Elf64_Rel):\nEach entry describes a single patch. Key fields:\n\nr_offset: The offset within the target section where the patch needs to be applied. Let’s call the final address P = section_base + r_offset.\nr_info: A combined field containing:\n\nSymbol Index (ELF64_R_SYM(r_info)): An index into the .symtab section. This identifies the symbol whose address is needed for the calculation. Let the symbol’s final address be S.\nRelocation Type (ELF64_R_TYPE(r_info)): Specifies how to calculate the value to be patched and how to insert it at P. This is architecture-specific. Examples:\n\nFor X86_64, we only support R_X86_64_PC32, calculate S + A - P (Symbol + Addend - PatchLocation) and store the 32-bit result.\nFor ARM 64, we have more options, and we actually have to patch some bits in the target instruction.\n\n\nr_addend (Elf64_Rela only): An explicit constant value (A) to be used in the relocation calculation (e.g., S + A - P). For Elf64_Rel, the addend A is implicitly the value already stored in the instruction/data at P before patching.\n\n\n\n\nRunning code on CPU\nLet’s look at an example:\nelftest.c\n\nfloat foo(int x) { return x + 12345678.f; } // I use float because an int const stays in .text for some reason\n\n\n!clang -c -x c -march=native --target=x86_64-none-unknown-elf -O0 -fPIC \\\n    -ffreestanding -fno-math-errno -fno-ident -nostdlib elftest.c -o elftest.o\n\n\n!objdump -r elftest.o\n\n\nelftest.o:     file format elf64-x86-64\n\nRELOCATION RECORDS FOR [.text]:\nOFFSET           TYPE              VALUE\n0000000000000010 R_X86_64_PC32     .LCPI0_0-0x0000000000000004\n\n\n\n\nWe have a single relocation entry, it’s for that float constant that went onto .rodata.\n\n!objdump -d elftest.o\n\n\nelftest.o:     file format elf64-x86-64\n\n\nDisassembly of section .text:\n\n0000000000000000 &lt;foo&gt;:\n   0:   55                      push   %rbp\n   1:   48 89 e5                mov    %rsp,%rbp\n   4:   89 7d fc                mov    %edi,-0x4(%rbp)\n   7:   c5 fa 2a 45 fc          vcvtsi2ssl -0x4(%rbp),%xmm0,%xmm0\n   c:   c5 fa 10 0d 00 00 00    vmovss 0x0(%rip),%xmm1        # 14 &lt;foo+0x14&gt;\n  13:   00 \n  14:   c5 fa 58 c1             vaddss %xmm1,%xmm0,%xmm0\n  18:   5d                      pop    %rbp\n  19:   c3                      ret\n\n\nYou can see for that vmovss instruction, the address part is currently 00 00 00 00. The address part is located at 0x10, which matches the offset in the relocation table.\nLet’s look at the .rodata.cst4 section. The .cst4 means this .rodata sectoin is for 4-byte constants.\n\n!objdump -s -j .rodata.cst4 elftest.o\n\n\nelftest.o:     file format elf64-x86-64\n\nContents of section .rodata.cst4:\n 0000 4e613c4b                             Na&lt;K            \n\n\nMust be the float\n\nimport numpy as np\nhex(np.float32(12345678.).view(np.uint32))\n\n'0x4b3c614e'\n\n\nYes it is.\nThe bytes are in reverse order because x86 is a little-endian architecture.\nLet’s load it with elf_loader\n\nfrom tinygrad.runtime.support.elf import elf_loader, relocate\n\n\nwith open('elftest.o', 'rb') as f:\n    elf_bytes = f.read()\n\nimage, sections, relocs = elf_loader(elf_bytes)\n\nprint(f\"Image size: {len(image)} bytes\")\nprint(\"\\nSections:\")\nfor i, section in enumerate(sections):\n    print(f\"  {i}: {section.name} (size: {section.header.sh_size}, addr: {hex(section.header.sh_addr)})\")\n\nif relocs:\n    print(\"\\nRelocations:\")\n    for i, (offset, target, r_type, addend) in enumerate(relocs):\n        print(f\"  {i}: offset={hex(offset)}, target={hex(target)}, type={r_type}, addend={addend}\")\n\nImage size: 32 bytes\n\nSections:\n  0:  (size: 0, addr: 0x0)\n  1: .strtab (size: 94, addr: 0x0)\n  2: .text (size: 26, addr: 0x0)\n  3: .rela.text (size: 24, addr: 0x0)\n  4: .rodata.cst4 (size: 4, addr: 0x1c)\n  5: .note.GNU-stack (size: 0, addr: 0x20)\n  6: .llvm_addrsig (size: 0, addr: 0x0)\n  7: .symtab (size: 96, addr: 0x0)\n\nRelocations:\n  0: offset=0x10, target=0x1c, type=2, addend=-4\n\n\nDisassemble the code in the image:\n\nfrom tinygrad.helpers import capstone_flatdump\n\ncapstone_flatdump(image[:0x1a])\n\n0x000000: push  rbp\n0x000001: mov   rbp, rsp\n0x000004: mov   dword ptr [rbp - 4], edi\n0x000007: vcvtsi2ss xmm0, xmm0, dword ptr [rbp - 4]\n0x00000c: vmovss    xmm1, dword ptr [rip]\n0x000014: vaddss    xmm0, xmm0, xmm1\n0x000018: pop   rbp\n0x000019: ret   \n\n\nThe .rodata was added jught after .text (with 2 bytes to align it on a 4-byte boundary)\n\nimage[0x1c:0x20].hex()\n\n'4e613c4b'\n\n\nNow we can apply the relocations. This is pretty much what elf.py:jit_loader does.\n\nNote: jit_loader is not directly related to TinyJit, JIT is an overloaded term, and we compile the kernels … just in time.\n\n\nimport struct\nfor ploc,tgt,r_type,r_addend in relocs:\n    print(f\"Relocating at address {hex(ploc)}, PC at {hex(ploc+4)}\")\n    print(f\"Before: 0x{image[ploc:ploc+4].hex()}\")\n    image[ploc:ploc+4] = struct.pack(\"&lt;I\", relocate(struct.unpack(\"&lt;I\", image[ploc:ploc+4])[0], ploc, tgt+r_addend, r_type))\n    print(f\"After:  0x{image[ploc:ploc+4].hex()}\")\n\nRelocating at address 0x10, PC at 0x14\nBefore: 0x00000000\nAfter:  0x08000000\n\n\nThe 0x08000000 is actually just 8, keep the endianess in mind. While we are patching at address 0x10, the PC must be at the next instruction already, 0x14, skipping the 4 byes of 00 00 00 00.\n\nhex(0x14 + 8)\n\n'0x1c'\n\n\nAnd this is indeed the address of the constant. Now, let’s run the code. This is normally done in device.py:CPUProgram.__init__\n\nfrom tinygrad.helpers import mv_address\nfrom tinygrad.device import CPUProgram\nimport ctypes\nfrom mmap import mmap, PROT_READ, PROT_WRITE, PROT_EXEC, MAP_ANON, MAP_PRIVATE\n\nmem = mmap(-1, len(image), MAP_ANON | MAP_PRIVATE, PROT_READ | PROT_WRITE | PROT_EXEC)\nmem.write(image)\n\nCPUProgram.rt_lib[\"__clear_cache\"](ctypes.c_void_p(mv_address(mem)), ctypes.c_void_p(mv_address(mem) + len(image)))\n\nfxn = ctypes.CFUNCTYPE(ctypes.c_float)(mv_address(mem))\n\n# CPUProgram.__call__\nfxn(ctypes.c_int32(321))\n\n12345999.0\n\n\n\nAllocate a piece of memory, and make it executable. We can’t just mark our image as executable, because the flags are applied to memory pages (4096 bytes), and our image is not aligned to page boundary.\nCopy the image (lib) into that memory.\nClear instruction cache. I’m not entirely sure how the instruction cache works on different architectures, I guess it might be required at least on some architecures. Skipping this step did not cause issues for me.\nWrap it in a python function using ctypes and call it.\n\n\n\nNV backend\nNow, we also use the same loader with the NV backend without relying on the CUDA Runtime library to load it. Let’s take a look at ops_nv.py:NVProgram\n    image, sections, relocs = elf_loader(self.lib, force_section_align=128)\n    self.lib_gpu = self.dev.allocator.alloc(round_up(image.nbytes, 0x1000) + 0x1000, BufferSpec(cpu_access=True))\nLoad the elf and allocate a buffer on the GPU.\n\nNote: We use the CUDA Unified Memory, so the buffer is mapped at the same vitual address on both the host and the GPU. I did not know this was even possible.\n\n    self.prog_addr, self.prog_sz, self.regs_usage, self.shmem_usage, self.lcmem_usage = self.lib_gpu.va_addr, image.nbytes, 0, 0x400, 0\n    self.constbufs: dict[int, tuple[int, int]] = {0: (0, 0x160)} # dict[constbuf index, tuple[va_addr, size]]\n    for sh in sections:\n      if sh.name == f\".nv.shared.{self.name}\": self.shmem_usage = round_up(0x400 + sh.header.sh_size, 128)\n      if sh.name == f\".text.{self.name}\":\n        self.prog_addr, self.prog_sz, self.regs_usage = self.lib_gpu.va_addr+sh.header.sh_addr, sh.header.sh_size, max(sh.header.sh_info&gt;&gt;24, 16)\n      elif m:=re.match(r'\\.nv\\.constant(\\d+)', sh.name): self.constbufs[int(m.group(1))] = (self.lib_gpu.va_addr+sh.header.sh_addr, sh.header.sh_size)\n      elif sh.name.startswith(\".nv.info\"):\n        for typ, param, data in self._parse_elf_info(sh):\n          if sh.name == f\".nv.info.{name}\" and param == 0xa: cbuf0_size = struct.unpack_from(\"IH\", data)[1] # EIATTR_PARAM_CBANK\n          elif sh.name == \".nv.info\" and param == 0x12: self.lcmem_usage = struct.unpack_from(\"II\", data)[1] + 0x240 # EIATTR_MIN_STACK_SIZE\nExtract information from the Nvidia-specific headers, like the size of shared memory, registers, etc, used by the kernel.\n    for apply_image_offset, rel_sym_offset, typ, _ in relocs:\n      # These types are CUDA-specific, applying them here\n      if typ == 2: image[apply_image_offset:apply_image_offset+8] = struct.pack('&lt;Q', self.lib_gpu.va_addr + rel_sym_offset) # R_CUDA_64\n      elif typ == 0x38: image[apply_image_offset+4:apply_image_offset+8] = struct.pack('&lt;I', (self.lib_gpu.va_addr + rel_sym_offset) & 0xffffffff)\n      elif typ == 0x39: image[apply_image_offset+4:apply_image_offset+8] = struct.pack('&lt;I', (self.lib_gpu.va_addr + rel_sym_offset) &gt;&gt; 32)\n      else: raise RuntimeError(f\"unknown NV reloc {typ}\")\nApply relocations. Nvidia has its own types of relocations for different addressing modes on GPU.\n\nNote that va_addr is valid on both the host and the GPU.\n\n    ctypes.memmove(self.lib_gpu.va_addr, mv_address(image), image.nbytes)\nCopy the relocated image into the buffer.\nA bit more Nvidia magic needs to happen to kick off the execution, but you get the idea.",
    "crumbs": [
      "Misc 1 - elf.py and the ELF format"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My notes on TinyGrad internals",
    "section": "",
    "text": "Tinygrad source: https://github.com/tinygrad/tinygrad\nTinygrad docs: https://docs.tinygrad.org/\nWritten with help from good people in TinyGrad Discord, and a great deal of inspiration has been taken from mesozoic-egg.\n\nChapters\n\n0 - Introduction\n1 - UOps\n2 - View and ShapeTracker\n3 - The Pattern Matcher\n4 - The .arange() insanity\n\n\n\nAppendices\n\nA: helpers.py\nB: UOp Summary\n\n\n\nMiscellaneous\n\n1 - elf.py and the ELF format\n2 - Misc 2 - CUDA Runtime library\n\nReading order:\n\nScroll through Appendix A after reading the intro. It will make reading the code much easier\nScroll through Appendix B after reading the chapter of UOps\n\nIf you want to contribute, install nbdev\nRun\nnbdev_clean\nnbdev_docs",
    "crumbs": [
      "My notes on TinyGrad internals"
    ]
  },
  {
    "objectID": "shapetracker.html",
    "href": "shapetracker.html",
    "title": "2 - View and ShapeTracker",
    "section": "",
    "text": "So far we have been making scalar UOps that don’t have a shape associated with them.\nWhile we have been getting away with it so far, the UOp trees we made are not really valid without a shape.\nimport os\nos.environ[\"CPU\"] = \"1\"\nos.environ[\"DEBUG\"]=\"4\"\n\nfrom tinygrad import  dtypes\nfrom tinygrad.ops import UOp, Ops\na = UOp.const(dtypes.float, 1)\na\n\nUOp(Ops.CONST, dtypes.float, arg=1.0, src=())\ntry:\n    print(a.shape)\nexcept Exception as e:\n    print_last_frame_context(e)\n\nAssertionError in /home/xl0/work/projects/grads/tinygrad/tinygrad/helpers.py:61 in unwrap()\n\nCode context:\n       59   return ret\n       60 def unwrap(x:Optional[T]) -&gt; T:\n---&gt;   61   assert x is not None\n       62   return x\n       63 def get_single_element(x:list[T]) -&gt; T:\nAnother thing we were missing is the device:\ntry:\n    print(a.device)\nexcept Exception as e:\n    print_last_frame_context(e)\n\nAssertionError in /home/xl0/work/projects/grads/tinygrad/tinygrad/helpers.py:61 in unwrap()\n\nCode context:\n       59   return ret\n       60 def unwrap(x:Optional[T]) -&gt; T:\n---&gt;   61   assert x is not None\n       62   return x\n       63 def get_single_element(x:list[T]) -&gt; T:\nLet’s fix this real quick\nfrom tinygrad.shape.shapetracker import ShapeTracker, View\n\na = UOp.const(dtypes.float, 1).replace(src=(\n        UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker.from_shape( (0,) ), src=(\n            UOp(Ops.DEVICE, dtypes.void, arg=\"CPU\", src=()),)),))\na\n\nUOp(Ops.CONST, dtypes.float, arg=1.0, src=(\n  UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(0,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=(\n    UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),))\na.shape\n\n(0,)\na.device\n\n'CPU'\nLooks better.\nNow, what’s up with that ShapeTracker and View. Let’s start with the later.",
    "crumbs": [
      "2 - View and ShapeTracker"
    ]
  },
  {
    "objectID": "shapetracker.html#shape-and-stride",
    "href": "shapetracker.html#shape-and-stride",
    "title": "2 - View and ShapeTracker",
    "section": "Shape and Stride",
    "text": "Shape and Stride\nYou are probably familiar with how shape and strides work in Pytorch or Numpy:\n\nimport torch\n\n\na = torch.linspace(0, 31, 32, dtype=torch.int32).view(4, 8)\na\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31]], dtype=torch.int32)\n\n\n\na.shape\n\ntorch.Size([4, 8])\n\n\nThe shape defined the, well, the shape of the array. It can have any number of dimensions (2 in this case), and each dimension has its size.\nA Tensor is just a linear array, and the shape is there for convenience, because we usually want to work with multi-dimensional data.\nWe can change the shape, as long as the number of elements in the new shape stays the same.\n\nb = a.view(2,4,4) # This creates a view that refers to the same data, but now it's seen as a 3-d array.\nb\n\ntensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15]],\n\n        [[16, 17, 18, 19],\n         [20, 21, 22, 23],\n         [24, 25, 26, 27],\n         [28, 29, 30, 31]]], dtype=torch.int32)\n\n\nThe stride tells us how many elements do we need to move in the underlying 1-d array (base), to get to the next element in the given dimension.\nFor out 2x4x4 array, to move 1 element in the row (last dimension), we need to move … 1 element in the base.\nAnd to move by one element in the column dimension, we need to move by 4 elements in the base, because each row is 4 elements.\n\nThis is the standard C, or row-major order format for multidimensional data.\n\n\n\n\n\nRow-major and Column-major order\n\n\n\n\nYou might have seen references to the F, or column-major order at some point. Historically this is how data was stored in Fortran, and I’m sure they had their reasons for it, but it’s definitely less intuitive.\n\nTo move in the next dimension, we’d have to skip 4 columns, and for each column we skip 4 elements, so 16 in total:\n\nb.stride()\n\n(16, 4, 1)\n\n\nNow, if the stride always matched the shape, things would be boring. We can set the stride independently.\nLet’s go back to our 4x8 view to make things easies. In this case we need to skip 8 elements to move by one row:\n\nprint(a)\nprint(\"Shape: \",a.shape)\nprint(\"Stride:\",a.stride())\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31]], dtype=torch.int32)\nShape:  torch.Size([4, 8])\nStride: (8, 1)\n\n\nWhat if we want to create a view that would skip every other element in the rows? We can do this by creatig a view with shape (torch refers to it as size) 4x4, and stride (8, 2)!\n\nc = a.as_strided(size=(4,4), stride=(8, 2))\nc\n\ntensor([[ 0,  2,  4,  6],\n        [ 8, 10, 12, 14],\n        [16, 18, 20, 22],\n        [24, 26, 28, 30]], dtype=torch.int32)\n\n\nWe can also specify an offset from the start of the base array. This will give us the odd elements in each row:\n\na.as_strided(size=(4,4), stride=(8, 2), storage_offset=1)\n\ntensor([[ 1,  3,  5,  7],\n        [ 9, 11, 13, 15],\n        [17, 19, 21, 23],\n        [25, 27, 29, 31]], dtype=torch.int32)\n\n\nLet’s create a view that has the diagonal elements of a (0, 9, 18, 27)\n\na.as_strided(size=(4,), stride=(9,))\n\ntensor([ 0,  9, 18, 27], dtype=torch.int32)\n\n\nAnother fun thing we can do - set one of more of the strides to 0, to duplicate (broadcast) dimensions:\n\nd = torch.linspace(1, 4, 4, dtype=torch.int32)\nd\n\ntensor([1, 2, 3, 4], dtype=torch.int32)\n\n\n\nd.as_strided(size=(4,4), stride=(1, 0))\n\ntensor([[1, 1, 1, 1],\n        [2, 2, 2, 2],\n        [3, 3, 3, 3],\n        [4, 4, 4, 4]], dtype=torch.int32)\n\n\nFor each step in the output column, we take 1 step in the base, and for each step in the output row, we don’t take any steps at all!\nThat’s how .full() works - it creates 1 single element, and makes all elements in the Tensor refer to it by setting the strides to 0.\n\ne = torch.Tensor([1]).to(torch.int32)\ne\n\ntensor([1], dtype=torch.int32)\n\n\n\ne.as_strided(size=(4,4), stride=(0,0))\n\ntensor([[1, 1, 1, 1],\n        [1, 1, 1, 1],\n        [1, 1, 1, 1],\n        [1, 1, 1, 1]], dtype=torch.int32)",
    "crumbs": [
      "2 - View and ShapeTracker"
    ]
  },
  {
    "objectID": "shapetracker.html#class-view",
    "href": "shapetracker.html#class-view",
    "title": "2 - View and ShapeTracker",
    "section": "class View",
    "text": "class View\nThe View class is intended to keep track of the shape and stride of the data. Let’s play with it a bit.\n\nv = View(shape=(4,8), strides=(8,1), offset=0, mask=None, contiguous=True)\nv # A normal array 4x8\n\nView(shape=(4, 8), strides=(8, 1), offset=0, mask=None, contiguous=True)\n\n\n\na\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31]], dtype=torch.int32)\n\n\n\na.as_strided(size=v.shape, stride=v.strides)\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31]], dtype=torch.int32)\n\n\n\nv32 = v.reshape( (32,) ) # 1-d array of 32 elements\nv32\n\nView(shape=(32,), strides=(1,), offset=0, mask=None, contiguous=True)\n\n\n\na.as_strided(size=v32.shape, stride=v32.strides)\n\ntensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n       dtype=torch.int32)\n\n\n\nv_flip = v.flip( (False, True) ) # Flip the last dimension\nv_flip\n\nView(shape=(4, 8), strides=(8, -1), offset=7, mask=None, contiguous=False)\n\n\n\ntry:\n    a.as_strided(size=v_flip.shape, stride=v_flip.strides)\nexcept Exception as e:\n    print_last_frame_context(e, 0)\n\nRuntimeError in /tmp/ipykernel_795477/772861321.py:2 in &lt;module&gt;()\n\nCode context:\n---&gt;    2     a.as_strided(size=v_flip.shape, stride=v_flip.strides)\n\n\nOops, torch actually does not support negative strides. This should have looked like this:\n\na.flip((1))\n\ntensor([[ 7,  6,  5,  4,  3,  2,  1,  0],\n        [15, 14, 13, 12, 11, 10,  9,  8],\n        [23, 22, 21, 20, 19, 18, 17, 16],\n        [31, 30, 29, 28, 27, 26, 25, 24]], dtype=torch.int32)\n\n\n\nThe Mask\nNow, what’s up with the mask? It allows us to create arrays with elements that are outside of the underlying storage!\nFor example, if we want to pad a 2-d array, we don’t want to allocate a new array - just mark the padded elements as being not valid!\n\nv\n\nView(shape=(4, 8), strides=(8, 1), offset=0, mask=None, contiguous=True)\n\n\n\nv.pad(((2,2,),(2,2))) # left-right, top-bottom\n\nView(shape=(8, 12), strides=(8, 1), offset=-18, mask=((2, 6), (2, 10)), contiguous=False)\n\n\nTorch does not allow negative offsets either, but I think the idea is clear:",
    "crumbs": [
      "2 - View and ShapeTracker"
    ]
  },
  {
    "objectID": "shapetracker.html#class-shapetracker",
    "href": "shapetracker.html#class-shapetracker",
    "title": "2 - View and ShapeTracker",
    "section": "class ShapeTracker",
    "text": "class ShapeTracker\nNot all transforms can be represented with a single View.\n\nv = View.create((3,2))\nv_padded = v.pad(((1,1),(1,1)))\nv_padded\n\nView(shape=(5, 4), strides=(2, 1), offset=-3, mask=((1, 4), (1, 3)), contiguous=False)\n\n\n\nv_padded_reshaped = v_padded.reshape((20,)) # Linearize into a 1-d array\nprint(v_padded_reshaped)\n\nNone\n\n\nOops, we get a None, which means the operation could not be performed!\nIt makes sense, because we can’t specify the valid mask of the linearized result using just the start/stop indices.\nThe ShapeTracker keeps a list of sequentially applied Views.\n\nst = ShapeTracker((v,))\nst\n\nShapeTracker(views=(View(shape=(3, 2), strides=(2, 1), offset=0, mask=None, contiguous=True),))\n\n\nIf some of the views can be merged together, it will do so.\nIn this example, we reshaped 3x2 -&gt; 2x3 and flipped it along the first axis and padded it on all sides by 1.\nThis can be represented with a single View:\n\nst_rfp = st.reshape((2,3)).flip((True, False)).pad( ((1,1),(1,1)) )\nst_rfp\n\nShapeTracker(views=(View(shape=(4, 5), strides=(-3, 1), offset=5, mask=((1, 3), (1, 4)), contiguous=False),))\n\n\nIf the transformation can not be represented with a single View, the Shapetracker will keep them separate\n\nst_rfp.reshape( (20,) )\n\nShapeTracker(views=(View(shape=(4, 5), strides=(-3, 1), offset=5, mask=((1, 3), (1, 4)), contiguous=False), View(shape=(20,), strides=(1,), offset=0, mask=None, contiguous=True)))\n\n\nWe can also generate the UOp trees that represent the expressoins for indexing and valudating memory acces in the code:\n\nidx, valid = st_rfp.to_indexed_uops()\nidx\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.ADD, dtypes.int, arg=None, src=(\n    UOp(Ops.MUL, dtypes.int, arg=None, src=(\n      UOp(Ops.RANGE, dtypes.int, arg=0, src=(\n        x3:=UOp(Ops.CONST, dtypes.int, arg=0, src=()),\n        UOp(Ops.CONST, dtypes.int, arg=4, src=()),)),\n      UOp(Ops.CONST, dtypes.int, arg=-3, src=()),)),\n    UOp(Ops.RANGE, dtypes.int, arg=1, src=(\n       x3,\n      x7:=UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),)),\n   x7,))\n\n\n\nvalid\n\nUOp(Ops.AND, dtypes.bool, arg=None, src=(\n  UOp(Ops.AND, dtypes.bool, arg=None, src=(\n    UOp(Ops.AND, dtypes.bool, arg=None, src=(\n      UOp(Ops.CMPNE, dtypes.bool, arg=None, src=(\n        UOp(Ops.CMPLT, dtypes.bool, arg=None, src=(\n          x4:=UOp(Ops.RANGE, dtypes.int, arg=0, src=(\n            x5:=UOp(Ops.CONST, dtypes.int, arg=0, src=()),\n            x6:=UOp(Ops.CONST, dtypes.int, arg=4, src=()),)),\n          x7:=UOp(Ops.CONST, dtypes.int, arg=1, src=()),)),\n        x8:=UOp(Ops.CONST, dtypes.bool, arg=True, src=()),)),\n      UOp(Ops.CMPLT, dtypes.bool, arg=None, src=(\n         x4,\n        UOp(Ops.CONST, dtypes.int, arg=3, src=()),)),)),\n    UOp(Ops.CMPNE, dtypes.bool, arg=None, src=(\n      UOp(Ops.CMPLT, dtypes.bool, arg=None, src=(\n        x13:=UOp(Ops.RANGE, dtypes.int, arg=1, src=(\n           x5,\n          UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),\n         x7,)),\n       x8,)),)),\n  UOp(Ops.CMPLT, dtypes.bool, arg=None, src=(\n     x13,\n     x6,)),))\n\n\nLet’s use .render() to try and convert the UOps into equivalent C expressions:\n\nNote: .render() is not how TinyGrad normally generates the code, it’s for debug purpose only.\n\n\nidx.render()\n\n'(((ridx0*-3)+ridx1)+5)'\n\n\nAnd valid can be used to check the validity of input elements in and if statement:\n\nvalid.render()\n\n'(((((ridx0&lt;1)!=True)&(ridx0&lt;3))&((ridx1&lt;1)!=True))&(ridx1&lt;4))'",
    "crumbs": [
      "2 - View and ShapeTracker"
    ]
  },
  {
    "objectID": "uops.html",
    "href": "uops.html",
    "title": "1 - UOps",
    "section": "",
    "text": "As we saw in the previous chapter, UOps are the intermediate device-independent representation on the computation tree that sits between the user-facing Tensor and device-specific code that is generated to perform the computations.\n\nimport os\n\nos.environ[\"CPU\"] = \"1\"\n# os.environ[\"TRACEMETA\"] = \"0\"\nos.environ[\"DEBUG\"]=\"4\"\n# os.environ[\"NOOPT\"]=\"1\"\n\n\nimport tinygrad as tg\nfrom tinygrad import Tensor, dtypes\n\nUOp is the basic building block of TinyGrad, used to represent the computation tree that we create by manipulating Tensors:\nclass UOp(...):\n    op:Ops\n    dtype:DType = dtypes.void\n    src:tuple[UOp, ...] = tuple()\n    arg:Any = None\n\n    ...\n\nThe op is the type of the operation, like ADD, MUL or CONST\nThe dtype is of of the tinygrad dtypes, like dtypes.float32 or dtypes.uint8\nThe source is a tuple of UOps this UOp operates on\nThe arg changes meaning depending on the op, for example for a CONST it can be a number 3.0, or it can be the new shape for a VIEW op.\n\n\nUOp reference\nUOps are used throughout TinyGrad, some are specific to certain stages of processing (from Tensors to code), some are valid at any stage.\nTake a look at the UOp Reference to get a feel for the UOps we have.\n\n\nUOp is a singleton\nAs noted by mesozoic-egg@github, UOp is a singleton.\nIt’s implemented using a MetaClass:\nclass UOpMetaClass(type):\n  ucache:dict[tuple, weakref.ReferenceType[UOp]] = {}\n  def __call__(cls, op:Ops, dtype:DType=dtypes.void, src:tuple[UOp,...]=tuple(), arg:Any=None, _buffer:Buffer|None=None):\n    if (wret:=UOpMetaClass.ucache.get(key:=(op, dtype, src, arg), None)) is not None and (ret:=wret()) is not None: return ret\n    UOpMetaClass.ucache[key] = ref = weakref.ref(created:=super().__call__(*key))\n    ...\n    return created\n\n@dataclass(eq=False, slots=True)\nclass UOp(MathTrait, metaclass=UOpMetaClass):\n    def __del__(self):\n        if (ref:=UOpMetaClass.ucache.get(k:=(self.op, self.dtype, self.src, self.arg))) is not None:\n            ...\n            del UOpMetaClass.ucache[k]\n(TinyGrad really loves its := operators)\nThe main idea is, if you have 2 UOp (sub-)trees, it’s very easy to compare them, because the roots of both trees will be the same object if they are identical.\n\nfrom tinygrad.ops import UOp, Ops\n\n\n# Create two identical UOp trees (3 * 5 + 2)\nx1 = UOp(Ops.CONST, dtype=dtypes.int, arg=5)\nmul1 = UOp(Ops.MUL, dtype=dtypes.int, src=(UOp(Ops.CONST, dtype=dtypes.int, arg=3), x1))\nadd1 = UOp(Ops.ADD, dtype=dtypes.int, src=(mul1, UOp(Ops.CONST, dtype=dtypes.int, arg=2)))\n\n# Second tree\nx2 = UOp(Ops.CONST, dtype=dtypes.int, arg=5)\nmul2 = UOp(Ops.MUL, dtype=dtypes.int, src=(UOp(Ops.CONST, dtype=dtypes.int, arg=3), x2))\nadd2 = UOp(Ops.ADD, dtype=dtypes.int, src=(mul2, UOp(Ops.CONST, dtype=dtypes.int, arg=2)))\n\nid(add1) == id(add2)\n\nTrue\n\n\n\n# Third tree is different (3 * 5 + 1)\nx3 = UOp(Ops.CONST, dtype=dtypes.int, arg=5)\nmul3 = UOp(Ops.MUL, dtype=dtypes.int, src=(UOp(Ops.CONST, dtype=dtypes.int, arg=3), x3))\nadd3 = UOp(Ops.ADD, dtype=dtypes.int, src=(mul3, UOp(Ops.CONST, dtype=dtypes.int, arg=1)))\n\nid(add1) == id(add3)\n\nFalse\n\n\n\n\nSymbolic evaluation\nAnother cool feature of UOps - if all inputs are constants and the result is a scalar, it can be evaluated without generating any device code at all:\n\nadd1\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=3, src=()),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),\n  UOp(Ops.CONST, dtypes.int, arg=2, src=()),))\n\n\n\nadd1.simplify()\n\nUOp(Ops.CONST, dtypes.int, arg=17, src=())\n\n\nAnother way to do the same - cast the UOp to float or an int depending on dtype.\n\nint(add1)\n\n17\n\n\n\n\nUOp.render()\n.render() converts the UOp tree into a C expression.\nThis is not how TinyGrad generates the code normally, but it’s useful for debugging.\n\nadd1.render(simplify=False)\n\n'((3*5)+2)'\n\n\nThis only works for a limited subset of trees with simple operations though.\nIf the tree can not be rendered, the function returns an str(op).\n\n\nUOp creation helpers\nIn many cases, the UOp class has methods for creating specific UOps. It’s often more convenient and concise to use them\nFor example UOp.const() creates either a CONST or a VCONST (vector const, used internally for buffers), and also takes care of the arg type matching dtype:\n\nUOp.const(dtypes.float16, 2)\n\nUOp(Ops.CONST, dtypes.half, arg=2.0, src=())\n\n\nNote the arg has been converted to a float, even though we gave it an int\nThere are a few that are very straight-forward:\n\n# The SINK is the end of a computation graph\ndef sink(self, *srcs:UOp): return UOp(Ops.SINK, dtypes.void, (self,)+srcs)\n\n# Detach from the backprop\ndef detach(self): return UOp(Ops.DETACH, self.dtype, (self,))\n\ndef cast(self, dtype:DType): return UOp(Ops.CAST, dtype, (self,))\ndef bitcast(self, dtype:DType): return UOp(Ops.BITCAST, dtype, (self,))\ndef load(self, *src:UOp, **kwargs): return UOp(Ops.LOAD, src=(self,)+src, **kwargs)\ndef store(self, *src:UOp, **kwargs): return UOp(Ops.STORE, dtypes.void, (self,)+src, **kwargs)\n\n# The RANGE is actually a `for` loop\ndef range(dtype:DType, start:sint, end:sint, idx:int): return UOp(Ops.RANGE, dtype=dtype, src=(sint_to_uop(start), sint_to_uop(end)), arg=idx)\n\ndef assign(self, x:UOp): return UOp(Ops.ASSIGN, self.dtype, (self,x))\ndef contiguous(self): return self.alu(Ops.CONTIGUOUS)\ndef contiguous_backward(self): return self.alu(Ops.CONTIGUOUS_BACKWARD)\n\n\nToposort\nQuite often we need to access a UOp tree in “topological order”.\nUOp.toposort is a property function that returns a dictionary with UOps being the keys, and the values being None.\n\nNote: This emulates a sorted Set, which Python lacks\n\n\nprint(\"===== 3 * 5 + 2 =====\")\nfor o in add1.toposort.keys():\n    print(o.op, o.arg)\n\n===== 3 * 5 + 2 =====\nOps.CONST 3\nOps.CONST 5\nOps.MUL None\nOps.CONST 2\nOps.ADD None\n\n\nYou get the idea - the children always come before the parents\n\n\nOther UOp methods\nWhen reading the Tiny Grad code, you will often see other UOp methods called. To make this task easier, let’s go over some popular ones.\n\n.replace()\nDespite its name, this does not replace, but rather creates a new UOp that is a copy of the original UOp, except for the args (op, dtype, arg, src) you want to change:\n\nadd1.replace(op=Ops.SUB)\n\nUOp(Ops.SUB, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=3, src=()),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),\n  UOp(Ops.CONST, dtypes.int, arg=2, src=()),))\n\n\nadd1 did not change:\n\nadd1\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=3, src=()),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),\n  UOp(Ops.CONST, dtypes.int, arg=2, src=()),))\n\n\nUOps are actually supposed to be immutable, but this is not enforced for performance reasons:\n# NOTE: this should be frozen, but frozen is slower\n@dataclass(eq=False, slots=True)\nclass UOp(MathTrait, metaclass=UOpMetaClass):\n    ...\n\n\n\nUOp to code\n\nfrom tinygrad.engine.schedule import create_schedule_with_vars\nfrom tinygrad.engine.realize import lower_schedule_item\n\nYou did a bunch of Tensor operations, constructed a chonky UOp tree, and now you want to actually compute it.\n\na = (Tensor.full((10, 10), 1) + Tensor.full((10, 10), 2)).contiguous()\na.lazydata\n\nUOp(Ops.CONTIGUOUS, dtypes.int, arg=None, src=(\n  UOp(Ops.ADD, dtypes.int, arg=None, src=(\n    UOp(Ops.EXPAND, dtypes.int, arg=(10, 10), src=(\n      UOp(Ops.RESHAPE, dtypes.int, arg=(1, 1), src=(\n        UOp(Ops.CONST, dtypes.int, arg=1, src=(\n          x4:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n            UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),)),)),\n    UOp(Ops.EXPAND, dtypes.int, arg=(10, 10), src=(\n      UOp(Ops.RESHAPE, dtypes.int, arg=(1, 1), src=(\n        UOp(Ops.CONST, dtypes.int, arg=2, src=(\n           x4,)),)),)),)),))\n\n\nThe first step is to “schedule” the computation. This converts the UOp tree to a lover level one. You might also notice that it computed the 1+2=3.\n\nNote: We will cover the ShapeTracker in a separate chapter soon\n\n\nschedule, vars = a.schedule_with_vars()\nschedule, vars\n\n([ScheduleItem(ast=UOp(Ops.SINK, dtypes.void, arg=None, src=(\n    UOp(Ops.STORE, dtypes.void, arg=None, src=(\n      UOp(Ops.DEFINE_GLOBAL, dtypes.int.ptr(100), arg=0, src=()),\n      UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(10, 10), strides=(10, 1), offset=0, mask=None, contiguous=True),)), src=()),\n      UOp(Ops.CONST, dtypes.int, arg=3, src=(\n        UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(10, 10), strides=(0, 0), offset=0, mask=None, contiguous=False),)), src=()),)),)),)), bufs=(&lt;buf real:False device:CPU size:100 dtype:dtypes.int offset:0&gt;,), metadata=(contiguous, __add__))],\n {})\n\n\nThe next step is to convert the ScheduleItem into executable code.\n\nei = lower_schedule_item(schedule[0])\nei\n\nopened device CPU from pid:549064\nE_25_4\n 0: (25, 4)                   int.ptr(100)         (4, 1)                         ShapeTracker(views=(View(shape=(25, 4), strides=(4, 1), offset=0, mask=None, contiguous=True),))\n[Opt(op=OptOps.UPCAST, axis=0, arg=4)]\n\nvoid E_25_4(int* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 25; ridx0++) {\n    int alu0 = (ridx0&lt;&lt;2);\n    *(data0+alu0) = 3;\n    *(data0+(alu0+1)) = 3;\n    *(data0+(alu0+2)) = 3;\n    *(data0+(alu0+3)) = 3;\n  }\n}\n\n\n\nExecItem(prg=&lt;tinygrad.engine.realize.CompiledRunner object&gt;, bufs=[&lt;buf real:False device:CPU size:100 dtype:dtypes.int offset:0&gt;], metadata=(contiguous, __add__))\n\n\nThis brings the UOp tree to the lowest level, that maps ~1:1 to the generated code:\n\nfor o in ei.prg.p.uops:\n    print(o.op, o.arg, [s.arg for s in o.src if s.op == Ops.CONST] if o.src else \"\")\n\nOps.NAME E_25_4 \nOps.DEFINE_GLOBAL 0 \nOps.CONST 0 \nOps.CONST 1 \nOps.CONST 2 \nOps.CONST 3 \nOps.CONST 25 \nOps.RANGE 0 [0, 25]\nOps.SHL None [2]\nOps.INDEX None []\nOps.STORE None [3]\nOps.ADD None [1]\nOps.INDEX None []\nOps.STORE None [3]\nOps.ADD None [2]\nOps.INDEX None []\nOps.STORE None [3]\nOps.ADD None [3]\nOps.INDEX None []\nOps.STORE None [3]\nOps.ENDRANGE None []\n\n\n\nprint(ei.prg.p.src)\n\n\nvoid E_25_4(int* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 25; ridx0++) {\n    int alu0 = (ridx0&lt;&lt;2);\n    *(data0+alu0) = 3;\n    *(data0+(alu0+1)) = 3;\n    *(data0+(alu0+2)) = 3;\n    *(data0+(alu0+3)) = 3;\n  }\n}\n\n\n\nLet’s compile and run the code. We will go into much more details on individual steps later.\n\nei.run()\n\n*** CPU        1 E_25_4                                    arg  1 mem  0.00 GB tm      7.96us/     0.01ms (     0.00 GFLOPS    0.1|0.1     GB/s) ['contiguous', '__add__']\n\n\n7.961993105709553e-06\n\n\nThe result has been stored to the buffer:\n\nimport numpy as np\n\nview = memoryview(a.lazydata.base.realized._buf)\nnp.frombuffer(view, dtype=np.int32).reshape(a.shape)\n\narray([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]], dtype=int32)",
    "crumbs": [
      "1 - UOps"
    ]
  },
  {
    "objectID": "misc_2.html",
    "href": "misc_2.html",
    "title": "Misc 2 - CUDA Runtime library",
    "section": "",
    "text": "TinyGrad has 2 backends for Nvidia GPUS - CUDA and NV.\n\nCUDA just performs calls into the CUDA Runtime library, not much different from host code in a .cu file.\nNV skips the library, and talks to the driver directly.\n\nWe will play with the CUDA library here, and do things NV-style in the next chapter.\nSee CUDA Driver API.\n\nsaxpy.cu\nLet’s look at a free-standing CUDA kernel. This does not include any host code:\nsaxpy.cu\n\n#include &lt;stdint.h&gt;\n\n// SAXPY kernel: y = alpha*x + y\n__global__ void saxpy(float alpha, float *x, float *y, int32_t n)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i &lt; n) {\n        y[i] = alpha * x[i] + y[i];\n    }\n}\n\n\n!nvcc -cubin saxpy.cu -o saxpy.cubin -arch=sm_86\n\nCuda compilation.\n.cu files often contain a mixture of host and device code. Cuda will split and group them, and will compile them separately, with different compilers for host and device. In out example, we don’t have any host code - we will perform all calls to CUDA Runtime Library from python using ctypes.\nNVCC compile the device part of .cu in 2 stages:\n\nSource: NVCC Documentation\n\nPTX is the higher-level and mode device-independent aseembly-like code.\nSASS is the final assembly code that maps 1:1 to machine code. It is probably backward-compatible, but if you compile old PTX for a new target device SASS, you might get more optimal code that won’t run on the old devices.\n\nHere we combined the 2 steps into one, and generated a .cubin. It’s actually just an ELF file that contains metadata and the machine code for the kernel:\n\n!objdump --headers saxpy.cubin\n\n\nsaxpy.cubin:     file format elf64-little\n\nSections:\nIdx Name          Size      VMA               LMA               File off  Algn\n  0 .debug_frame  00000070  0000000000000000  0000000000000000  00000320  2**0\n                  CONTENTS, RELOC, READONLY, DEBUGGING, OCTETS\n  1 .nv.info      00000024  0000000000000000  0000000000000000  00000390  2**2\n                  CONTENTS, READONLY\n  2 .nv.info._Z5saxpyfPfS_i 0000006c  0000000000000000  0000000000000000  000003b4  2**2\n                  CONTENTS, READONLY\n  3 .nv.callgraph 00000020  0000000000000000  0000000000000000  00000420  2**2\n                  CONTENTS, READONLY\n  4 .nv.rel.action 00000010  0000000000000000  0000000000000000  00000440  2**3\n                  CONTENTS, READONLY\n  5 .nv.constant0._Z5saxpyfPfS_i 0000017c  0000000000000000  0000000000000000  00000460  2**2\n                  CONTENTS, ALLOC, LOAD, READONLY, DATA\n  6 .text._Z5saxpyfPfS_i 00000180  0000000000000000  0000000000000000  00000600  2**7\n                  CONTENTS, ALLOC, LOAD, READONLY, CODE\n\n\n.text._Z5saxpyfPfS_i must have the code for the kernel.\nI don’t know why .nv.constant0._Z5saxpyfPfS_i is so large (300 bytes), but it’s all zeros.\nThere is also cuobjdump that can parse the CUDA-specific sections:\n\n!cuobjdump saxpy.cubin --dump-resource-usage\n\n\nResource usage:\n Common:\n  GLOBAL:0\n Function _Z5saxpyfPfS_i:\n  REG:10 STACK:0 SHARED:0 LOCAL:0 CONSTANT[0]:380 TEXTURE:0 SURFACE:0 SAMPLER:0\n\n\nOr disassemble the code, although it’s not very pretty:\n\n!cuobjdump saxpy.cubin --dump-sass | head -n 20 && echo \"...\"\n\n\n    code for sm_86\n        Function : _Z5saxpyfPfS_i\n    .headerflags    @\"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM86 EF_CUDA_VIRTUAL_SM(EF_CUDA_SM86)\"\n        /*0000*/                   MOV R1, c[0x0][0x28] ;                        /* 0x00000a0000017a02 */\n                                                                                 /* 0x000fe40000000f00 */\n        /*0010*/                   S2R R4, SR_CTAID.X ;                          /* 0x0000000000047919 */\n                                                                                 /* 0x000e280000002500 */\n        /*0020*/                   S2R R3, SR_TID.X ;                            /* 0x0000000000037919 */\n                                                                                 /* 0x000e240000002100 */\n        /*0030*/                   IMAD R4, R4, c[0x0][0x0], R3 ;                /* 0x0000000004047a24 */\n                                                                                 /* 0x001fca00078e0203 */\n        /*0040*/                   ISETP.GE.AND P0, PT, R4, c[0x0][0x178], PT ;  /* 0x00005e0004007a0c */\n                                                                                 /* 0x000fda0003f06270 */\n        /*0050*/               @P0 EXIT ;                                        /* 0x000000000000094d */\n                                                                                 /* 0x000fea0003800000 */\n        /*0060*/                   MOV R5, 0x4 ;                                 /* 0x0000000400057802 */\n                                                                                 /* 0x000fe20000000f00 */\n        /*0070*/                   ULDC.64 UR4, c[0x0][0x118] ;                  /* 0x0000460000047ab9 */\n                                                                                 /* 0x000fc80000000a00 */\n...\n\n\n\n\nDevice init\nTinyGrad autogenerates ctypes bindings for the CUDA library, so let’s just use them.\nThis works pretty much the same way as calling those functions from C.\nI will use the lower-level CUDA Driver API functions here.\n\nimport ctypes\nfrom tinygrad.runtime.autogen import cuda  #\nfrom tinygrad.helpers import init_c_var\n\n# import os\n# os.environ[\"IOCTL\"] = \"1\"\n# import nv_ioctl\n\ndef check(status):\n    if status != 0:\n        error_str = ctypes.string_at(init_c_var(ctypes.POINTER(ctypes.c_char)(),\n                                               lambda x: cuda.cuGetErrorString(status, ctypes.byref(x)))).decode()\n        raise RuntimeError(f\"CUDA Error {status}, {error_str}\")\n\n\n# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html\ncheck(cuda.cuInit(0)) # This 0 is always 0\n\n\ncu_device = ctypes.c_int() # It's actually just an int with the value of the device ID\n# It fails if you try to get a device that does not exist, but oherwise if just returns the device ID you gave it.\ncheck(cuda.cuDeviceGet(cu_device, 0)) # cu_device is passed by pointer, it's converted automatically based on the cuDeviceGet function signature\ncu_device.value\n\n0\n\n\n\n\nDevice information\n\ndevice_count = ctypes.c_int()\ncheck(cuda.cuDeviceGetCount(device_count))\nprint(f\"Number of CUDA devices available: {device_count.value}\")\n\nNumber of CUDA devices available: 1\n\n\n\ndevice_name = ctypes.create_string_buffer(100)\ncheck(cuda.cuDeviceGetName(device_name, len(device_name), cu_device))\nprint(device_name.value.decode())\n\nNVIDIA GeForce RTX 3080 Laptop GPU\n\n\n\nminor = ctypes.c_int()\nmajor = ctypes.c_int()\ncheck(cuda.cuDeviceComputeCapability(major, minor, 0))\nmajor.value, minor.value\n\n(8, 6)\n\n\n\n# Get total memory on the device\ntotal_memory = ctypes.c_size_t()\ncheck(cuda.cuDeviceTotalMem_v2(ctypes.byref(total_memory), cu_device))\nprint(f\"Total memory on device: {total_memory.value / (1024**3):.2f} GB\")\n\nTotal memory on device: 15.59 GB\n\n\n\n\nContext\nIn CUDA, a context represents the primary object for managing resources and execution on a GPU device.\nEach thread can have one active context at a time, and contexts can be pushed/popped from a stack. The context must be created before any CUDA operations can be performed on a device.\n\ncu_context = cuda.CUcontext()\n\n# cuCtxCreate_v2 is actually identical to cuCtxCreate:\n# include/cuda.h:\n# ...\n# #define cuCtxCreate                         cuCtxCreate_v2\n# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html\ncheck(cuda.cuCtxCreate_v2(cu_context, 0, cu_device))\ncu_context.contents # This is a pointer to the context object. It is opaque, no idea what is the size or composition of the context object.\n\n&lt;tinygrad.runtime.autogen.cuda.struct_CUctx_st&gt;\n\n\n\n\nMemory management\nThe device is ready. Let’s allocate the memory for the input and output arrays.\n\nimport numpy as np\nN = 128\n\n# saxpy performs the operation y = a*x + y\ndev_x = cuda.CUdeviceptr() # 64 bit pointer to device memory\ndev_y = cuda.CUdeviceptr()\n\n\n# Allocate the buffers on the device\n# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html\n# include/cuda.h:\n# #define cuMemAlloc                          cuMemAlloc_v2\ncheck(cuda.cuMemAlloc_v2(dev_x, N*4))\ncheck(cuda.cuMemAlloc_v2(dev_y, N*4))\n\n\nhost_x = np.linspace(0, 100, N).astype(np.float32)\nhost_y = np.zeros(N).astype(np.float32)\n\n# Copy data to device. IDK why they are all called _v2\ncheck(cuda.cuMemcpyHtoD_v2(dev_x, host_x.ctypes.data_as(ctypes.c_void_p), N*4))\ncheck(cuda.cuMemcpyHtoD_v2(dev_y, host_y.ctypes.data_as(ctypes.c_void_p), N*4))\n\n\n\nLoad and run the kernel\n\nimage = open(\"saxpy.cubin\", \"rb\").read()\nmodule = cuda.CUmodule() # Another opaque object.\n\n# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MODULE.html\ncheck(cuda.cuModuleLoadData(module, image))\nmodule\n\n&lt;tinygrad.runtime.autogen.cuda.LP_struct_CUmod_st&gt;\n\n\n\nfx = cuda.CUfunction() # You guessed it, anoher opaque object\ncheck(cuda.cuModuleGetFunction(fx, module, \"_Z5saxpyfPfS_i\".encode(\"utf-8\")))\nfx\n\n&lt;tinygrad.runtime.autogen.cuda.LP_struct_CUfunc_st&gt;\n\n\nCreate the parameter array for the kernel\n\ngrid_size =  (1,1,1)\nblock_size = (N,1,1)\n\nshared_mem_size = 0 # We don't need shared memory for this kernel\n\n# Args\n# 0 - alpha (float)\n# 1 - input (pointer to float)\n# 2 - output (pointer to float)\n# 3 - N (int32)\n\n# The params argument to cuLaunchKernel is a pointer to an array of pointers to the parameters.\n# The CUDA library knows the size of each parameter from the metadata, so it can figure out how to pass them to the kernel.\n# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC.html\n\nalpha_val = ctypes.c_float(2.0)\nn = ctypes.c_int32(N)\n\nalpha_ptr = ctypes.cast(ctypes.addressof(alpha_val), ctypes.c_void_p) # Pointer to alpha value\ndev_x_ptr = ctypes.cast(ctypes.addressof(dev_x), ctypes.c_void_p) # Pointer to the x array\ndev_y_ptr = ctypes.cast(ctypes.addressof(dev_y), ctypes.c_void_p) # Pointer to the y array\nn_ptr = ctypes.cast(ctypes.addressof(n), ctypes.c_void_p) # Pointer to the N value\n\nVoidPtrArrayType = ctypes.c_void_p * 4\nparams = VoidPtrArrayType() # Create the array to hold pointers to args\n\n# Populate the array with pointers to the actual kernel arguments\nparams[0] = alpha_ptr\nparams[1] = dev_x_ptr\nparams[2] = dev_y_ptr\nparams[3] = n_ptr\n\nRun the kernel\n\n# https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EXEC.html\ncheck(cuda.cuLaunchKernel(fx, *grid_size, *block_size, shared_mem_size, None, params, None))\n\n\n# Copy the result back to the host\ncheck(cuda.cuMemcpyDtoH_v2(host_y.ctypes.data_as(ctypes.c_void_p), dev_y, N*4))\n\nVerify the result\n\nhost_x[:16]\n\narray([ 0.        ,  0.78740156,  1.5748031 ,  2.3622048 ,  3.1496062 ,\n        3.937008  ,  4.7244096 ,  5.5118113 ,  6.2992125 ,  7.086614  ,\n        7.874016  ,  8.661417  ,  9.448819  , 10.23622   , 11.0236225 ,\n       11.811024  ], dtype=float32)\n\n\n\nhost_y[:16]\n\narray([ 0.       ,  1.5748031,  3.1496062,  4.7244096,  6.2992125,\n        7.874016 ,  9.448819 , 11.0236225, 12.598425 , 14.173228 ,\n       15.748032 , 17.322834 , 18.897638 , 20.47244  , 22.047245 ,\n       23.622047 ], dtype=float32)\n\n\n\n(host_y == host_x * alpha_val.value).all()\n\nnp.True_",
    "crumbs": [
      "Misc 2 - CUDA Runtime library"
    ]
  },
  {
    "objectID": "arange.html",
    "href": "arange.html",
    "title": "4 - The .arange() insanity",
    "section": "",
    "text": "import os\nos.environ[\"CPU\"] = \"1\"\nos.environ[\"DEBUG\"]=\"4\"\n# os.environ[\"NOOPT\"]=\"1\"\n\nfrom tinygrad import Tensor, dtypes\nfrom tinygrad.ops import UOp, Ops, PatternMatcher, UPat, graph_rewrite\n\nLet’s try something light and fun - a simple arange.\n\na = Tensor.arange(0.5, 2, 0.2) # Start, stop, step =&gt; [0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9]\na.lazydata\n\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n  UOp(Ops.RESHAPE, dtypes.float, arg=(8,), src=(\n    UOp(Ops.REDUCE_AXIS, dtypes.float, arg=(Ops.ADD, (1,)), src=(\n      UOp(Ops.PERMUTE, dtypes.float, arg=(1, 0), src=(\n        UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8), src=(\n          UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8, 1), src=(\n            UOp(Ops.SHRINK, dtypes.float, arg=((0, 8), (0, 8)), src=(\n              UOp(Ops.RESHAPE, dtypes.float, arg=(8, 16), src=(\n                UOp(Ops.SHRINK, dtypes.float, arg=((0, 128),), src=(\n                  UOp(Ops.RESHAPE, dtypes.float, arg=(135,), src=(\n                    UOp(Ops.EXPAND, dtypes.float, arg=(9, 15), src=(\n                      UOp(Ops.RESHAPE, dtypes.float, arg=(1, 15), src=(\n                        UOp(Ops.PAD, dtypes.float, arg=((7, 0),), src=(\n                          UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n                            UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n                              UOp(Ops.CONST, dtypes.float, arg=0.2, src=(\n                                x15:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n                                  UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),)),)),)),)),)),)),)),)),)),)),)),)),)),)),\n  UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n    UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n      UOp(Ops.CONST, dtypes.float, arg=0.3, src=(\n         x15,)),)),)),))\n\n\nOhh, what the hell? I just want 8 numbers, what’s this insanity?\nWhen something is not obvious, you can usually just go step by step.\n                          UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n                            UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n                              UOp(Ops.CONST, dtypes.float, arg=0.2, src=(\n0.2 is the start element (0.5) minus the step (0.3)\n\nNext we pad it with 7 (n-1) 0 elements on the left and expand vertically to 9 (n+1):\n                    UOp(Ops.EXPAND, dtypes.float, arg=(9, 15), src=(\n                      UOp(Ops.RESHAPE, dtypes.float, arg=(1, 15), src=(\n                        UOp(Ops.PAD, dtypes.float, arg=((7, 0),), src=(\n\nNext, we string it into a 135-element array, cut off the last 7 elements, making it end in ... 0, 0, 0, 0.2 ]:\nThen we reshape it back to a rectangle, now (8, 16). The transformation created an interesting pattern.\n              UOp(Ops.RESHAPE, dtypes.float, arg=(8, 16), src=(\n                UOp(Ops.SHRINK, dtypes.float, arg=((0, 128),), src=(\n                  UOp(Ops.RESHAPE, dtypes.float, arg=(135,), src=(\n\nNext we take just the left half othe the pattern, and apply 3 transformations that don’t actually do anything.\nWe wil explore the source of those transforms later.\n      UOp(Ops.PERMUTE, dtypes.float, arg=(1, 0), src=(\n        UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8), src=(\n          UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8, 1), src=(\n            UOp(Ops.SHRINK, dtypes.float, arg=((0, 8), (0, 8)), src=(\n\nThis was a long wat to get a triangular matrix (sad Linear Algebra noises).\nBut now that we have it, let’s sum over the elements in one axis (axis 1 in this case):\n  UOp(Ops.RESHAPE, dtypes.float, arg=(8,), src=(\n    UOp(Ops.REDUCE_AXIS, dtypes.float, arg=(Ops.ADD, (1,)), src=(\n\nI thin you see where this is going. Let’s conver the second branch of the top ADD:\n  UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n    UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n      UOp(Ops.CONST, dtypes.float, arg=0.3, src=(\n\nAnd the final step - we add the 2 to get to our numbers!\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n\nLet’s have a look at the code I guess.\n\na.numpy()\n\nopened device CPU from pid:154318\nr_8_8\n 0: (8, 1)                    float.ptr(8)         (1, 0)                         ShapeTracker(views=(View(shape=(8, 1), strides=(1, 0), offset=0, mask=None, contiguous=True),))\n[Opt(op=OptOps.UNROLL, axis=0, arg=0)]\n\nvoid r_8_8(float* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 8; ridx0++) {\n    *(data0+ridx0) = ((((ridx0&lt;7)!=1)?0.2f:0.0f)+(((ridx0&lt;6)!=1)?0.2f:0.0f)+(((ridx0&lt;5)!=1)?0.2f:0.0f)+(((ridx0&lt;4)!=1)?0.2f:0.0f)+(((ridx0&lt;3)!=1)?0.2f:0.0f)+(((ridx0&lt;2)!=1)?0.2f:0.0f)+(((ridx0&lt;1)!=1)?0.2f:0.0f)+0.5f);\n  }\n}\n\n*** CPU        1 r_8_8                                     arg  1 mem  0.00 GB tm      2.79us/     0.00ms (     0.08 GFLOPS    0.0|0.0     GB/s) ['numpy', 'arange']\n\n\narray([0.5      , 0.7      , 0.9      , 1.1      , 1.3      , 1.5      ,\n       1.7      , 1.9000001], dtype=float32)\n\n\nNot gonna lie, this is an ugly way to do it. I suspect this could be improved.\nvoid r_8_8(float* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 8; ridx0++) {\n    *(data0 + ridx0) = ((((ridx0 &lt; 7) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 6) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 5) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 4) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 3) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 2) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 1) != 1) ? 0.2f : 0.0f) + 0.5f);\n  }\n}\nThis must be some optimization mis-behaving. With NOOPT=1 we get good code:\nvoid r_8_8(float* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 8; ridx0++) {\n    *(data0+ridx0) = ((((float)((ridx0+1)))*0.2f)+0.3f);\n  }\n}\nAt least TinyGrad was able to optimize out all the instane transformations!\nLet’s take a look at the code:\ntensor.py (simplified)\n  def arange(start, stop=None, step=1, **kwargs) -&gt; Tensor:\n    dtype = ...              # Figure out the output dtype\n    output_len=ceildiv(stop-start, step)\n    return (Tensor.full((output_len,), step, dtype=dtype, **kwargs)._cumalu(0, Ops.ADD) + (start - step)).cast(dtype)\n\n(Tensor.full((output_len,), step, dtype=dtype, **kwargs) This creates the (8,) vector filled with 0.2\n(start - step) This is the second branch of the top ADD.\n_cumalu(0, Ops.ADD) And all the magic happens here.\n\n  def _cumalu(self, axis:int, op:Ops, _include_initial=False) -&gt; Tensor:\n    pl_sz = self.shape[axis] - int(not _include_initial)\n    pooled = self.transpose(axis,-1).pad((pl_sz, -int(_include_initial)), value=identity_element(op, self.dtype))._pool((self.shape[axis],))\n\n    # For ADD:\n    return pooled.sum(-1).transpose(axis, -1)\n\npl_sz will be 7\nself.transpose(axis,-1) makes the axis we are interested in the last. In our case we only have 1 axis, so this does nothing, and gets optimized out immediately.\npad((pl_sz, -int(_include_initial)) - Here is the pad that adds the 7 elements to the left filled with zeros (identity element for ADD).\n_pool((self.shape[axis],)): Here be dragons\n\n  def _pool(self, k_:tuple[sint, ...], stride:int|tuple[int, ...]=1, dilation:int|tuple[int, ...]=1) -&gt; Tensor:\n    assert len(self.shape) &gt;= len(k_), f\"can't pool {self.shape} with {k_}\"\n    s_, d_ = make_tuple(stride, len(k_)), make_tuple(dilation, len(k_))\n    assert len(k_) == len(s_) == len(d_), f\"stride/dilation mismatch kernel:{k_} stride:{s_} dilation:{d_}\"\n    noop, i_ = [None] * (self.ndim-len(k_)), self.shape[-len(k_):]\n    assert all(resolve(d*(k-1)+1 &lt;= i) for k,d,i in zip(k_,d_,i_)), \"kernel size cannot be greater than actual input size\"\n    o_ = [ceildiv(i-d*(k-1), s) for i,d,k,s in zip(i_,d_,k_,s_)]\n    if any(resolve(k &gt; s) for k,s in zip(k_,s_)) or any(d != 1 for d in d_):\n      # input size scaling factor to make sure shrink for stride is possible\n      f_ = [1 + int(resolve(o*s &gt; (i - d*(k-1)))) for o,s,i,d,k in zip(o_,s_,i_,d_,k_)]\n      # # repeats such that we don't need padding\n      x = self.repeat([1]*len(noop) + [ceildiv(k*(i*f+d),i) for k,i,d,f in zip(k_,i_,d_,f_)])\n      # handle dilation\n      x = x.shrink(tuple(noop + [(0,k*(i*f+d)) for k,i,d,f in zip(k_,i_,d_,f_)])).reshape(noop + flatten((k,(i*f+d)) for k,i,d,f in zip(k_,i_,d_,f_)))\n      # handle stride\n      x = x.shrink(tuple(noop + flatten(((0,k), (0,o*s)) for k,o,s in zip(k_,o_,s_)))).reshape(noop + flatten((k,o,s) for k,o,s in zip(k_,o_,s_)))\n      x = x.shrink(tuple(noop + flatten(((0,k), (0,o), (0,1)) for k,o in zip(k_,o_)))).reshape(noop + flatten((k,o) for k,o in zip(k_,o_)))\n      # permute to move reduce to the end\n      return x.permute(*range(len(noop)), *[len(noop)+i*2+1 for i in range(len(i_))], *[len(noop)+i*2 for i in range(len(i_))])\n    # TODO: once the shapetracker can optimize well, remove this alternative implementation\n    x = self.pad(tuple(noop + [(0, max(0,o*s-i)) for i,o,s in zip(i_,o_,s_)])).shrink(tuple(noop + [(0,o*s) for o,s in zip(o_,s_)]))\n    x = x.reshape(noop + flatten(((o,s) for o,s in zip(o_,s_))))\n    x = x.shrink(tuple(noop + flatten(((0,o), (0,k)) for o,k in zip(o_,k_))))\n    return x.permute(*range(len(noop)), *[len(noop)+i*2 for i in range(len(i_))], *[len(noop)+i*2+1 for i in range(len(i_))])\nLol I’m not going to pretend I can follow what’s going on here. This obviously does the crazy triangle.\n\npooled.sum(-1).transpose(axis, -1) This must be that REDUCE_AXIS operation, and another transpose that does nothing in this case.\n\nSometimes I wish TinyGrad was easier to understand. :)",
    "crumbs": [
      "4 - The `.arange()` insanity"
    ]
  },
  {
    "objectID": "patternmatcher.html",
    "href": "patternmatcher.html",
    "title": "3 - The Pattern Matcher",
    "section": "",
    "text": "Our next TinyGrad abstraction is the Pattern Matcher (PM)\nPM is used all over TinyGrad for different purposes\n\nimport os\nos.environ[\"CPU\"] = \"1\"\nos.environ[\"DEBUG\"]=\"4\"\n\nfrom tinygrad import Tensor, dtypes\nfrom tinygrad.ops import UOp, Ops, PatternMatcher, UPat, graph_rewrite\n\n\na = (Tensor(2) * 5 + 1).lazydata\na\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=2, src=(\n      x2:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n        UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=(\n       x2,)),)),\n  UOp(Ops.CONST, dtypes.int, arg=1, src=(\n     x2,)),))\n\n\nThe PM operates on a list of rules.\nEach rule consists of a UPat, and a function that is called when the pattern matches part of the tree.\nThe return value of the function is the result of the match, or it’s a None if no match was found among the rules:\n\ntest_rules = PatternMatcher([\n    (UPat(Ops.DEVICE), lambda: \"a DEVICE Uop\"),                                         # This rule matches any `DEVICE` UOp\n    (UPat(Ops.CONST, name=\"x\"), lambda x: f\"Got a CONST dtype {x.dtype} arg {x.arg}\"),  # Can pass the Op to the function\n    (UPat(Ops.CONST), lambda x: f\"Another rule for CONST\"),                             # Oops, only one rule can match!\n    (UPat((Ops.ADD, Ops.MUL)), lambda: \"ADD or MUL\"),                                   # Can match more than one UOp type\n    (UPat(Ops.EXPAND, src=(UPat(Ops.RESHAPE, src=UPat(Ops.CONST, arg=2)))),\n        lambda: \"Expand with reshape from a const with arg=2\")                          # Can match a specific sub-tree.\n                                                                                        # Note: This one only matches the EXPAND for 2, not 1\n    # No match - return Null\n])\n\n[test_rules.rewrite(op) for op in a.toposort]\n\n['a DEVICE Uop',\n None,\n 'Got a CONST dtype dtypes.int arg 2',\n 'Got a CONST dtype dtypes.int arg 5',\n 'ADD or MUL',\n 'Got a CONST dtype dtypes.int arg 1',\n 'ADD or MUL']\n\n\n\nRewriting trees\nA more interesting pattern is to replace the matched UOps with some other UOps. We can also use graph_rewrite to operate on a tree.\n\ninsanity = PatternMatcher([\n    (UPat(Ops.ADD, name=\"x\"), lambda x: UOp(Ops.SUB, dtype=x.dtype, arg=x.arg, src=x.src)),\n    (UPat(Ops.MUL, dtype=dtypes.ints, name=\"x\"), lambda x: UOp(Ops.IDIV, dtype=x.dtype, src=x.src))\n])\n\nrewritten = graph_rewrite(a, insanity)\nrewritten\n\nUOp(Ops.SUB, dtypes.int, arg=None, src=(\n  UOp(Ops.IDIV, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=2, src=(\n      x2:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n        UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=(\n       x2,)),)),\n  UOp(Ops.CONST, dtypes.int, arg=1, src=(\n     x2,)),))\n\n\n\na.render(simplify=False)\n\n'((2*5)+1)'\n\n\n\nrewritten.render(simplify=False)\n\n'((2//5)-1)'\n\n\n\nint(rewritten)\n\n-1\n\n\n\n\nPatternMatcher in TinyGrad\nI think you get the idea. The Pattern Matches is a powerful tool that is used throughout Tinygrad.\nWhen we played with Tensor.schedule_with_vars() and lower_schedule_item() in the chapter on UOps, both function made extensive use of many Pattern Matchers. We will attempt a deep dive into their details in the next chapter.\n\nTinyGrad spec\nAnother use for the Pattern Matcher - checking the validity of UOp trees, according to the spec, , found in tinyngrad/spec.py.\nIt’s very much possible to create UOp trees that are not valid in general, or not valid at certain stages of processing.\nThe spec contains rules that check for silly mistakes in different types of (sub-)trees.\nFor example thre is a tensor_uop_spec for sanity checking the UOp trees created by tensor operations:\n\na\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=2, src=(\n      x2:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n        UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=(\n       x2,)),)),\n  UOp(Ops.CONST, dtypes.int, arg=1, src=(\n     x2,)),))\n\n\n\nfrom tinygrad.spec import type_verify, tensor_uop_spec\n\ntype_verify(list(a.toposort.keys()), tensor_uop_spec) # It throws on errors, no errors found!\n\nLet’s make a broken tree by changing the dtype of the ADD UOp in a to float:\n\nbad = a.replace(dtype=dtypes.float)\nbad\n\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=2, src=(\n      x2:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n        UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=(\n       x2,)),)),\n  UOp(Ops.CONST, dtypes.int, arg=1, src=(\n     x2,)),))\n\n\nThis is not a valid tree - we are adding 2 ints, but the result is a float? There would need to be a cast there!\n\ntry:\n    type_verify(list(bad.toposort.keys()), tensor_uop_spec)\nexcept Exception as e:\n    print(f\"{type(e).__name__}: {' '.join(e.args)}\")\n\n   0 Ops.DEVICE          : dtypes.void                    []                               CPU\n   1 Ops.VIEW            : dtypes.void                    [0]                              ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),))\n   2 Ops.CONST           : dtypes.int                     [1]                              2\n   3 Ops.CONST           : dtypes.int                     [1]                              5\n   4 Ops.MUL             : dtypes.int                     ['2', '5']                       None\n   5 Ops.CONST           : dtypes.int                     [1]                              1\n   6 Ops.ADD             : dtypes.float                   [4, '1']                         None\nRuntimeError: UOp verification failed at 6 on Ops.ADD dtypes.float 2 [&lt;Ops.MUL: 50&gt;, &lt;Ops.CONST: 76&gt;] None\n\n\nIndeed, we caught the error. Let’s fix the tree by casting the 2 ADD sources to a float.\n\nfixed = bad.replace(src=tuple([UOp(Ops.CAST, dtype=dtypes.float, src=(src,)) for src in bad.src]))\nfixed\n\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n  UOp(Ops.CAST, dtypes.float, arg=None, src=(\n    UOp(Ops.MUL, dtypes.int, arg=None, src=(\n      UOp(Ops.CONST, dtypes.int, arg=2, src=(\n        x3:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n          UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n      UOp(Ops.CONST, dtypes.int, arg=5, src=(\n         x3,)),)),)),\n  UOp(Ops.CAST, dtypes.float, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=1, src=(\n       x3,)),)),))\n\n\n\ntype_verify(list(fixed.toposort.keys()), tensor_uop_spec)\n\nNow it works!",
    "crumbs": [
      "3 - The Pattern Matcher"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "0 - Introduction",
    "section": "",
    "text": "import os\n\nos.environ[\"CPU\"] = \"1\"\nos.environ[\"TRACEMETA\"] = \"0\"\nos.environ[\"DEBUG\"]=\"4\"\n\n\nimport tinygrad as tg\nfrom tinygrad import Tensor, dtypes\n\n\nLazy tensors and UOps\nTinygrad API is quite similar to PyTorch, with some quirks.\n\na = Tensor.ones(10, 10)\nb = a + 2\nb\n\n&lt;Tensor &lt;UOp CPU (10, 10) float (&lt;Ops.ADD: 48&gt;, None)&gt; on CPU with grad None&gt;\n\n\nTinyGrad is lazy - it does not perform any computation until explicitly asked to.\nInstead, it saves the operations required to get the result as a tree of UOps:\n\nb.lazydata # It's called `lazydaya` for historic reasons. Rename to `Tensor.uops`?\n\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n  UOp(Ops.EXPAND, dtypes.float, arg=(10, 10), src=(\n    UOp(Ops.RESHAPE, dtypes.float, arg=(1, 1), src=(\n      UOp(Ops.CONST, dtypes.float, arg=1.0, src=(\n        x3:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n          UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),)),)),\n  UOp(Ops.EXPAND, dtypes.float, arg=(10, 10), src=(\n    UOp(Ops.RESHAPE, dtypes.float, arg=(1, 1), src=(\n      UOp(Ops.CONST, dtypes.float, arg=2.0, src=(\n         x3,)),)),)),))\n\n\nThe ADD UOp has 2 sources, both being constants (1 and 2) that are both reshaped to (1, 1) and expanded to shape (10, 10).\nThe CONST UOp takes the value as argument, and has a VIEW UOp as it’s source, which in turn sources from a DEVICE Uop.\nNote that x3:= walrus assignment, and x3 being reused for the second CONST UOp.\n\nfrom tinygrad.ops import UOp, Ops\n\nWe will take a detailed look at UOps in the next chapter, but for now, let’s see how to actually compute the value of b.\n\nKernels on CPU\n\n# This runs the computations needed to get the value of the tensor\n# It does not get realized without the .contiguous() though (TODO: Explain why)\n# Also, should .contiguous() just always be part of .realize()?\nb_realized = b.contiguous().realize()\n\nopened device CPU from pid:958683\nE_25_4\n 0: (25, 4)                   float.ptr(100)       (4, 1)\n[Opt(op=OptOps.UPCAST, axis=0, arg=4)]\ntypedef float float4 __attribute__((aligned(16),vector_size(16)));\nvoid E_25_4(float* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 25; ridx0++) {\n    *((float4*)((data0+(ridx0&lt;&lt;2)))) = (float4){3.0f,3.0f,3.0f,3.0f};\n  }\n}\n*** CPU        1 E_25_4                                    arg  1 mem  0.00 GB tm      3.28us/     0.00ms (     0.00 GFLOPS    0.1|0.1     GB/s) \n\n\nThe debug output gives us a glimpse into how tinygrad performs the computations. It will take the UOp tree, perform a number of transformations on it, and creates one or more kernels - functions that run on the device (potentially many instances in parallel) and do the actual computation.\nIn this case, the device is CPU, which means the kernel will be just plain sequential C code, which will be compiled with clang into a small piece of position-independent binary, then loaded and executed using ctypes.\nThe float4 is a common optimization that you see on both CPU and GPU - it’s more optimal to access memory in 128-byte chunks (4 32-bit floats) at a time, so TinyGrad is being smart here. The optimal number might be device-specific, but 128 bytes is common.\nAnd of course, since we used constants in our computation, there was no need to add 1+2 - TinyGrad was able to just fill the output with the correct value.\nIf we ran it on an NVida GPU, it would instead generate and run CUDA code, same for other devices.\nWe will cover the details of transformations done on the UOps tree at a later time, but for now, let’s look at the result.\nHere is the buffer that contains the data:\n\nprint(type(b_realized.lazydata.base.realized))\n\nb_realized.lazydata.base.realized\n\n&lt;class 'tinygrad.device.Buffer'&gt;\n\n\n&lt;buf real:True device:CPU size:100 dtype:dtypes.float offset:0&gt;\n\n\nSince we used the CPU device, it’s in CPU memory, and we can peek into it directly using memoryview\n\nview = memoryview(b_realized.lazydata.base.realized._buf)\nview[:4].hex()\n\n'00004040'\n\n\n0x00004040 is the hex for float32 ‘3.0’. Let’s use numpy to get a better view.\n\nimport numpy as np\n\n# Note: The buffer is shapeless, so we use `.reshape()` to bring it back to the correct shape\nnp.frombuffer(view, dtype=np.float32).reshape(b.shape)\n\narray([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)\n\n\n\n\n.numpy() and .tolist()\nOf course there is a more convenient way to access the result from Python - use .numpy() on the tensor.\nThis will make sure the tensor ends up on CPU, realize it, and will give the result the correct shape and dtype.\n.numpy() will allso create a copy of the data, so the memory buffer does not just disappear from under our feer when the tensor gets garbage collected.\n\nb.numpy()\n\n*** CPU        2 E_25_4                                    arg  1 mem  0.00 GB tm      8.03us/     0.01ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n\n\narray([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)\n\n\nOr you can use .tolist() to convert it to a python list (or a list of liss of lists … for the correct number of dimensions)\n\nb.tolist()\n\n*** CPU        3 E_25_4                                    arg  1 mem  0.00 GB tm      8.17us/     0.02ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n\n\n[[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0]]\n\n\n\n\nKernels on GPU\nTinyGrad has a concept of “default device”, which we set using the “CPU” env variable in the beginning of the notebook.\nThe device and dtype can be set when creating the tensor, and you can also use .to() to copy the tensor to a different device.\n\na = Tensor(1, device=\"CUDA\", dtype=tg.dtypes.float16)\na\n\n&lt;Tensor &lt;UOp CUDA () half (&lt;Ops.CONST: 74&gt;, None)&gt; on CUDA with grad None&gt;\n\n\n\na.to(device=\"CPU\")\n\n&lt;Tensor &lt;UOp CPU () half (&lt;Ops.COPY: 9&gt;, None)&gt; on CPU with grad None&gt;\n\n\nLet’s have a look at a CUDA kernel for the same computation\n\na = Tensor.ones((10, 10), device=\"CUDA\")\nb = a + 2\nb.numpy()\n\nopened device CUDA from pid:958683\nE_25_4n1\n 0: (25, 4)                   float.ptr(100)       (4, 1)\n[Opt(op=OptOps.UPCAST, axis=0, arg=4)]\n#define INFINITY (__int_as_float(0x7f800000))\n#define NAN (__int_as_float(0x7fffffff))\nextern \"C\" __global__ void __launch_bounds__(1) E_25_4n1(float* data0) {\n  int gidx0 = blockIdx.x; /* 25 */\n  *((float4*)((data0+(gidx0&lt;&lt;2)))) = make_float4(3.0f,3.0f,3.0f,3.0f);\n}\n*** CUDA       4 E_25_4n1                                  arg  1 mem  0.00 GB tm     25.60us/     0.05ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n*** CPU        5 copy      400,     CPU &lt;- CUDA            arg  2 mem  0.00 GB tm     55.73us/     0.10ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n\n\narray([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)\n\n\nWe see a similar pattern, but since the GPU is highly parallel, TinyGrad decided to create 25 threads, each setting its own 4-float chunk.\n\nIf you are familiar with CUDA, you might notice that we created 25 separate thread blocks with 1 thread each instead of 1 block with 25 threads, which is definitely suboptimal.\n\n\nb_realized = b.contiguous().realize()\n\n*** CUDA       6 E_25_4n1                                  arg  1 mem  0.00 GB tm    574.21us/     0.68ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n\n\n\nprint(b_realized.lazydata.base.realized)\nprint(b_realized.lazydata.base.realized._buf)\n\n&lt;buf real:True device:CUDA size:100 dtype:dtypes.float offset:0&gt;\nc_ulong(140551080902656)\n\n\nAs we can see, the output buffer is on the GPU this time, so we can’t access it from the CPU directly.\nBut trust me, the data is definitely there. Let’s use PyCuda to peek into the GPU memory.\n\nimport pycuda\nimport pycuda.driver as cuda\nimport numpy as np\n\n# Create a numpy array to hold the data (100 32-bit floats)\ncpu_array = np.empty((10, 10), dtype=np.float32)\n\n# Copy data from GPU to CPU\ncuda.memcpy_dtoh(cpu_array, b_realized.lazydata.base.realized._buf.value)\n\ncpu_array\n\narray([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)\n\n\nTinyGrad is a complex beast, so it’s normal if this intro left you with more questions than answers. :)",
    "crumbs": [
      "0 - Introduction"
    ]
  },
  {
    "objectID": "appendix_a.html",
    "href": "appendix_a.html",
    "title": "Appendix A: helpers.py",
    "section": "",
    "text": "tinygrad/helpers.py contains a bunch of helper functions that are used all over TinyGrad.\nGo over the list, it will make reading the code easier. When in doubt, always refer to the code.\n\ndedup(x: Iterable[T]) -&gt; list[T]\n\nDescription: Removes duplicate elements from an iterable while preserving the original order of the first appearance.\nInput: An iterable x.\nOutput: A list containing unique elements from x in their original order.\nExample: dedup([1, 3, 2, 3, 1]) returns [1, 3, 2].\nUsage: Used throughout tinygrad to ensure uniqueness in lists where order matters, like kernel arguments, variable lists, or UOp sources.\nGotchas: Requires hashable elements.\n\n\n\nargfix(*x) -&gt; tuple\n\nDescription: Normalizes function arguments. It allows a function to accept arguments either as a sequence (list/tuple) or as individual arguments.\nInput: Variable positional arguments *x.\nOutput: A tuple.\nExample:\n\nargfix(1, 2, 3) returns (1, 2, 3).\nargfix((1, 2, 3)) or argfix([1, 2, 3]) returns (1, 2, 3).\n\nUsage: Commonly used in Tensor methods (e.g., reshape, pad, permute, expand) to allow flexible input like t.reshape(2, 3) or t.reshape((2, 3)).\nGotchas: Raises a ValueError if the first argument is a list/tuple and there are more arguments (e.g., argfix((1, 2), 3) is invalid).\n\n\n\nargsort(x: Union[list, tuple])\n\nDescription: Returns the indices that would sort the input iterable x. Mimics numpy.argsort.\nInput: An iterable x.\nOutput: A sequence of the same type as x (e.g., list or tuple) containing integer indices.\nExample: argsort([30, 10, 20]) returns [1, 2, 0].\nUsage: Used internally, for example, in gradient.py to compute the inverse permutation needed for the gradient of permute. Also used in View.__add__ for sorting strides during view merging/inversion.\nGotchas: Returns a list or a tuple, depending on the input type. Not intended to be used on Tensors.\n\n\n\nall_same(items: Union[tuple[T, ...], list[T]]) -&gt; bool\n\nDescription: Checks if all elements in a non-empty list or tuple are identical (equal to the first element).\nInput: A list or tuple.\nOutput: bool.\nExample: all_same([1, 1, 1]) returns True. all_same((2, 3, 2)) returns False.\nUsage: Used for assertions and checks, e.g., ensuring source UOps have the same shape (UOp.st), checking if all elements in a VCONST are the same (UOp.const), verifying multi-device tensors have consistent properties (tensor.py, multi.py), checking kernel arguments (Kernel.linearize).\nGotchas: Returns True for empty or single-element lists/tuples.\n\n\n\nall_int(t: Sequence[Any]) -&gt; TypeGuard[tuple[int, ...]]\n\nDescription: Checks if all elements in a sequence are of type int. Uses TypeGuard for static analysis benefits.\nInput: A Sequence (like list or tuple).\nOutput: bool. If True, type checkers can infer the input is specifically a sequence of int.\nExample: all_int([1, 2, 3]) returns True. all_int((1, 2.0)) returns False.\nUsage: Crucial for distinguishing between concrete shapes (tuples of int) and symbolic shapes (tuples containing Variable or UOp). Used extensively in ShapeTracker, View, and Tensor methods to determine if operations like size calculation or certain reshapes are possible without symbolic evaluation.\n\n\n\ncolored(st, color: Optional[str], background=False) -&gt; str\n\nDescription: Adds ANSI escape codes to a string st to color it for terminal output. Supports foreground and background colors. Bright colors can be selected by passing the color name in uppercase (e.g., “RED”).\nInput: String st, optional color name color (e.g., “red”, “GREEN”), boolean background.\nOutput: A string, potentially with ANSI color codes.\nExample: colored(\"Error\", \"RED\") returns \"\\u001b[91mError\\u001b[0m\".\nUsage: Used for enhancing debug prints, progress bars (tqdm), visualizing kernel shapes (Kernel.colored_shape), and highlighting warnings/errors.\nGotchas: Only effective in terminals that support ANSI escape codes.\n\n\n\ncolorize_float(x: float) -&gt; str\n\nDescription: Formats a float x (intended as a ratio, often speedup/slowdown) and colors it: green if &lt; 0.75, yellow if 0.75 &lt;= x &lt;= 1.15, red if &gt; 1.15.\nInput: A float x.\nOutput: A colored string representation (e.g., \"  0.50x\").\nExample: colorize_float(0.5) returns a green string. colorize_float(1.2) returns a red string.\nUsage: Used in profiling output (helpers.Profiling) to visually indicate performance ratios.\n\n\n\ntime_to_str(t: float, w=8) -&gt; str\n\nDescription: Converts a time duration t (in seconds) into a human-readable string, automatically selecting units (s, ms, us) based on magnitude. Formats the output to a specified width w.\nInput: Time t in seconds (float), optional field width w (int).\nOutput: A formatted string representing the time.\nExample: time_to_str(0.0015) might return \"   1.50ms\". time_to_str(1.2) might return \"   1.20s \".\nUsage: Used extensively in debug output (DEBUG&gt;=2), profiling (Profiling, Timing), and beam search logging to display kernel runtimes and compile times concisely.\n\n\n\nansistrip(s: str) -&gt; str\n\nDescription: Removes ANSI escape codes (primarily color/formatting codes) from a string s.\nInput: A string s.\nOutput: The string s with ANSI codes removed.\nExample: ansistrip(\"\\u001b[91mError\\u001b[0m\") returns \"Error\".\nUsage: Used by ansilen to calculate visible length and by to_function_name to sanitize potentially colored strings before creating identifiers.\n\n\n\nansilen(s: str) -&gt; int\n\nDescription: Calculates the visible length of a string s by first removing ANSI escape codes using ansistrip.\nInput: A string s.\nOutput: The length of the string without ANSI codes (int).\nExample: ansilen(colored(\"hello\", \"red\")) returns 5.\nUsage: Useful for formatting output containing colored text, ensuring correct alignment based on visible characters (e.g., in Kernel.colored_shape(pad=...)).\n\n\n\nmake_tuple(x: Union[int, Sequence[int]], cnt: int) -&gt; tuple[int, ...]\n\nDescription: Creates a tuple of integers of length cnt. If x is an integer, it’s repeated cnt times. If x is a sequence, it’s converted to a tuple (its length should match cnt).\nInput: An integer or sequence of integers x, and a target count cnt.\nOutput: A tuple of integers of length cnt.\nExample: make_tuple(3, 2) returns (3, 3). make_tuple([1, 2], 2) returns (1, 2).\nUsage: Normalizes arguments like stride, padding which can be a single value or a sequence.\nGotchas: Does not check if len(x) == cnt if x is not an int.\n\n\n\nflatten(l: Iterable[Iterable[T]]) -&gt; list[T]\n\nDescription: Flattens an iterable of iterables by one level.\nInput: An iterable of iterables l.\nOutput: A flattened list.\nExample: flatten([[1, 2], [3, 4]]) returns [1, 2, 3, 4].\nUsage: Widespread utility function used throughout the codebase.\n\n\n\nfully_flatten(l)\n\nDescription: Recursively flattens nested lists/tuples into a single list. Handles numpy 0-dim arrays.\nInput: A potentially nested structure l.\nOutput: A completely flattened list.\nExample: fully_flatten([1, [2, [3, 4]]]) returns [1, 2, 3, 4].\nUsage: Primarily used by _frompy to prepare data for struct.pack.\n\n\n\nfromimport(mod, frm)\n\nDescription: Dynamically imports frm from module mod. Equivalent to from mod import frm, returning frm.\nInput: Module name mod (str), item name frm (str).\nOutput: The imported object.\nExample: fromimport(\"os\", \"path\") is equivalent to from os import path.\n\n\n\nstrip_parens(fst: str) -&gt; str\n\nDescription: Heuristically removes the outermost matching parentheses from a string.\nInput: A string fst.\nOutput: The string with outermost parentheses removed, if they match.\nExample: strip_parens(\"(a+b)\") returns \"a+b\". strip_parens(\"(a+b)*(c+d)\") returns \"(a+b)*(c+d)\".\nUsage: Used by C-style renderers to simplify generated code expressions.\n\n\n\nceildiv(num, amt) -&gt; int\n\nDescription: Computes ceil(num / amt) using integer arithmetic.\nInput: Numerator num, denominator amt.\nOutput: Ceiling of the division as an int.\nExample: ceildiv(5, 2) returns 3.\nUsage: Ubiquitous for calculating sizes, bounds, counts that require rounding up.\n\n\n\nround_up(num: int, amt: int) -&gt; int\n\nDescription: Rounds num up to the nearest multiple of amt.\nInput: Integer num, integer amt.\nOutput: num rounded up to the nearest multiple of amt.\nExample: round_up(5, 4) returns 8.\nUsage: Used for alignment and ensuring sizes are multiples of factors (memory, block sizes).\n\n\n\nlo32(x: Any) -&gt; Any, hi32(x: Any) -&gt; Any\n\nDescription: Extract the lower (lo32) or upper (hi32) 32 bits of a (presumably 64-bit) integer or symbolic integer (sint).\nInput: A value x (typically an integer or symbolic integer).\nOutput: The lower or upper 32 bits of x.\nExample: lo32(0x12345678ABCDEF) returns 0xABCDEF. hi32(0x12345678ABCDEF) returns 0x12345678.\nUsage: Used in low-level code generation or runtime interactions needing 32-bit components of 64-bit values (e.g., specific hardware instructions, register packing).\nGotchas: Uses bitwise AND (&) and right shift (&gt;&gt;). The Any type hint means behavior with non-integer/sint types is undefined.\n\n\n\ndata64(data: Any) -&gt; tuple[Any, Any], data64_le(data: Any) -&gt; tuple[Any, Any]\n\nDescription: Split a 64-bit value into two 32-bit components. data64 returns (high_32, low_32). data64_le returns (low_32, high_32) (Little-Endian order).\nInput: A 64-bit value data.\nOutput: A tuple of two 32-bit components.\nExample: data64(0x12345678ABCDEF) returns (0x12345678, 0xABCDEF).\nUsage: Similar to lo32/hi32, used for low-level interactions requiring split 64-bit values, potentially considering endianness.\n\n\n\ngetbits(value: int, start: int, end: int) -&gt; int\n\nDescription: Extracts bits start through end (inclusive) from value.\nInput: Integer value, bit positions start and end.\nOutput: The extracted bits as an integer.\nExample: getbits(0b110101, 1, 3) returns 0b101 (5).\n\n\n\ni2u(bits: int, value: int) -&gt; int\n\nDescription: Converts a signed bits-bit integer value to its unsigned representation.\nInput: Bit width bits, signed integer value.\nOutput: Unsigned integer representation.\nExample: i2u(8, -1) returns 255.\n\n\n\nmerge_dicts(ds: Iterable[dict[T, U]]) -&gt; dict[T, U]\n\nDescription: Merges dictionaries, asserting that shared keys have identical values.\nInput: An iterable of dictionaries ds.\nOutput: A merged dictionary.\nExample: merge_dicts([{\"a\": 1}, {\"b\": 2}]) returns {\"a\": 1, \"b\": 2}.\nUsage: Consistently combines var_vals dictionaries during graph processing and JIT compilation.\n\n\n\npartition(itr: Iterable[T], fxn: Callable[[T], bool]) -&gt; tuple[list[T], list[T]]\n\nDescription: Splits iterable itr into two lists based on predicate fxn.\nInput: An iterable itr and a predicate function fxn.\nOutput: A tuple of two lists: (items_where_fxn_is_true, items_where_fxn_is_false).\nExample: partition([1, 2, 3, 4], lambda x: x % 2 == 0) returns ([2, 4], [1, 3]).\nUsage: General utility for separating elements based on a condition, used in various places like separating UOps by type.\n\n\n\nunwrap(x: Optional[T]) -&gt; T\n\nDescription: Asserts x is not None and returns it, satisfying type checkers.\nInput: A potentially None value x.\nOutput: The value x if it’s not None.\nExample: unwrap(some_dict.get(\"key\")) raises an assertion error if the key doesn’t exist.\nUsage: Widely used for safety and to satisfy type checkers, especially when accessing optional values that should be present.\n\n\n\nget_single_element(x: list[T]) -&gt; T\n\nDescription: Asserts list x has exactly one element and returns it.\nInput: A list x.\nOutput: The single element in the list.\nExample: get_single_element([42]) returns 42.\nUsage: Used when a list is expected to contain exactly one item, often in graph processing or optimization passes.\n\n\n\nget_child(obj, key)\n\nDescription: Accesses nested attributes/items in obj using a dot-separated key string.\nInput: An object obj and a string key with dot notation (e.g., “attr1.attr2.0”).\nOutput: The nested attribute or item.\nExample: get_child({\"a\": {\"b\": [10, 20]}}, \"a.b.1\") returns 20.\nUsage: Used for accessing deeply nested structures, especially in debugging and serialization.\nNote: Handles both attribute access (for objects) and item access (for dicts/lists), converting numeric strings to integers for indexing.\n\n\n\nword_wrap(x, wrap=80)\n\nDescription: Basic line wrapping at wrap characters.\nInput: A string x and optional wrap length wrap.\nOutput: The string with newlines inserted every wrap characters.\nExample: word_wrap(\"abcdefghijklmnopqrstuvwxyz\", 10) returns \"abcdefghij\\nklmnopqrst\\nuvwxyz\".\nUsage: Used for formatting debug output and error messages.\nNote: Doesn’t respect word boundaries, simply inserts newlines every wrap characters.\n\n\n\npluralize(st: str, cnt: int) -&gt; str\n\nDescription: Formats count cnt with noun st, adding ‘s’ if cnt != 1.\nInput: A string st (noun) and an integer cnt.\nOutput: A formatted string with the count and properly pluralized noun.\nExample: pluralize(\"item\", 1) returns \"1 item\". pluralize(\"item\", 2) returns \"2 items\".\nUsage: Used for generating human-readable log messages and status reports.\nNote: Uses simple ‘s’ pluralization, which doesn’t handle irregular plurals.\n\n\n\npolyN(x: T, p: list[float]) -&gt; T\n\nDescription: Evaluates polynomial p at x using Horner’s method.\nInput: A value x and a list of coefficients p in descending order of power.\nOutput: The polynomial evaluation result.\nExample: polyN(2, [1, 2, 3]) computes 1*2^2 + 2*2^1 + 3*2^0 = 1*4 + 2*2 + 3*1 = 11.\nUsage: Used in transcendental.py for efficient function approximations where x is often a UOp.\nNote: Coefficients are in descending order (highest power first).\n\n\n\nto_function_name(s: str) -&gt; str\n\nDescription: Converts string s to a valid C/Python function name by removing ANSI codes and hex-encoding invalid characters.\nInput: A string s.\nOutput: A valid function name string.\nExample: to_function_name(\"kernel(a+b)\") might return \"kernel_a_b_\".\nUsage: Generates valid, deterministic kernel function names for code generation.\nNote: Results are cached for performance. Handles ANSI color codes by removing them.\n\n\n\ngetenv(key: str, default=0)\n\nDescription: Gets environment variable key, casting to type of default.\nInput: Environment variable name key and optional default value.\nOutput: The environment variable value cast to the type of default.\nExample: getenv(\"DEBUG\", 0) returns the integer value of the DEBUG environment variable, or 0 if not set.\nUsage: Pervasive configuration mechanism for tinygrad features and debug levels.\nGotchas: Caches results for performance, which means runtime changes to environment variables aren’t seen after first access.\n\n\n\ntemp(x: str, append_user: bool = False) -&gt; str\n\nDescription: Creates a path string in the system’s temporary directory, optionally making it user-specific.\nInput: A string x (path component) and optional flag append_user.\nOutput: A full path string in the temporary directory.\nExample: temp(\"tinygrad/cache\") might return /tmp/tinygrad/cache.\nUsage: Locating caches, downloads, profiling outputs in a system-appropriate temporary location.\nGotchas: If append_user is True, appends the username to make the path user-specific.",
    "crumbs": [
      "Appendix A: `helpers.py`"
    ]
  }
]