[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "0 - Introduction",
    "section": "",
    "text": "import os\n\nos.environ[\"CPU\"] = \"1\"\nos.environ[\"TRACEMETA\"] = \"0\"\nos.environ[\"DEBUG\"]=\"4\"\n\n\nimport tinygrad as tg\nfrom tinygrad import Tensor, dtypes\n\n\nLazy tensors and UOps\nTinygrad API is quite similar to PyTorch, with some quirks.\n\na = Tensor.ones(10, 10)\nb = a + 2\nb\n\n&lt;Tensor &lt;UOp CPU (10, 10) float (&lt;Ops.ADD: 48&gt;, None)&gt; on CPU with grad None&gt;\n\n\nTinyGrad is lazy - it does not perform any computation until explicitly asked to.\nInstead, it saves the operations required to get the result as a tree of UOps:\n\nb.lazydata # It's called `lazydaya` for historic reasons. Rename to `Tensor.uops`?\n\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n  UOp(Ops.EXPAND, dtypes.float, arg=(10, 10), src=(\n    UOp(Ops.RESHAPE, dtypes.float, arg=(1, 1), src=(\n      UOp(Ops.CONST, dtypes.float, arg=1.0, src=(\n        x3:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n          UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),)),)),\n  UOp(Ops.EXPAND, dtypes.float, arg=(10, 10), src=(\n    UOp(Ops.RESHAPE, dtypes.float, arg=(1, 1), src=(\n      UOp(Ops.CONST, dtypes.float, arg=2.0, src=(\n         x3,)),)),)),))\n\n\nThe ADD UOp has 2 sources, both being constants (1 and 2) that are both reshaped to (1, 1) and expanded to shape (10, 10).\nThe CONST UOp takes the value as argument, and has a VIEW UOp as it’s source, which in turn sources from a DEVICE Uop.\nNote that x3:= walrus assignment, and x3 being reused for the second CONST UOp.\n\nfrom tinygrad.ops import UOp, Ops\n\nWe will take a detailed look at UOps in the next chapter, but for now, let’s see how to actually compute the value of b.\n\nKernels on CPU\n\n# This runs the computations needed to get the value of the tensor\n# It does not get realized without the .contiguous() though (TODO: Explain why)\n# Also, should .contiguous() just always be part of .realize()?\nb_realized = b.contiguous().realize()\n\nopened device CPU from pid:958683\nE_25_4\n 0: (25, 4)                   float.ptr(100)       (4, 1)\n[Opt(op=OptOps.UPCAST, axis=0, arg=4)]\ntypedef float float4 __attribute__((aligned(16),vector_size(16)));\nvoid E_25_4(float* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 25; ridx0++) {\n    *((float4*)((data0+(ridx0&lt;&lt;2)))) = (float4){3.0f,3.0f,3.0f,3.0f};\n  }\n}\n*** CPU        1 E_25_4                                    arg  1 mem  0.00 GB tm      3.28us/     0.00ms (     0.00 GFLOPS    0.1|0.1     GB/s) \n\n\nThe debug output gives us a glimpse into how tinygrad performs the computations. It will take the UOp tree, perform a number of transformations on it, and creates one or more kernels - functions that run on the device (potentially many instances in parallel) and do the actual computation.\nIn this case, the device is CPU, which means the kernel will be just plain sequential C code, which will be compiled with clang into a small piece of position-independent binary, then loaded and executed using ctypes.\nThe float4 is a common optimization that you see on both CPU and GPU - it’s more optimal to access memory in 128-byte chunks (4 32-bit floats) at a time, so TinyGrad is being smart here. The optimal number might be device-specific, but 128 bytes is common.\nAnd of course, since we used constants in our computation, there was no need to add 1+2 - TinyGrad was able to just fill the output with the correct value.\nIf we ran it on an NVida GPU, it would instead generate and run CUDA code, same for other devices.\nWe will cover the details of transformations done on the UOps tree at a later time, but for now, let’s look at the result.\nHere is the buffer that contains the data:\n\nprint(type(b_realized.lazydata.base.realized))\n\nb_realized.lazydata.base.realized\n\n&lt;class 'tinygrad.device.Buffer'&gt;\n\n\n&lt;buf real:True device:CPU size:100 dtype:dtypes.float offset:0&gt;\n\n\nSince we used the CPU device, it’s in CPU memory, and we can peek into it directly using memoryview\n\nview = memoryview(b_realized.lazydata.base.realized._buf)\nview[:4].hex()\n\n'00004040'\n\n\n0x00004040 is the hex for float32 ‘3.0’. Let’s use numpy to get a better view.\n\nimport numpy as np\n\n# Note: The buffer is shapeless, so we use `.reshape()` to bring it back to the correct shape\nnp.frombuffer(view, dtype=np.float32).reshape(b.shape)\n\narray([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)\n\n\n\n\n.numpy() and .tolist()\nOf course there is a more convenient way to access the result from Python - use .numpy() on the tensor.\nThis will make sure the tensor ends up on CPU, realize it, and will give the result the correct shape and dtype.\n.numpy() will allso create a copy of the data, so the memory buffer does not just disappear from under our feer when the tensor gets garbage collected.\n\nb.numpy()\n\n*** CPU        2 E_25_4                                    arg  1 mem  0.00 GB tm      8.03us/     0.01ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n\n\narray([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)\n\n\nOr you can use .tolist() to convert it to a python list (or a list of liss of lists … for the correct number of dimensions)\n\nb.tolist()\n\n*** CPU        3 E_25_4                                    arg  1 mem  0.00 GB tm      8.17us/     0.02ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n\n\n[[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0]]\n\n\n\n\nKernels on GPU\nTinyGrad has a concept of “default device”, which we set using the “CPU” env variable in the beginning of the notebook.\nThe device and dtype can be set when creating the tensor, and you can also use .to() to copy the tensor to a different device.\n\na = Tensor(1, device=\"CUDA\", dtype=tg.dtypes.float16)\na\n\n&lt;Tensor &lt;UOp CUDA () half (&lt;Ops.CONST: 74&gt;, None)&gt; on CUDA with grad None&gt;\n\n\n\na.to(device=\"CPU\")\n\n&lt;Tensor &lt;UOp CPU () half (&lt;Ops.COPY: 9&gt;, None)&gt; on CPU with grad None&gt;\n\n\nLet’s have a look at a CUDA kernel for the same computation\n\na = Tensor.ones((10, 10), device=\"CUDA\")\nb = a + 2\nb.numpy()\n\nopened device CUDA from pid:958683\nE_25_4n1\n 0: (25, 4)                   float.ptr(100)       (4, 1)\n[Opt(op=OptOps.UPCAST, axis=0, arg=4)]\n#define INFINITY (__int_as_float(0x7f800000))\n#define NAN (__int_as_float(0x7fffffff))\nextern \"C\" __global__ void __launch_bounds__(1) E_25_4n1(float* data0) {\n  int gidx0 = blockIdx.x; /* 25 */\n  *((float4*)((data0+(gidx0&lt;&lt;2)))) = make_float4(3.0f,3.0f,3.0f,3.0f);\n}\n*** CUDA       4 E_25_4n1                                  arg  1 mem  0.00 GB tm     25.60us/     0.05ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n*** CPU        5 copy      400,     CPU &lt;- CUDA            arg  2 mem  0.00 GB tm     55.73us/     0.10ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n\n\narray([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)\n\n\nWe see a similar pattern, but since the GPU is highly parallel, TinyGrad decided to create 25 threads, each setting its own 4-float chunk.\n\nIf you are familiar with CUDA, you might notice that we created 25 separate thread blocks with 1 thread each instead of 1 block with 25 threads, which is definitely suboptimal.\n\n\nb_realized = b.contiguous().realize()\n\n*** CUDA       6 E_25_4n1                                  arg  1 mem  0.00 GB tm    574.21us/     0.68ms (     0.00 GFLOPS    0.0|0.0     GB/s) \n\n\n\nprint(b_realized.lazydata.base.realized)\nprint(b_realized.lazydata.base.realized._buf)\n\n&lt;buf real:True device:CUDA size:100 dtype:dtypes.float offset:0&gt;\nc_ulong(140551080902656)\n\n\nAs we can see, the output buffer is on the GPU this time, so we can’t access it from the CPU directly.\nBut trust me, the data is definitely there. Let’s use PyCuda to peek into the GPU memory.\n\nimport pycuda\nimport pycuda.driver as cuda\nimport numpy as np\n\n# Create a numpy array to hold the data (100 32-bit floats)\ncpu_array = np.empty((10, 10), dtype=np.float32)\n\n# Copy data from GPU to CPU\ncuda.memcpy_dtoh(cpu_array, b_realized.lazydata.base.realized._buf.value)\n\ncpu_array\n\narray([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)\n\n\nTinyGrad is a complex beast, so it’s normal if this intro left you with more questions than answers. :)",
    "crumbs": [
      "0 - Introduction"
    ]
  },
  {
    "objectID": "uops_annotated.html",
    "href": "uops_annotated.html",
    "title": "1.1 - UOp Summary",
    "section": "",
    "text": "UOps form the intermediate representation (IR) in tinygrad’s computation graph after the initial Tensor operations are defined and before final code generation. They represent a lower-level, more explicit graph that undergoes several optimization and transformation passes.\n\n\nMeta / Framework Ops\nThese UOps are primarily used by the tinygrad framework itself for graph structure, scheduling, device management, and metadata, rather than direct computation on tensor data.\n\nSINK\n\nDescription: Represents the final output(s) of a computation graph or kernel. It acts as a root node for graph traversal and scheduling.\nPurpose: Marks the end of a computation path that needs to be realized. Used to define the boundaries of kernels during scheduling.\nArgs: Optional KernelInfo containing metadata about the kernel (name, local dims, etc.).\nSources: One or more UOps that produce the final results (often STORE or ASSIGN ops).\nStage: Graph Construction, Scheduling (marks kernel boundaries).\nNotes: Essential for defining what needs to be computed. Multiple UOps can feed into a single SINK. Simplified away before final rendering.\n\n\n\nKERNEL\n\nDescription: An internal UOp used during scheduling to encapsulate the operations belonging to a single kernel launch.\nPurpose: Groups UOps together that will be executed as one unit on the target device. Helps manage kernel boundaries and dependencies.\nArgs: Kernel object containing the kernel’s AST and metadata.\nSources: UOps representing the inputs required by the kernel (often BUFFER or other KERNEL outputs via ASSIGN).\nStage: Scheduling.\nNotes: This is a temporary node used by the scheduler and is expanded/replaced before final code generation.\n\n\n\nNAME\n\nDescription: Assigns a name (string) to a UOp, typically used for naming kernels or variables in the generated code.\nPurpose: Code readability and debugging. Allows generated kernels/variables to have meaningful names based on the high-level operations.\nArgs: str - the desired name.\nSources: Usually none, but can wrap other ops during specific transformations.\nStage: Scheduling, Code Generation.\nNotes: Often associated with SINK or DEFINE_VAR.\n\n\n\nDEVICE\n\nDescription: Represents a target device (e.g., “CPU”, “CUDA:0”).\nPurpose: Specifies the device for memory allocation or computation. Used as a source for BUFFER, COPY, CONST.\nArgs: str - the device name.\nSources: None.\nStage: Graph Construction, Scheduling.\n\n\n\nMULTI\n\nDescription: Represents a tensor sharded across multiple devices.\nPurpose: Manages data parallelism by holding references to UOps representing shards on different devices.\nArgs: (Optional[int], tuple[bool, ...]) - The sharding axis (or None) and a tuple indicating which shards hold real data (vs padding/zeros).\nSources: Multiple UOps, each representing a shard on a specific device.\nStage: Graph Construction (created by Tensor.shard).\nNotes: Handled by a specific rewrite pass (get_multi_map) to distribute operations across shards.\n\n\n\nCOPY\n\nDescription: Copies data from one buffer/device to another.\nPurpose: Explicit data movement between devices or cloning a buffer.\nArgs: bool - clone=True forces a new allocation even on the same device.\nSources: (DEVICE, UOp) - Target device and the UOp to copy from.\nStage: Graph Construction, Scheduling.\nNotes: Simplified away if source and destination devices are the same and clone=False. Can become BufferXfer for optimized transfers.\n\n\n\nASSIGN\n\nDescription: Represents an assignment operation. At the Tensor level, it signifies tensor.assign(other_tensor). At the UOp level after lowering, it often represents updating an accumulator (DEFINE_ACC).\nPurpose: In-place modification of data (conceptually) or accumulator updates.\nArgs: None.\nSources: (target, value) - The UOp being assigned to (often BUFFER or DEFINE_ACC) and the new value UOp.\nStage: Graph Construction, Lowering (Accumulator updates).\nNotes: High-level ASSIGN is often lowered to STORE or kernel operations. Low-level ASSIGN is crucial for reductions.\n\n\n\nBIND\n\nDescription: Binds a symbolic Variable (DEFINE_VAR) to a concrete integer value (CONST).\nPurpose: Resolves symbolic dimensions or parameters at runtime or during JIT compilation.\nArgs: None.\nSources: (DEFINE_VAR, CONST) - The variable and the constant value it’s bound to.\nStage: Graph Construction (Symbolic Shapes), JIT.\nNotes: Removed during scheduling/lowering by substituting the variable with its bound constant value.\n\n\n\nDEFINE_VAR\n\nDescription: Defines a symbolic variable, typically representing a dimension size.\nPurpose: Allows for operations on tensors with shapes that are not known at compile time (symbolic shapes).\nArgs: (name: str, min_val: int, max_val: int) - Name and range of the variable.\nSources: None.\nStage: Graph Construction (Symbolic Shapes).\nNotes: Usually bound using BIND before execution.\n\n\n\nUNIQUE\n\nDescription: Represents a unique identifier, typically used for buffer allocation.\nPurpose: Ensures that different BUFFER UOps representing distinct allocations get unique identities, even if they have the same size/dtype/device.\nArgs: int - A unique integer.\nSources: None.\nStage: Graph Construction (Buffer creation).\n\n\n\nEMPTY\n\nDescription: Represents an empty tensor (placeholder).\nPurpose: Used internally, often as a starting point for tensor creation before data is specified (e.g., Tensor.empty).\nArgs: None.\nSources: None.\nStage: Graph Construction.\nNotes: Usually replaced or filled quickly.\n\n\n\nNOOP\n\nDescription: A no-operation instruction.\nPurpose: Used as a placeholder or identity operation during graph transformations, often inserted and then removed by subsequent passes. Can sometimes force materialization in specific backends (e.g., before BITCAST).\nArgs: None.\nSources: (UOp,) - The UOp to pass through.\nStage: Intermediate Transformations.\n\n\n\nCUSTOM / CUSTOMI\n\nDescription: Represents a custom operation defined by a string, potentially with specific backend implementations. CUSTOMI implies inline code.\nPurpose: Allows extending tinygrad with operations not covered by standard Ops, often for backend-specific intrinsics or complex fused operations.\nArgs: str - A format string for the custom code.\nSources: Variable number of UOps, used to fill the format string.\nStage: Lowering, Code Generation.\n\n\n\n\n\nConstants\n\nCONST\n\nDescription: Represents a scalar constant value.\nPurpose: Embeds constant values directly into the computation graph.\nArgs: ConstType (int, float, bool) - The constant value.\nSources: Usually none, but can have a VIEW(DEVICE) source to indicate device placement.\nStage: All stages.\nNotes: Tensor.full creates CONST ops expanded to the correct shape.\n\n\n\nVCONST\n\nDescription: Represents a vector constant value.\nPurpose: Similar to CONST but for vector types.\nArgs: tuple[ConstType, ...] - The tuple of constant values.\nSources: Usually none.\nStage: All stages.\nNotes: Often lowered to VECTORIZE(CONST, CONST, ...) during rendering.\n\n\n\n\n\nMovement Ops\nThese ops change the logical view (shape, strides, offset, mask) of data without necessarily moving or copying it in memory immediately. They primarily manipulate the ShapeTracker associated with a buffer.\n\nRESHAPE\n\nDescription: Changes the shape of the tensor while preserving the total number of elements.\nPurpose: Modifies the logical dimensions.\nArgs: tuple[sint, ...] - The new shape.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops.\n\n\n\nPERMUTE\n\nDescription: Reorders the dimensions of the tensor.\nPurpose: Changes the logical order of axes.\nArgs: tuple[int, ...] - The permutation order.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops.\n\n\n\nEXPAND\n\nDescription: Expands dimensions of size 1 to a larger size.\nPurpose: Broadcasting.\nArgs: tuple[sint, ...] - The target shape with expanded dimensions.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops. Stride becomes 0 for expanded dimensions.\n\n\n\nPAD\n\nDescription: Adds padding to the tensor along specified dimensions.\nPurpose: Increases the size of dimensions, typically for convolutions or alignment.\nArgs: tuple[tuple[sint, sint], ...] - Padding amounts (before, after) for each dimension.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops. Adjusts offset and mask.\n\n\n\nSHRINK\n\nDescription: Shrinks the tensor along specified dimensions by selecting a sub-region.\nPurpose: Cropping or selecting parts of a tensor.\nArgs: tuple[tuple[sint, sint], ...] - Start and end indices (exclusive) for shrinking each dimension.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops. Adjusts offset and mask.\n\n\n\nFLIP\n\nDescription: Reverses the order of elements along specified dimensions.\nPurpose: Data augmentation or specific algorithms requiring reversed views.\nArgs: tuple[bool, ...] - A boolean tuple indicating which axes to flip.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into VIEW ops. Modifies strides and offset.\n\n\n\n\n\nLowering / Indexing Ops\nThese UOps appear during the lowering process, translating logical views and operations into memory accesses and validity checks.\n\nVIEW\n\nDescription: Represents a logical view (ShapeTracker) applied to a base buffer UOp. This is the primary way movement ops are represented after initial lowering.\nPurpose: Encapsulates shape, stride, offset, and mask information without creating new data. Connects logical tensor operations to underlying buffer representations.\nArgs: ShapeTracker - The view information.\nSources: (UOp,) - The base UOp (often BUFFER, CONST, or another VIEW). Can also have a DEVICE source for CONST.\nStage: Lowering, Scheduling.\nNotes: Multiple VIEW ops are merged into one. CONTIGUOUS ops often trigger realization before a VIEW. VIEW ops are pushed towards memory operations (LOAD/STORE) or constants during simplification.\n\n\n\nINDEX\n\nDescription: Calculates a memory address/index based on a buffer and logical indices, potentially applying a validity mask.\nPurpose: Translates multi-dimensional logical indexing into a linear memory index for LOAD and STORE. Encapsulates the ShapeTracker.to_indexed_uops logic.\nArgs: None.\nSources: (buffer_uop, logical_indices_uop, Optional[valid_uop]) - The buffer (e.g., DEFINE_GLOBAL), the calculated index expression, and an optional validity mask (dtypes.bool).\nStage: Lowering (post rewrite_shapetracker_with_index), Codegen.\nNotes: This is part of the “new style” load/store introduced to simplify rendering.\n\n\n\nVALID\n\nDescription: Represents the validity mask derived from a ShapeTracker’s mask attribute.\nPurpose: Computes whether a given logical index corresponds to a valid element within the original (unpadded, unshrunk) data. Used for masking operations, especially loads/stores near boundaries.\nArgs: None.\nSources: (VIEW,) - The VIEW UOp containing the ShapeTracker with mask information.\nStage: Lowering.\nNotes: Often simplified or combined with index calculations. Becomes the valid part of INDEX or the gate in LOAD/STORE.\n\n\n\nGEP (Get Element Pointer)\n\nDescription: Extracts specific elements from a vector UOp.\nPurpose: Accessing individual lanes of a vectorized operation or constant.\nArgs: tuple[int, ...] - The indices of the elements to extract.\nSources: (UOp,) - The vector UOp.\nStage: Codegen, Final Rendering.\nNotes: The inverse of VECTORIZE. Allows scalar operations on elements previously combined into a vector.\n\n\n\n\n\nMemory Ops\nThese UOps deal directly with memory allocation, definition, and access.\n\nBUFFER\n\nDescription: Represents a raw memory buffer allocated on a specific device. This is the “base” UOp for most tensor data after initial allocation.\nPurpose: Holds the reference to the actual allocated memory used by tensors.\nArgs: int - Size of the buffer in elements.\nSources: (DEVICE, UNIQUE) - The device and a unique identifier.\nStage: Graph Construction, Scheduling.\nNotes: BUFFER UOps map to Buffer objects which manage the actual memory. BUFFER itself doesn’t have a ShapeTracker; VIEW ops are applied on top.\n\n\n\nBUFFER_VIEW\n\nDescription: Represents a view of an existing BUFFER UOp, potentially with an offset. Introduced for DISK buffers.\nPurpose: Allows accessing parts of a larger buffer (e.g., a file on disk) without loading the entire thing.\nArgs: (size: int, offset: int) - Size in elements and offset in elements from the base buffer.\nSources: (BUFFER,) - The base buffer.\nStage: Scheduling (Specific backends like DISK).\n\n\n\nDEFINE_GLOBAL\n\nDescription: Defines a global buffer in the kernel arguments.\nPurpose: Declares input/output buffers passed into the kernel.\nArgs: int (optional, buffer index) or None.\nSources: None.\nStage: Final Lowering (inside linearize_uop), Codegen.\nNotes: Has a PtrDType. The renderer uses this to generate kernel signatures.\n\n\n\nDEFINE_LOCAL\n\nDescription: Defines a buffer in local (shared) memory.\nPurpose: Allocation of shared memory for intermediate results accessible by threads within a workgroup.\nArgs: str - Name for the local buffer.\nSources: None.\nStage: Lowering, Codegen.\nNotes: Has a PtrDType with local=True. Requires synchronization (BARRIER).\n\n\n\nDEFINE_ACC\n\nDescription: Defines an accumulator register or variable, typically initialized to an identity element and used within reduction loops.\nPurpose: Holds the intermediate state during reduction operations.\nArgs: (int,) - An accumulator index/identifier.\nSources: (initial_value, *reduce_ranges) - The identity element (CONST) and the RANGE UOps defining the reduction loops.\nStage: Lowering (created from REDUCE_AXIS).\nNotes: Lowered REDUCE_AXIS becomes DEFINE_ACC -&gt; ALU -&gt; ASSIGN(acc).\n\n\n\nLOAD\n\nDescription: Loads data from memory (global or local).\nPurpose: Reading data from a buffer into registers/variables for computation.\nArgs: Optional load configuration (e.g., cache hints, not currently used extensively).\nSources:\n\nOld style (pre-linearize): (BUFFER, VIEW, Optional[STORE]) - Buffer, ShapeTracker view, optional dependency.\nNew style (post-linearize): (INDEX, Optional[alt_value], Optional[gate], Optional[BARRIER]) - Indexed address, value if gate is false, gate condition, barrier dependency.\n\nStage: Lowering, Codegen.\nNotes: Validity checks (from ShapeTracker masks) are incorporated into the INDEX or gate.\n\n\n\nSTORE\n\nDescription: Stores data into memory (global or local).\nPurpose: Writing computation results back to a buffer.\nArgs: None.\nSources:\n\nOld style (pre-linearize): (BUFFER, VIEW, value) - Buffer, ShapeTracker view, value to store.\nNew style (post-linearize): (INDEX, value, Optional[gate]) - Indexed address, value to store, optional gate condition.\n\nStage: Lowering, Codegen.\nNotes: Often the final operation(s) feeding into a SINK.\n\n\n\n\n\nCore Compute / ALU Ops\nThese perform basic element-wise arithmetic, logical, comparison, and transcendental operations. They generally expect sources to have the same shape and dtype (except for comparisons and specific cases like WHERE).\n\nUnary (EXP2, LOG2, SIN, SQRT, RECIP, NEG)\n\nDescription: Apply standard unary mathematical functions.\nPurpose: Element-wise computation.\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: All stages.\nNotes: NEG is often represented as x * -1. Transcendental ops (EXP2, LOG2, SIN) might be rewritten to use approximations or backend-specific implementations.\n\n\n\nBinary (ADD, MUL, IDIV, MAX, MOD, CMPLT, CMPNE, XOR, SHL, SHR, OR, AND, SUB, FDIV, POW)\n\nDescription: Apply standard binary mathematical or logical functions.\nPurpose: Element-wise computation between two operands.\nArgs: None.\nSources: (UOp, UOp) - The two input UOps.\nStage: All stages.\nNotes:\n\nCommutative ops (ADD, MUL, MAX, CMPNE, XOR, AND, OR) might have sources swapped during optimization.\nIDIV is integer division (truncates towards zero). FDIV (internal) represents float division (x / y), often lowered to x * RECIP(y).\nCMPLT/CMPNE output bool dtype.\nSUB is often represented as x + (-y).\n\n\n\n\nTernary (WHERE, MULACC)\n\nDescription: Apply standard ternary functions.\nPurpose: Element-wise computation involving three operands.\nArgs: None.\nSources:\n\nWHERE: (condition, true_value, false_value) - Condition must be bool.\nMULACC: (a, b, c) - Computes a * b + c.\n\nStage: All stages.\nNotes: MULACC (Multiply-Accumulate) can often map efficiently to hardware FMA (Fused Multiply-Add) instructions.\n\n\n\nCAST\n\nDescription: Changes the data type of the elements.\nPurpose: Type conversion (e.g., float to int, int to float, float16 to float32).\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: All stages.\nNotes: Behavior depends on the source and destination types (truncation, rounding, etc.).\n\n\n\nBITCAST\n\nDescription: Reinterprets the bits of the elements as a different data type of the same size.\nPurpose: Low-level manipulation, often used for specific algorithms or interacting with hardware types (e.g., float &lt;-&gt; int).\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: All stages.\nNotes: Does not change the underlying bit pattern, only the type interpretation. Requires source and destination dtypes to have the same itemsize.\n\n\n\n\n\nReduce Ops\n\nREDUCE_AXIS\n\nDescription: Performs a reduction operation (like sum, max) along specified axes.\nPurpose: Aggregates data across dimensions.\nArgs: (Ops, tuple[int, ...]) - The reduction operation (e.g., Ops.ADD, Ops.MAX) and the axes to reduce.\nSources: (UOp,) - The input UOp.\nStage: High-level, Lowering.\nNotes: Lowered into a combination of DEFINE_ACC, RANGE loops, ALU ops, and ASSIGN. Can be split or grouped during optimization.\n\n\n\nWMMA (Warp Matrix Multiply Accumulate)\n\nDescription: Represents a hardware-accelerated matrix multiplication operation performed cooperatively by a group of threads (warp/wavefront).\nPurpose: Leverages specialized hardware units (like Tensor Cores on NVIDIA GPUs, Matrix Cores on AMD GPUs, AMX on Apple Silicon) for high-performance matrix multiplication.\nArgs: (name, dims, dtype_in, dtype_out, device, threads, upcast_axes, reduce_axes) - Detailed configuration for the WMMA operation.\nSources: (A, B, C) - Input matrices A, B, and accumulator C.\nStage: Codegen (inserted by optimization passes like apply_tensor_cores).\nNotes: Highly backend-specific. Requires specific data layouts and operand types.\n\n\n\n\n\nControl Flow Ops\n\nRANGE\n\nDescription: Represents a loop range, typically used for iterating over tensor dimensions.\nPurpose: Defines the iteration space for loops in the generated code.\nArgs: int - An identifier for the loop variable (axis index).\nSources: (start, end) - UOps defining the start (inclusive) and end (exclusive) of the loop.\nStage: Lowering (created by get_index), Codegen.\nNotes: Rendered as for loops in C-style backends. Used as sources for DEFINE_ACC.\n\n\n\nIF\n\nDescription: Represents a conditional block.\nPurpose: Conditional execution in the generated code.\nArgs: None.\nSources: (condition, Optional[BARRIER]) - The boolean condition UOp and an optional barrier dependency.\nStage: Lowering, Codegen.\nNotes: Requires a corresponding ENDIF. Code between IF and ENDIF is executed only if the condition is true. Used for gating LOAD/STORE.\n\n\n\nENDRANGE / ENDIF\n\nDescription: Marks the end of a RANGE or IF block, respectively.\nPurpose: Defines the scope of loops and conditionals.\nArgs: None.\nSources: (RANGE,) or (IF,) - The corresponding start block UOp.\nStage: Lowering, Codegen.\nNotes: Rendered as closing braces } in C-style backends.\n\n\n\nBARRIER\n\nDescription: Represents a synchronization point, typically for local (shared) memory.\nPurpose: Ensures that all threads in a workgroup reach this point before any thread proceeds, making writes to shared memory visible to other threads.\nArgs: None.\nSources: Usually (STORE,) operations to local memory that need to complete before subsequent reads. Can also be a source for IF.\nStage: Lowering, Codegen.\nNotes: Essential for correctness when using shared memory for reductions or caching.\n\n\n\n\n\nVectorization / Structure Ops\n\nVECTORIZE\n\nDescription: Combines multiple scalar UOps into a single vector UOp.\nPurpose: Explicitly represents vector operations for backends that support them. Also used internally as a structural node (e.g., for VCONST).\nArgs: None.\nSources: (scalar_uop_1, scalar_uop_2, ...) - The scalar UOps to be combined.\nStage: Codegen, Final Rendering.\nNotes: The inverse of GEP. Renderers translate this into vector types and operations if supported.\n\n\n\nUNROLL\n\nDescription: Represents loop unrolling during code generation. Structurally similar to VECTORIZE but used for axes that are fully unrolled rather than vectorized.\nPurpose: Optimization to reduce loop overhead by duplicating the loop body. Also used structurally in Tensor Core lowering.\nArgs: tuple[tuple[int, int], ...] - The axes being unrolled and their sizes ((axis, size), ...).\nSources: (UOp,) - The UOp whose unrolled axes are represented.\nStage: Codegen (inserted by Kernel.apply_opt), Expansion.\nNotes: The expander pass (do_expand) processes UNROLL ops, effectively performing the unrolling by duplicating and adjusting source UOps.\n\n\n\nCONTRACT\n\nDescription: Represents the contraction (summation) part of a vectorized reduction or Tensor Core operation. Inverse of UNROLL for specific axes.\nPurpose: Used structurally during the expansion/contraction passes related to vectorization and Tensor Cores.\nArgs: tuple[tuple[int, int], ...] - The axes being contracted and their sizes ((axis, size), ...).\nSources: (UOp,) - The UOp (often an UNROLL) being contracted.\nStage: Expansion.\n\n\n\nCAT\n\nDescription: Concatenates multiple vector UOps. (Internal use).\nPurpose: Used during expansion/vectorization passes to combine vectors.\nArgs: None.\nSources: Multiple vector UOps.\nStage: Expansion.\nNotes: Lowered to VECTORIZE with GEP sources before rendering.\n\n\n\n\n\nInternal / Removed Early Ops\nThese ops exist briefly during graph construction or early simplification but are typically removed before significant lowering or scheduling.\n\nCONTIGUOUS / CONTIGUOUS_BACKWARD\n\nDescription: Marks a requirement for the data to be in contiguous memory layout. CONTIGUOUS_BACKWARD affects the backward pass only.\nPurpose: Triggers realization or specific memory layouts. Often used before operations that require contiguous inputs (like some COPY operations or external calls).\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: Graph Construction, Early Simplification.\nNotes: Usually removed by simplification rules (sym) if the input is already known to be contiguous or if the operation can be achieved via a VIEW.\n\n\n\nDETACH\n\nDescription: Removes the UOp from the computation graph for gradient calculation purposes.\nPurpose: Implements Tensor.detach().\nArgs: None.\nSources: (UOp,) - The input UOp.\nStage: Graph Construction, Early Simplification.\nNotes: Removed by simplification rules (sym).\n\n\n\nBLOCK / BLOCKSTART / BLOCKFORK / BLOCKEND\n\nDescription: Internal ops used by linearize_uop to group UOps into basic blocks based on control flow (RANGE, IF).\nPurpose: Facilitates structuring the UOp list into code blocks for rendering.\nArgs: BasicBlock or int.\nSources: Variable, depending on the block structure.\nStage: Codegen (linearize_uop).\nNotes: These are temporary structural nodes used only within the linearization process and do not appear in the final UOp list passed to the renderer.\n\n\nThis summary covers the primary UOps and their roles. The exact behavior and interactions can be complex, as they are subject to numerous rewrite rules during the compilation process.",
    "crumbs": [
      "1.1 - UOp Summary"
    ]
  },
  {
    "objectID": "shapetracker.html",
    "href": "shapetracker.html",
    "title": "2 - View and ShapeTracker",
    "section": "",
    "text": "So far we have been making scalar UOps that don’t have a shape associated with them.\nWhile we have been getting away with it so far, the UOp trees we made are not really valid without a shape.\nimport os\nos.environ[\"CPU\"] = \"1\"\nos.environ[\"DEBUG\"]=\"4\"\n\nfrom tinygrad import  dtypes\nfrom tinygrad.ops import UOp, Ops\na = UOp.const(dtypes.float, 1)\na\n\nUOp(Ops.CONST, dtypes.float, arg=1.0, src=())\ntry:\n    print(a.shape)\nexcept Exception as e:\n    print_last_frame_context(e)\n\nAssertionError in /home/xl0/work/projects/grads/tinygrad/tinygrad/helpers.py:61 in unwrap()\n\nCode context:\n       59   return ret\n       60 def unwrap(x:Optional[T]) -&gt; T:\n---&gt;   61   assert x is not None\n       62   return x\n       63 def get_single_element(x:list[T]) -&gt; T:\nAnother thing we were missing is the device:\ntry:\n    print(a.device)\nexcept Exception as e:\n    print_last_frame_context(e)\n\nAssertionError in /home/xl0/work/projects/grads/tinygrad/tinygrad/helpers.py:61 in unwrap()\n\nCode context:\n       59   return ret\n       60 def unwrap(x:Optional[T]) -&gt; T:\n---&gt;   61   assert x is not None\n       62   return x\n       63 def get_single_element(x:list[T]) -&gt; T:\nLet’s fix this real quick\nfrom tinygrad.shape.shapetracker import ShapeTracker, View\n\na = UOp.const(dtypes.float, 1).replace(src=(\n        UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker.from_shape( (0,) ), src=(\n            UOp(Ops.DEVICE, dtypes.void, arg=\"CPU\", src=()),)),))\na\n\nUOp(Ops.CONST, dtypes.float, arg=1.0, src=(\n  UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(0,), strides=(0,), offset=0, mask=None, contiguous=True),)), src=(\n    UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),))\na.shape\n\n(0,)\na.device\n\n'CPU'\nLooks better.\nNow, what’s up with that ShapeTracker and View. Let’s start with the later.",
    "crumbs": [
      "2 - View and ShapeTracker"
    ]
  },
  {
    "objectID": "shapetracker.html#shape-and-stride",
    "href": "shapetracker.html#shape-and-stride",
    "title": "2 - View and ShapeTracker",
    "section": "Shape and Stride",
    "text": "Shape and Stride\nYou are probably familiar with how shape and strides work in Pytorch or Numpy:\n\nimport torch\n\n\na = torch.linspace(0, 31, 32, dtype=torch.int32).view(4, 8)\na\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31]], dtype=torch.int32)\n\n\n\na.shape\n\ntorch.Size([4, 8])\n\n\nThe shape defined the, well, the shape of the array. It can have any number of dimensions (2 in this case), and each dimension has its size.\nA Tensor is just a linear array, and the shape is there for convenience, because we usually want to work with multi-dimensional data.\nWe can change the shape, as long as the number of elements in the new shape stays the same.\n\nb = a.view(2,4,4) # This creates a view that refers to the same data, but now it's seen as a 3-d array.\nb\n\ntensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15]],\n\n        [[16, 17, 18, 19],\n         [20, 21, 22, 23],\n         [24, 25, 26, 27],\n         [28, 29, 30, 31]]], dtype=torch.int32)\n\n\nThe stride tells us how many elements do we need to move in the underlying 1-d array (base), to get to the next element in the given dimension.\nFor out 2x4x4 array, to move 1 element in the row (last dimension), we need to move … 1 element in the base.\nAnd to move by one element in the column dimension, we need to move by 4 elements in the base, because each row is 4 elements.\n\nThis is the standard C, or row-major order format for multidimensional data.\n\n\n\n\n\nRow-major and Column-major order\n\n\n\n\nYou might have seen references to the F, or column-major order at some point. Historically this is how data was stored in Fortran, and I’m sure they had their reasons for it, but it’s definitely less intuitive.\n\nTo move in the next dimension, we’d have to skip 4 columns, and for each column we skip 4 elements, so 16 in total:\n\nb.stride()\n\n(16, 4, 1)\n\n\nNow, if the stride always matched the shape, things would be boring. We can set the stride independently.\nLet’s go back to our 4x8 view to make things easies. In this case we need to skip 8 elements to move by one row:\n\nprint(a)\nprint(\"Shape: \",a.shape)\nprint(\"Stride:\",a.stride())\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31]], dtype=torch.int32)\nShape:  torch.Size([4, 8])\nStride: (8, 1)\n\n\nWhat if we want to create a view that would skip every other element in the rows? We can do this by creatig a view with shape (torch refers to it as size) 4x4, and stride (8, 2)!\n\nc = a.as_strided(size=(4,4), stride=(8, 2))\nc\n\ntensor([[ 0,  2,  4,  6],\n        [ 8, 10, 12, 14],\n        [16, 18, 20, 22],\n        [24, 26, 28, 30]], dtype=torch.int32)\n\n\nWe can also specify an offset from the start of the base array. This will give us the odd elements in each row:\n\na.as_strided(size=(4,4), stride=(8, 2), storage_offset=1)\n\ntensor([[ 1,  3,  5,  7],\n        [ 9, 11, 13, 15],\n        [17, 19, 21, 23],\n        [25, 27, 29, 31]], dtype=torch.int32)\n\n\nLet’s create a view that has the diagonal elements of a (0, 9, 18, 27)\n\na.as_strided(size=(4,), stride=(9,))\n\ntensor([ 0,  9, 18, 27], dtype=torch.int32)\n\n\nAnother fun thing we can do - set one of more of the strides to 0, to duplicate (broadcast) dimensions:\n\nd = torch.linspace(1, 4, 4, dtype=torch.int32)\nd\n\ntensor([1, 2, 3, 4], dtype=torch.int32)\n\n\n\nd.as_strided(size=(4,4), stride=(1, 0))\n\ntensor([[1, 1, 1, 1],\n        [2, 2, 2, 2],\n        [3, 3, 3, 3],\n        [4, 4, 4, 4]], dtype=torch.int32)\n\n\nFor each step in the output column, we take 1 step in the base, and for each step in the output row, we don’t take any steps at all!\nThat’s how .full() works - it creates 1 single element, and makes all elements in the Tensor refer to it by setting the strides to 0.\n\ne = torch.Tensor([1]).to(torch.int32)\ne\n\ntensor([1], dtype=torch.int32)\n\n\n\ne.as_strided(size=(4,4), stride=(0,0))\n\ntensor([[1, 1, 1, 1],\n        [1, 1, 1, 1],\n        [1, 1, 1, 1],\n        [1, 1, 1, 1]], dtype=torch.int32)",
    "crumbs": [
      "2 - View and ShapeTracker"
    ]
  },
  {
    "objectID": "shapetracker.html#class-view",
    "href": "shapetracker.html#class-view",
    "title": "2 - View and ShapeTracker",
    "section": "class View",
    "text": "class View\nThe View class is intended to keep track of the shape and stride of the data. Let’s play with it a bit.\n\nv = View(shape=(4,8), strides=(8,1), offset=0, mask=None, contiguous=True)\nv # A normal array 4x8\n\nView(shape=(4, 8), strides=(8, 1), offset=0, mask=None, contiguous=True)\n\n\n\na\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31]], dtype=torch.int32)\n\n\n\na.as_strided(size=v.shape, stride=v.strides)\n\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14, 15],\n        [16, 17, 18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29, 30, 31]], dtype=torch.int32)\n\n\n\nv32 = v.reshape( (32,) ) # 1-d array of 32 elements\nv32\n\nView(shape=(32,), strides=(1,), offset=0, mask=None, contiguous=True)\n\n\n\na.as_strided(size=v32.shape, stride=v32.strides)\n\ntensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n       dtype=torch.int32)\n\n\n\nv_flip = v.flip( (False, True) ) # Flip the last dimension\nv_flip\n\nView(shape=(4, 8), strides=(8, -1), offset=7, mask=None, contiguous=False)\n\n\n\ntry:\n    a.as_strided(size=v_flip.shape, stride=v_flip.strides)\nexcept Exception as e:\n    print_last_frame_context(e, 0)\n\nRuntimeError in /tmp/ipykernel_795477/772861321.py:2 in &lt;module&gt;()\n\nCode context:\n---&gt;    2     a.as_strided(size=v_flip.shape, stride=v_flip.strides)\n\n\nOops, torch actually does not support negative strides. This should have looked like this:\n\na.flip((1))\n\ntensor([[ 7,  6,  5,  4,  3,  2,  1,  0],\n        [15, 14, 13, 12, 11, 10,  9,  8],\n        [23, 22, 21, 20, 19, 18, 17, 16],\n        [31, 30, 29, 28, 27, 26, 25, 24]], dtype=torch.int32)\n\n\n\nThe Mask\nNow, what’s up with the mask? It allows us to create arrays with elements that are outside of the underlying storage!\nFor example, if we want to pad a 2-d array, we don’t want to allocate a new array - just mark the padded elements as being not valid!\n\nv\n\nView(shape=(4, 8), strides=(8, 1), offset=0, mask=None, contiguous=True)\n\n\n\nv.pad(((2,2,),(2,2))) # left-right, top-bottom\n\nView(shape=(8, 12), strides=(8, 1), offset=-18, mask=((2, 6), (2, 10)), contiguous=False)\n\n\nTorch does not allow negative offsets either, but I think the idea is clear:",
    "crumbs": [
      "2 - View and ShapeTracker"
    ]
  },
  {
    "objectID": "shapetracker.html#class-shapetracker",
    "href": "shapetracker.html#class-shapetracker",
    "title": "2 - View and ShapeTracker",
    "section": "class ShapeTracker",
    "text": "class ShapeTracker\nNot all transforms can be represented with a single View.\n\nv = View.create((3,2))\nv_padded = v.pad(((1,1),(1,1)))\nv_padded\n\nView(shape=(5, 4), strides=(2, 1), offset=-3, mask=((1, 4), (1, 3)), contiguous=False)\n\n\n\nv_padded_reshaped = v_padded.reshape((20,)) # Linearize into a 1-d array\nprint(v_padded_reshaped)\n\nNone\n\n\nOops, we get a None, which means the operation could not be performed!\nIt makes sense, because we can’t specify the valid mask of the linearized result using just the start/stop indices.\nThe ShapeTracker keeps a list of sequentially applied Views.\n\nst = ShapeTracker((v,))\nst\n\nShapeTracker(views=(View(shape=(3, 2), strides=(2, 1), offset=0, mask=None, contiguous=True),))\n\n\nIf some of the views can be merged together, it will do so.\nIn this example, we reshaped 3x2 -&gt; 2x3 and flipped it along the first axis and padded it on all sides by 1.\nThis can be represented with a single View:\n\nst_rfp = st.reshape((2,3)).flip((True, False)).pad( ((1,1),(1,1)) )\nst_rfp\n\nShapeTracker(views=(View(shape=(4, 5), strides=(-3, 1), offset=5, mask=((1, 3), (1, 4)), contiguous=False),))\n\n\nIf the transformation can not be represented with a single View, the Shapetracker will keep them separate\n\nst_rfp.reshape( (20,) )\n\nShapeTracker(views=(View(shape=(4, 5), strides=(-3, 1), offset=5, mask=((1, 3), (1, 4)), contiguous=False), View(shape=(20,), strides=(1,), offset=0, mask=None, contiguous=True)))\n\n\nWe can also generate the UOp trees that represent the expressoins for indexing and valudating memory acces in the code:\n\nidx, valid = st_rfp.to_indexed_uops()\nidx\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.ADD, dtypes.int, arg=None, src=(\n    UOp(Ops.MUL, dtypes.int, arg=None, src=(\n      UOp(Ops.RANGE, dtypes.int, arg=0, src=(\n        x3:=UOp(Ops.CONST, dtypes.int, arg=0, src=()),\n        UOp(Ops.CONST, dtypes.int, arg=4, src=()),)),\n      UOp(Ops.CONST, dtypes.int, arg=-3, src=()),)),\n    UOp(Ops.RANGE, dtypes.int, arg=1, src=(\n       x3,\n      x7:=UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),)),\n   x7,))\n\n\n\nvalid\n\nUOp(Ops.AND, dtypes.bool, arg=None, src=(\n  UOp(Ops.AND, dtypes.bool, arg=None, src=(\n    UOp(Ops.AND, dtypes.bool, arg=None, src=(\n      UOp(Ops.CMPNE, dtypes.bool, arg=None, src=(\n        UOp(Ops.CMPLT, dtypes.bool, arg=None, src=(\n          x4:=UOp(Ops.RANGE, dtypes.int, arg=0, src=(\n            x5:=UOp(Ops.CONST, dtypes.int, arg=0, src=()),\n            x6:=UOp(Ops.CONST, dtypes.int, arg=4, src=()),)),\n          x7:=UOp(Ops.CONST, dtypes.int, arg=1, src=()),)),\n        x8:=UOp(Ops.CONST, dtypes.bool, arg=True, src=()),)),\n      UOp(Ops.CMPLT, dtypes.bool, arg=None, src=(\n         x4,\n        UOp(Ops.CONST, dtypes.int, arg=3, src=()),)),)),\n    UOp(Ops.CMPNE, dtypes.bool, arg=None, src=(\n      UOp(Ops.CMPLT, dtypes.bool, arg=None, src=(\n        x13:=UOp(Ops.RANGE, dtypes.int, arg=1, src=(\n           x5,\n          UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),\n         x7,)),\n       x8,)),)),\n  UOp(Ops.CMPLT, dtypes.bool, arg=None, src=(\n     x13,\n     x6,)),))\n\n\nLet’s use .render() to try and convert the UOps into equivalent C expressions:\n\nNote: .render() is not how TinyGrad normally generates the code, it’s for debug purpose only.\n\n\nidx.render()\n\n'(((ridx0*-3)+ridx1)+5)'\n\n\nAnd valid can be used to check the validity of input elements in and if statement:\n\nvalid.render()\n\n'(((((ridx0&lt;1)!=True)&(ridx0&lt;3))&((ridx1&lt;1)!=True))&(ridx1&lt;4))'",
    "crumbs": [
      "2 - View and ShapeTracker"
    ]
  },
  {
    "objectID": "uops.html",
    "href": "uops.html",
    "title": "1 - UOps",
    "section": "",
    "text": "As we saw in the previous chapter, UOps are the intermediate device-independent representation on the computation tree that sits between the user-facing Tensor and device-specific code that is generated to perform the computations.\n\nimport os\n\nos.environ[\"CPU\"] = \"1\"\n# os.environ[\"TRACEMETA\"] = \"0\"\nos.environ[\"DEBUG\"]=\"4\"\n# os.environ[\"NOOPT\"]=\"1\"\n\n\nimport tinygrad as tg\nfrom tinygrad import Tensor, dtypes\n\nUOp is the basic building block of TinyGrad, used to represent the computation tree that we create by manipulating Tensors:\nclass UOp(...):\n    op:Ops\n    dtype:DType = dtypes.void\n    src:tuple[UOp, ...] = tuple()\n    arg:Any = None\n\n    ...\n\nThe op is the type of the operation, like ADD, MUL or CONST\nThe dtype is of of the tinygrad dtypes, like dtypes.float32 or dtypes.uint8\nThe source is a tuple of UOps this UOp operates on\nThe arg changes meaning depending on the op, for example for a CONST it can be a number 3.0, or it can be the new shape for a VIEW op.\n\n\nUOp reference\nUOps are used throughout TinyGrad, some are specific to certain stages of processing (from Tensors to code), some are valid at any stage.\nTake a look at the UOp Reference to get a feel for the UOps we have.\n\n\nUOp is a singleton\nAs noted by mesozoic-egg@github, UOp is a singleton.\nIt’s implemented using a MetaClass:\nclass UOpMetaClass(type):\n  ucache:dict[tuple, weakref.ReferenceType[UOp]] = {}\n  def __call__(cls, op:Ops, dtype:DType=dtypes.void, src:tuple[UOp,...]=tuple(), arg:Any=None, _buffer:Buffer|None=None):\n    if (wret:=UOpMetaClass.ucache.get(key:=(op, dtype, src, arg), None)) is not None and (ret:=wret()) is not None: return ret\n    UOpMetaClass.ucache[key] = ref = weakref.ref(created:=super().__call__(*key))\n    ...\n    return created\n\n@dataclass(eq=False, slots=True)\nclass UOp(MathTrait, metaclass=UOpMetaClass):\n    def __del__(self):\n        if (ref:=UOpMetaClass.ucache.get(k:=(self.op, self.dtype, self.src, self.arg))) is not None:\n            ...\n            del UOpMetaClass.ucache[k]\n(TinyGrad really loves its := operators)\nThe main idea is, if you have 2 UOp (sub-)trees, it’s very easy to compare them, because the roots of both trees will be the same object if they are identical.\n\nfrom tinygrad.ops import UOp, Ops\n\n\n# Create two identical UOp trees (3 * 5 + 2)\nx1 = UOp(Ops.CONST, dtype=dtypes.int, arg=5)\nmul1 = UOp(Ops.MUL, dtype=dtypes.int, src=(UOp(Ops.CONST, dtype=dtypes.int, arg=3), x1))\nadd1 = UOp(Ops.ADD, dtype=dtypes.int, src=(mul1, UOp(Ops.CONST, dtype=dtypes.int, arg=2)))\n\n# Second tree\nx2 = UOp(Ops.CONST, dtype=dtypes.int, arg=5)\nmul2 = UOp(Ops.MUL, dtype=dtypes.int, src=(UOp(Ops.CONST, dtype=dtypes.int, arg=3), x2))\nadd2 = UOp(Ops.ADD, dtype=dtypes.int, src=(mul2, UOp(Ops.CONST, dtype=dtypes.int, arg=2)))\n\nid(add1) == id(add2)\n\nTrue\n\n\n\n# Third tree is different (3 * 5 + 1)\nx3 = UOp(Ops.CONST, dtype=dtypes.int, arg=5)\nmul3 = UOp(Ops.MUL, dtype=dtypes.int, src=(UOp(Ops.CONST, dtype=dtypes.int, arg=3), x3))\nadd3 = UOp(Ops.ADD, dtype=dtypes.int, src=(mul3, UOp(Ops.CONST, dtype=dtypes.int, arg=1)))\n\nid(add1) == id(add3)\n\nFalse\n\n\n\n\nSymbolic evaluation\nAnother cool feature of UOps - if all inputs are constants and the result is a scalar, it can be evaluated without generating any device code at all:\n\nadd1\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=3, src=()),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),\n  UOp(Ops.CONST, dtypes.int, arg=2, src=()),))\n\n\n\nadd1.simplify()\n\nUOp(Ops.CONST, dtypes.int, arg=17, src=())\n\n\nAnother way to do the same - cast the UOp to float or an int depending on dtype.\n\nint(add1)\n\n17\n\n\n\n\nUOp.render()\n.render() converts the UOp tree into a C expression.\nThis is not how TinyGrad generates the code normally, but it’s useful for debugging.\n\nadd1.render(simplify=False)\n\n'((3*5)+2)'\n\n\nThis only works for a limited subset of trees with simple operations though.\nIf the tree can not be rendered, the function returns an str(op).\n\n\nUOp creation helpers\nIn many cases, the UOp class has methods for creating specific UOps. It’s often more convenient and concise to use them\nFor example UOp.const() creates either a CONST or a VCONST (vector const, used internally for buffers), and also takes care of the arg type matching dtype:\n\nUOp.const(dtypes.float16, 2)\n\nUOp(Ops.CONST, dtypes.half, arg=2.0, src=())\n\n\nNote the arg has been converted to a float, even though we gave it an int\nThere are a few that are very straight-forward:\n\n# The SINK is the end of a computation graph\ndef sink(self, *srcs:UOp): return UOp(Ops.SINK, dtypes.void, (self,)+srcs)\n\n# Detach from the backprop\ndef detach(self): return UOp(Ops.DETACH, self.dtype, (self,))\n\ndef cast(self, dtype:DType): return UOp(Ops.CAST, dtype, (self,))\ndef bitcast(self, dtype:DType): return UOp(Ops.BITCAST, dtype, (self,))\ndef load(self, *src:UOp, **kwargs): return UOp(Ops.LOAD, src=(self,)+src, **kwargs)\ndef store(self, *src:UOp, **kwargs): return UOp(Ops.STORE, dtypes.void, (self,)+src, **kwargs)\n\n# The RANGE is actually a `for` loop\ndef range(dtype:DType, start:sint, end:sint, idx:int): return UOp(Ops.RANGE, dtype=dtype, src=(sint_to_uop(start), sint_to_uop(end)), arg=idx)\n\ndef assign(self, x:UOp): return UOp(Ops.ASSIGN, self.dtype, (self,x))\ndef contiguous(self): return self.alu(Ops.CONTIGUOUS)\ndef contiguous_backward(self): return self.alu(Ops.CONTIGUOUS_BACKWARD)\n\n\nToposort\nQuite often we need to access a UOp tree in “topological order”.\nUOp.toposort is a property function that returns a dictionary with UOps being the keys, and the values being None.\n\nNote: This emulates a sorted Set, which Python lacks\n\n\nprint(\"===== 3 * 5 + 2 =====\")\nfor o in add1.toposort.keys():\n    print(o.op, o.arg)\n\n===== 3 * 5 + 2 =====\nOps.CONST 3\nOps.CONST 5\nOps.MUL None\nOps.CONST 2\nOps.ADD None\n\n\nYou get the idea - the children always come before the parents\n\n\nOther UOp methods\nWhen reading the Tiny Grad code, you will often see other UOp methods called. To make this task easier, let’s go over some popular ones.\n\n.replace()\nDespite its name, this does not replace, but rather creates a new UOp that is a copy of the original UOp, except for the args (op, dtype, arg, src) you want to change:\n\nadd1.replace(op=Ops.SUB)\n\nUOp(Ops.SUB, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=3, src=()),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),\n  UOp(Ops.CONST, dtypes.int, arg=2, src=()),))\n\n\nadd1 did not change:\n\nadd1\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=3, src=()),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=()),)),\n  UOp(Ops.CONST, dtypes.int, arg=2, src=()),))\n\n\nUOps are actually supposed to be immutable, but this is not enforced for performance reasons:\n# NOTE: this should be frozen, but frozen is slower\n@dataclass(eq=False, slots=True)\nclass UOp(MathTrait, metaclass=UOpMetaClass):\n    ...\n\n\n\nUOp to code\n\nfrom tinygrad.engine.schedule import create_schedule_with_vars\nfrom tinygrad.engine.realize import lower_schedule_item\n\nYou did a bunch of Tensor operations, constructed a chonky UOp tree, and now you want to actually compute it.\n\na = (Tensor.full((10, 10), 1) + Tensor.full((10, 10), 2)).contiguous()\na.lazydata\n\nUOp(Ops.CONTIGUOUS, dtypes.int, arg=None, src=(\n  UOp(Ops.ADD, dtypes.int, arg=None, src=(\n    UOp(Ops.EXPAND, dtypes.int, arg=(10, 10), src=(\n      UOp(Ops.RESHAPE, dtypes.int, arg=(1, 1), src=(\n        UOp(Ops.CONST, dtypes.int, arg=1, src=(\n          x4:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n            UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),)),)),\n    UOp(Ops.EXPAND, dtypes.int, arg=(10, 10), src=(\n      UOp(Ops.RESHAPE, dtypes.int, arg=(1, 1), src=(\n        UOp(Ops.CONST, dtypes.int, arg=2, src=(\n           x4,)),)),)),)),))\n\n\nThe first step is to “schedule” the computation. This converts the UOp tree to a lover level one. You might also notice that it computed the 1+2=3.\n\nNote: We will cover the ShapeTracker in a separate chapter soon\n\n\nschedule, vars = a.schedule_with_vars()\nschedule, vars\n\n([ScheduleItem(ast=UOp(Ops.SINK, dtypes.void, arg=None, src=(\n    UOp(Ops.STORE, dtypes.void, arg=None, src=(\n      UOp(Ops.DEFINE_GLOBAL, dtypes.int.ptr(100), arg=0, src=()),\n      UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(10, 10), strides=(10, 1), offset=0, mask=None, contiguous=True),)), src=()),\n      UOp(Ops.CONST, dtypes.int, arg=3, src=(\n        UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(10, 10), strides=(0, 0), offset=0, mask=None, contiguous=False),)), src=()),)),)),)), bufs=(&lt;buf real:False device:CPU size:100 dtype:dtypes.int offset:0&gt;,), metadata=(contiguous, __add__))],\n {})\n\n\nThe next step is to convert the ScheduleItem into executable code.\n\nei = lower_schedule_item(schedule[0])\nei\n\nopened device CPU from pid:549064\nE_25_4\n 0: (25, 4)                   int.ptr(100)         (4, 1)                         ShapeTracker(views=(View(shape=(25, 4), strides=(4, 1), offset=0, mask=None, contiguous=True),))\n[Opt(op=OptOps.UPCAST, axis=0, arg=4)]\n\nvoid E_25_4(int* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 25; ridx0++) {\n    int alu0 = (ridx0&lt;&lt;2);\n    *(data0+alu0) = 3;\n    *(data0+(alu0+1)) = 3;\n    *(data0+(alu0+2)) = 3;\n    *(data0+(alu0+3)) = 3;\n  }\n}\n\n\n\nExecItem(prg=&lt;tinygrad.engine.realize.CompiledRunner object&gt;, bufs=[&lt;buf real:False device:CPU size:100 dtype:dtypes.int offset:0&gt;], metadata=(contiguous, __add__))\n\n\nThis brings the UOp tree to the lowest level, that maps ~1:1 to the generated code:\n\nfor o in ei.prg.p.uops:\n    print(o.op, o.arg, [s.arg for s in o.src if s.op == Ops.CONST] if o.src else \"\")\n\nOps.NAME E_25_4 \nOps.DEFINE_GLOBAL 0 \nOps.CONST 0 \nOps.CONST 1 \nOps.CONST 2 \nOps.CONST 3 \nOps.CONST 25 \nOps.RANGE 0 [0, 25]\nOps.SHL None [2]\nOps.INDEX None []\nOps.STORE None [3]\nOps.ADD None [1]\nOps.INDEX None []\nOps.STORE None [3]\nOps.ADD None [2]\nOps.INDEX None []\nOps.STORE None [3]\nOps.ADD None [3]\nOps.INDEX None []\nOps.STORE None [3]\nOps.ENDRANGE None []\n\n\n\nprint(ei.prg.p.src)\n\n\nvoid E_25_4(int* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 25; ridx0++) {\n    int alu0 = (ridx0&lt;&lt;2);\n    *(data0+alu0) = 3;\n    *(data0+(alu0+1)) = 3;\n    *(data0+(alu0+2)) = 3;\n    *(data0+(alu0+3)) = 3;\n  }\n}\n\n\n\nLet’s compile and run the code. We will go into much more details on individual steps later.\n\nei.run()\n\n*** CPU        1 E_25_4                                    arg  1 mem  0.00 GB tm      7.96us/     0.01ms (     0.00 GFLOPS    0.1|0.1     GB/s) ['contiguous', '__add__']\n\n\n7.961993105709553e-06\n\n\nThe result has been stored to the buffer:\n\nimport numpy as np\n\nview = memoryview(a.lazydata.base.realized._buf)\nnp.frombuffer(view, dtype=np.int32).reshape(a.shape)\n\narray([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]], dtype=int32)",
    "crumbs": [
      "1 - UOps"
    ]
  },
  {
    "objectID": "arange.html",
    "href": "arange.html",
    "title": "4 - The .arange() insanity",
    "section": "",
    "text": "import os\nos.environ[\"CPU\"] = \"1\"\nos.environ[\"DEBUG\"]=\"4\"\n# os.environ[\"NOOPT\"]=\"1\"\n\nfrom tinygrad import Tensor, dtypes\nfrom tinygrad.ops import UOp, Ops, PatternMatcher, UPat, graph_rewrite\n\nLet’s try something light and fun - a simple arange.\n\na = Tensor.arange(0.5, 2, 0.2) # Start, stop, step =&gt; [0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9]\na.lazydata\n\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n  UOp(Ops.RESHAPE, dtypes.float, arg=(8,), src=(\n    UOp(Ops.REDUCE_AXIS, dtypes.float, arg=(Ops.ADD, (1,)), src=(\n      UOp(Ops.PERMUTE, dtypes.float, arg=(1, 0), src=(\n        UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8), src=(\n          UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8, 1), src=(\n            UOp(Ops.SHRINK, dtypes.float, arg=((0, 8), (0, 8)), src=(\n              UOp(Ops.RESHAPE, dtypes.float, arg=(8, 16), src=(\n                UOp(Ops.SHRINK, dtypes.float, arg=((0, 128),), src=(\n                  UOp(Ops.RESHAPE, dtypes.float, arg=(135,), src=(\n                    UOp(Ops.EXPAND, dtypes.float, arg=(9, 15), src=(\n                      UOp(Ops.RESHAPE, dtypes.float, arg=(1, 15), src=(\n                        UOp(Ops.PAD, dtypes.float, arg=((7, 0),), src=(\n                          UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n                            UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n                              UOp(Ops.CONST, dtypes.float, arg=0.2, src=(\n                                x15:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n                                  UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),)),)),)),)),)),)),)),)),)),)),)),)),)),)),\n  UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n    UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n      UOp(Ops.CONST, dtypes.float, arg=0.3, src=(\n         x15,)),)),)),))\n\n\nOhh, what the hell? I just want 8 numbers, what’s this insanity?\nWhen something is not obvious, you can usually just go step by step.\n                          UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n                            UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n                              UOp(Ops.CONST, dtypes.float, arg=0.2, src=(\n0.2 is the start element (0.5) minus the step (0.3)\n\nNext we pad it with 7 (n-1) 0 elements on the left and expand vertically to 9 (n+1):\n                    UOp(Ops.EXPAND, dtypes.float, arg=(9, 15), src=(\n                      UOp(Ops.RESHAPE, dtypes.float, arg=(1, 15), src=(\n                        UOp(Ops.PAD, dtypes.float, arg=((7, 0),), src=(\n\nNext, we string it into a 135-element array, cut off the last 7 elements, making it end in ... 0, 0, 0, 0.2 ]:\nThen we reshape it back to a rectangle, now (8, 16). The transformation created an interesting pattern.\n              UOp(Ops.RESHAPE, dtypes.float, arg=(8, 16), src=(\n                UOp(Ops.SHRINK, dtypes.float, arg=((0, 128),), src=(\n                  UOp(Ops.RESHAPE, dtypes.float, arg=(135,), src=(\n\nNext we take just the left half othe the pattern, and apply 3 transformations that don’t actually do anything.\nWe wil explore the source of those transforms later.\n      UOp(Ops.PERMUTE, dtypes.float, arg=(1, 0), src=(\n        UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8), src=(\n          UOp(Ops.RESHAPE, dtypes.float, arg=(8, 8, 1), src=(\n            UOp(Ops.SHRINK, dtypes.float, arg=((0, 8), (0, 8)), src=(\n\nThis was a long wat to get a triangular matrix (sad Linear Algebra noises).\nBut now that we have it, let’s sum over the elements in one axis (axis 1 in this case):\n  UOp(Ops.RESHAPE, dtypes.float, arg=(8,), src=(\n    UOp(Ops.REDUCE_AXIS, dtypes.float, arg=(Ops.ADD, (1,)), src=(\n\nI thin you see where this is going. Let’s conver the second branch of the top ADD:\n  UOp(Ops.EXPAND, dtypes.float, arg=(8,), src=(\n    UOp(Ops.RESHAPE, dtypes.float, arg=(1,), src=(\n      UOp(Ops.CONST, dtypes.float, arg=0.3, src=(\n\nAnd the final step - we add the 2 to get to our numbers!\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n\nLet’s have a look at the code I guess.\n\na.numpy()\n\nopened device CPU from pid:154318\nr_8_8\n 0: (8, 1)                    float.ptr(8)         (1, 0)                         ShapeTracker(views=(View(shape=(8, 1), strides=(1, 0), offset=0, mask=None, contiguous=True),))\n[Opt(op=OptOps.UNROLL, axis=0, arg=0)]\n\nvoid r_8_8(float* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 8; ridx0++) {\n    *(data0+ridx0) = ((((ridx0&lt;7)!=1)?0.2f:0.0f)+(((ridx0&lt;6)!=1)?0.2f:0.0f)+(((ridx0&lt;5)!=1)?0.2f:0.0f)+(((ridx0&lt;4)!=1)?0.2f:0.0f)+(((ridx0&lt;3)!=1)?0.2f:0.0f)+(((ridx0&lt;2)!=1)?0.2f:0.0f)+(((ridx0&lt;1)!=1)?0.2f:0.0f)+0.5f);\n  }\n}\n\n*** CPU        1 r_8_8                                     arg  1 mem  0.00 GB tm      2.79us/     0.00ms (     0.08 GFLOPS    0.0|0.0     GB/s) ['numpy', 'arange']\n\n\narray([0.5      , 0.7      , 0.9      , 1.1      , 1.3      , 1.5      ,\n       1.7      , 1.9000001], dtype=float32)\n\n\nNot gonna lie, this is an ugly way to do it. I suspect this could be improved.\nvoid r_8_8(float* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 8; ridx0++) {\n    *(data0 + ridx0) = ((((ridx0 &lt; 7) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 6) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 5) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 4) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 3) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 2) != 1) ? 0.2f : 0.0f) +\n                        (((ridx0 &lt; 1) != 1) ? 0.2f : 0.0f) + 0.5f);\n  }\n}\nThis must be some optimization mis-behaving. With NOOPT=1 we get good code:\nvoid r_8_8(float* restrict data0) {\n  for (int ridx0 = 0; ridx0 &lt; 8; ridx0++) {\n    *(data0+ridx0) = ((((float)((ridx0+1)))*0.2f)+0.3f);\n  }\n}\nAt least TinyGrad was able to optimize out all the instane transformations!\nLet’s take a look at the code:\ntensor.py (simplified)\n  def arange(start, stop=None, step=1, **kwargs) -&gt; Tensor:\n    dtype = ...              # Figure out the output dtype\n    output_len=ceildiv(stop-start, step)\n    return (Tensor.full((output_len,), step, dtype=dtype, **kwargs)._cumalu(0, Ops.ADD) + (start - step)).cast(dtype)\n\n(Tensor.full((output_len,), step, dtype=dtype, **kwargs) This creates the (8,) vector filled with 0.2\n(start - step) This is the second branch of the top ADD.\n_cumalu(0, Ops.ADD) And all the magic happens here.\n\n  def _cumalu(self, axis:int, op:Ops, _include_initial=False) -&gt; Tensor:\n    pl_sz = self.shape[axis] - int(not _include_initial)\n    pooled = self.transpose(axis,-1).pad((pl_sz, -int(_include_initial)), value=identity_element(op, self.dtype))._pool((self.shape[axis],))\n\n    # For ADD:\n    return pooled.sum(-1).transpose(axis, -1)\n\nNote: I think the last line should be return {Ops.ADD: pooled.sum, Ops.MAX: pooled.max, Ops.MUL: pooled.prod}[op](-1).transpose(axis, -1)\n\n\npl_sz will be 7\nself.transpose(axis,-1) makes the axis we are interested in the last. In our case we only have 1 axis, so this does nothing, and gets optimized out immediately.\npad((pl_sz, -int(_include_initial)) - Here is the pad that adds the 7 elements to the left filled with zeros (identity element for ADD).\n_pool((self.shape[axis],)): Here be dragons\n\n  def _pool(self, k_:tuple[sint, ...], stride:int|tuple[int, ...]=1, dilation:int|tuple[int, ...]=1) -&gt; Tensor:\n    assert len(self.shape) &gt;= len(k_), f\"can't pool {self.shape} with {k_}\"\n    s_, d_ = make_tuple(stride, len(k_)), make_tuple(dilation, len(k_))\n    assert len(k_) == len(s_) == len(d_), f\"stride/dilation mismatch kernel:{k_} stride:{s_} dilation:{d_}\"\n    noop, i_ = [None] * (self.ndim-len(k_)), self.shape[-len(k_):]\n    assert all(resolve(d*(k-1)+1 &lt;= i) for k,d,i in zip(k_,d_,i_)), \"kernel size cannot be greater than actual input size\"\n    o_ = [ceildiv(i-d*(k-1), s) for i,d,k,s in zip(i_,d_,k_,s_)]\n    if any(resolve(k &gt; s) for k,s in zip(k_,s_)) or any(d != 1 for d in d_):\n      # input size scaling factor to make sure shrink for stride is possible\n      f_ = [1 + int(resolve(o*s &gt; (i - d*(k-1)))) for o,s,i,d,k in zip(o_,s_,i_,d_,k_)]\n      # # repeats such that we don't need padding\n      x = self.repeat([1]*len(noop) + [ceildiv(k*(i*f+d),i) for k,i,d,f in zip(k_,i_,d_,f_)])\n      # handle dilation\n      x = x.shrink(tuple(noop + [(0,k*(i*f+d)) for k,i,d,f in zip(k_,i_,d_,f_)])).reshape(noop + flatten((k,(i*f+d)) for k,i,d,f in zip(k_,i_,d_,f_)))\n      # handle stride\n      x = x.shrink(tuple(noop + flatten(((0,k), (0,o*s)) for k,o,s in zip(k_,o_,s_)))).reshape(noop + flatten((k,o,s) for k,o,s in zip(k_,o_,s_)))\n      x = x.shrink(tuple(noop + flatten(((0,k), (0,o), (0,1)) for k,o in zip(k_,o_)))).reshape(noop + flatten((k,o) for k,o in zip(k_,o_)))\n      # permute to move reduce to the end\n      return x.permute(*range(len(noop)), *[len(noop)+i*2+1 for i in range(len(i_))], *[len(noop)+i*2 for i in range(len(i_))])\n    # TODO: once the shapetracker can optimize well, remove this alternative implementation\n    x = self.pad(tuple(noop + [(0, max(0,o*s-i)) for i,o,s in zip(i_,o_,s_)])).shrink(tuple(noop + [(0,o*s) for o,s in zip(o_,s_)]))\n    x = x.reshape(noop + flatten(((o,s) for o,s in zip(o_,s_))))\n    x = x.shrink(tuple(noop + flatten(((0,o), (0,k)) for o,k in zip(o_,k_))))\n    return x.permute(*range(len(noop)), *[len(noop)+i*2 for i in range(len(i_))], *[len(noop)+i*2+1 for i in range(len(i_))])\nLol I’m not going to pretend I can follow what’s going on here. This obviously does the crazy triangle.\n\npooled.sum(-1).transpose(axis, -1) This must be that REDUCE_AXIS operation, and another transpose that does nothing in this case.\n\nSometimes I wish TinyGrad was easier to understand. :)",
    "crumbs": [
      "4 - The `.arange()` insanity"
    ]
  },
  {
    "objectID": "patternmatcher.html",
    "href": "patternmatcher.html",
    "title": "3 - The Pattern Matcher",
    "section": "",
    "text": "Our next TinyGrad abstraction is the Pattern Matcher (PM)\nPM is used all over TinyGrad for different purposes\n\nimport os\nos.environ[\"CPU\"] = \"1\"\nos.environ[\"DEBUG\"]=\"4\"\n\nfrom tinygrad import Tensor, dtypes\nfrom tinygrad.ops import UOp, Ops, PatternMatcher, UPat, graph_rewrite\n\n\na = (Tensor(2) * 5 + 1).lazydata\na\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=2, src=(\n      x2:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n        UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=(\n       x2,)),)),\n  UOp(Ops.CONST, dtypes.int, arg=1, src=(\n     x2,)),))\n\n\nThe PM operates on a list of rules.\nEach rule consists of a UPat, and a function that is called when the pattern matches part of the tree.\nThe return value of the function is the result of the match, or it’s a None if no match was found among the rules:\n\ntest_rules = PatternMatcher([\n    (UPat(Ops.DEVICE), lambda: \"a DEVICE Uop\"),                                         # This rule matches any `DEVICE` UOp\n    (UPat(Ops.CONST, name=\"x\"), lambda x: f\"Got a CONST dtype {x.dtype} arg {x.arg}\"),  # Can pass the Op to the function\n    (UPat(Ops.CONST), lambda x: f\"Another rule for CONST\"),                             # Oops, only one rule can match!\n    (UPat((Ops.ADD, Ops.MUL)), lambda: \"ADD or MUL\"),                                   # Can match more than one UOp type\n    (UPat(Ops.EXPAND, src=(UPat(Ops.RESHAPE, src=UPat(Ops.CONST, arg=2)))),\n        lambda: \"Expand with reshape from a const with arg=2\")                          # Can match a specific sub-tree.\n                                                                                        # Note: This one only matches the EXPAND for 2, not 1\n    # No match - return Null\n])\n\n[test_rules.rewrite(op) for op in a.toposort]\n\n['a DEVICE Uop',\n None,\n 'Got a CONST dtype dtypes.int arg 2',\n 'Got a CONST dtype dtypes.int arg 5',\n 'ADD or MUL',\n 'Got a CONST dtype dtypes.int arg 1',\n 'ADD or MUL']\n\n\n\nRewriting trees\nA more interesting pattern is to replace the matched UOps with some other UOps. We can also use graph_rewrite to operate on a tree.\n\ninsanity = PatternMatcher([\n    (UPat(Ops.ADD, name=\"x\"), lambda x: UOp(Ops.SUB, dtype=x.dtype, arg=x.arg, src=x.src)),\n    (UPat(Ops.MUL, dtype=dtypes.ints, name=\"x\"), lambda x: UOp(Ops.IDIV, dtype=x.dtype, src=x.src))\n])\n\nrewritten = graph_rewrite(a, insanity)\nrewritten\n\nUOp(Ops.SUB, dtypes.int, arg=None, src=(\n  UOp(Ops.IDIV, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=2, src=(\n      x2:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n        UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=(\n       x2,)),)),\n  UOp(Ops.CONST, dtypes.int, arg=1, src=(\n     x2,)),))\n\n\n\na.render(simplify=False)\n\n'((2*5)+1)'\n\n\n\nrewritten.render(simplify=False)\n\n'((2//5)-1)'\n\n\n\nint(rewritten)\n\n-1\n\n\n\n\nPatternMatcher in TinyGrad\nI think you get the idea. The Pattern Matches is a powerful tool that is used throughout Tinygrad.\nWhen we played with Tensor.schedule_with_vars() and lower_schedule_item() in the chapter on UOps, both function made extensive use of many Pattern Matchers. We will attempt a deep dive into their details in the next chapter.\n\nTinyGrad spec\nAnother use for the Pattern Matcher - checking the validity of UOp trees, according to the spec, , found in tinyngrad/spec.py.\nIt’s very much possible to create UOp trees that are not valid in general, or not valid at certain stages of processing.\nThe spec contains rules that check for silly mistakes in different types of (sub-)trees.\nFor example thre is a tensor_uop_spec for sanity checking the UOp trees created by tensor operations:\n\na\n\nUOp(Ops.ADD, dtypes.int, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=2, src=(\n      x2:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n        UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=(\n       x2,)),)),\n  UOp(Ops.CONST, dtypes.int, arg=1, src=(\n     x2,)),))\n\n\n\nfrom tinygrad.spec import type_verify, tensor_uop_spec\n\ntype_verify(list(a.toposort.keys()), tensor_uop_spec) # It throws on errors, no errors found!\n\nLet’s make a broken tree by changing the dtype of the ADD UOp in a to float:\n\nbad = a.replace(dtype=dtypes.float)\nbad\n\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n  UOp(Ops.MUL, dtypes.int, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=2, src=(\n      x2:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n        UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n    UOp(Ops.CONST, dtypes.int, arg=5, src=(\n       x2,)),)),\n  UOp(Ops.CONST, dtypes.int, arg=1, src=(\n     x2,)),))\n\n\nThis is not a valid tree - we are adding 2 ints, but the result is a float? There would need to be a cast there!\n\ntry:\n    type_verify(list(bad.toposort.keys()), tensor_uop_spec)\nexcept Exception as e:\n    print(f\"{type(e).__name__}: {' '.join(e.args)}\")\n\n   0 Ops.DEVICE          : dtypes.void                    []                               CPU\n   1 Ops.VIEW            : dtypes.void                    [0]                              ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),))\n   2 Ops.CONST           : dtypes.int                     [1]                              2\n   3 Ops.CONST           : dtypes.int                     [1]                              5\n   4 Ops.MUL             : dtypes.int                     ['2', '5']                       None\n   5 Ops.CONST           : dtypes.int                     [1]                              1\n   6 Ops.ADD             : dtypes.float                   [4, '1']                         None\nRuntimeError: UOp verification failed at 6 on Ops.ADD dtypes.float 2 [&lt;Ops.MUL: 50&gt;, &lt;Ops.CONST: 76&gt;] None\n\n\nIndeed, we caught the error. Let’s fix the tree by casting the 2 ADD sources to a float.\n\nfixed = bad.replace(src=tuple([UOp(Ops.CAST, dtype=dtypes.float, src=(src,)) for src in bad.src]))\nfixed\n\nUOp(Ops.ADD, dtypes.float, arg=None, src=(\n  UOp(Ops.CAST, dtypes.float, arg=None, src=(\n    UOp(Ops.MUL, dtypes.int, arg=None, src=(\n      UOp(Ops.CONST, dtypes.int, arg=2, src=(\n        x3:=UOp(Ops.VIEW, dtypes.void, arg=ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), src=(\n          UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),)),\n      UOp(Ops.CONST, dtypes.int, arg=5, src=(\n         x3,)),)),)),\n  UOp(Ops.CAST, dtypes.float, arg=None, src=(\n    UOp(Ops.CONST, dtypes.int, arg=1, src=(\n       x3,)),)),))\n\n\n\ntype_verify(list(fixed.toposort.keys()), tensor_uop_spec)\n\nNow it works!",
    "crumbs": [
      "3 - The Pattern Matcher"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My notes on TyniGrad internals",
    "section": "",
    "text": "Tinygrad source: https://github.com/tinygrad/tinygrad\nTinygrad docs: https://docs.tinygrad.org/\nA great deal of inspiration has been taken from mesozoic-egg Tynigrad Notes\n\n0 - Introduction\n1 - UOps\n1.1 - UOp full summary\n2 - View and ShapeTracker\n3 - The Pattern Matcher\n4 - The .arange() insanity",
    "crumbs": [
      "My notes on TyniGrad internals"
    ]
  }
]